I 2015-05-22 15:51:31 theanets.layers:465 layer in: out:0 -> 1000, linear, 0 parameters
I 2015-05-22 15:51:31 theanets.layers:465 layer bdlstm1_fw: in.out:1000 -> 50, logistic, 210350 parameters
I 2015-05-22 15:51:31 theanets.layers:465 layer bdlstm1_bw: in.out:1000 -> 50, logistic, 210350 parameters
I 2015-05-22 15:51:31 theanets.layers:465 layer bdlstm1: in.out:1000 -> 100, logistic, 420700 parameters
I 2015-05-22 15:51:31 theanets.layers:465 layer bdlstm2_fw: bdlstm1.out:100 -> 25, logistic, 12675 parameters
I 2015-05-22 15:51:31 theanets.layers:465 layer bdlstm2_bw: bdlstm1.out:100 -> 25, logistic, 12675 parameters
I 2015-05-22 15:51:31 theanets.layers:465 layer bdlstm2: bdlstm1.out:100 -> 50, logistic, 25350 parameters
I 2015-05-22 15:51:31 theanets.layers:465 layer out: bdlstm2.out:50 -> 1, linear, 51 parameters
I 2015-05-22 15:51:31 theanets.graph:145 network has 446101 total parameters
models_deep/models-100-50-1024-None-None-None.pkl
I 2015-05-22 15:51:31 theanets.dataset:133 valid: 3 mini-batches from callable
I 2015-05-22 15:51:31 theanets.dataset:133 train: 30 mini-batches from callable
I 2015-05-22 15:51:31 theanets.main:87 creating trainer <class 'theanets.trainer.RmsProp'>
I 2015-05-22 15:51:31 theanets.main:89 --batch_size = 1024
I 2015-05-22 15:51:31 theanets.main:89 --gradient_clip = 1
I 2015-05-22 15:51:31 theanets.main:89 --hidden_l1 = None
I 2015-05-22 15:51:31 theanets.main:89 --learning_rate = 0.001
I 2015-05-22 15:51:31 theanets.main:89 --train_batches = 30
I 2015-05-22 15:51:31 theanets.main:89 --valid_batches = 3
I 2015-05-22 15:51:31 theanets.main:89 --weight_l1 = None
I 2015-05-22 15:51:31 theanets.main:89 --weight_l2 = None
I 2015-05-22 15:51:31 theanets.trainer:134 compiling evaluation function
I 2015-05-22 15:51:35 theanets.trainer:294 compiling RmsProp learning function
I 2015-05-22 15:52:32 theanets.trainer:168 validation 0 loss=212185.453125 err=212185.453125 *
I 2015-05-22 15:52:38 theanets.trainer:168 RmsProp 1 loss=207498.843750 err=207498.843750
I 2015-05-22 15:52:46 theanets.trainer:168 RmsProp 2 loss=170145.031250 err=170145.031250
I 2015-05-22 15:52:54 theanets.trainer:168 RmsProp 3 loss=69084.007812 err=69084.007812
I 2015-05-22 15:53:02 theanets.trainer:168 RmsProp 4 loss=39225.300781 err=39225.300781
I 2015-05-22 15:53:10 theanets.trainer:168 RmsProp 5 loss=33810.648438 err=33810.648438
I 2015-05-22 15:53:18 theanets.trainer:168 RmsProp 6 loss=20707.044922 err=20707.044922
I 2015-05-22 15:53:24 theanets.trainer:168 RmsProp 7 loss=19522.560547 err=19522.560547
I 2015-05-22 15:53:32 theanets.trainer:168 RmsProp 8 loss=17782.583984 err=17782.583984
I 2015-05-22 15:53:39 theanets.trainer:168 RmsProp 9 loss=14725.781250 err=14725.781250
I 2015-05-22 15:53:46 theanets.trainer:168 RmsProp 10 loss=13244.297852 err=13244.297852
I 2015-05-22 15:53:46 theanets.trainer:168 validation 1 loss=13447.078125 err=13447.078125 *
I 2015-05-22 15:53:53 theanets.trainer:168 RmsProp 11 loss=12470.001953 err=12470.001953
I 2015-05-22 15:53:59 theanets.trainer:168 RmsProp 12 loss=11628.961914 err=11628.961914
I 2015-05-22 15:54:06 theanets.trainer:168 RmsProp 13 loss=10868.038086 err=10868.038086
I 2015-05-22 15:54:12 theanets.trainer:168 RmsProp 14 loss=10180.486328 err=10180.486328
I 2015-05-22 15:54:19 theanets.trainer:168 RmsProp 15 loss=9789.077148 err=9789.077148
I 2015-05-22 15:54:25 theanets.trainer:168 RmsProp 16 loss=9378.405273 err=9378.405273
I 2015-05-22 15:54:33 theanets.trainer:168 RmsProp 17 loss=8836.573242 err=8836.573242
I 2015-05-22 15:54:41 theanets.trainer:168 RmsProp 18 loss=8831.803711 err=8831.803711
I 2015-05-22 15:54:49 theanets.trainer:168 RmsProp 19 loss=8310.762695 err=8310.762695
I 2015-05-22 15:54:57 theanets.trainer:168 RmsProp 20 loss=8021.736328 err=8021.736328
I 2015-05-22 15:54:58 theanets.trainer:168 validation 2 loss=8659.084961 err=8659.084961 *
I 2015-05-22 15:55:04 theanets.trainer:168 RmsProp 21 loss=7775.875488 err=7775.875488
I 2015-05-22 15:55:11 theanets.trainer:168 RmsProp 22 loss=7473.649414 err=7473.649414
I 2015-05-22 15:55:18 theanets.trainer:168 RmsProp 23 loss=7197.149414 err=7197.149414
I 2015-05-22 15:55:26 theanets.trainer:168 RmsProp 24 loss=7028.145508 err=7028.145508
I 2015-05-22 15:55:34 theanets.trainer:168 RmsProp 25 loss=6872.880859 err=6872.880859
I 2015-05-22 15:55:42 theanets.trainer:168 RmsProp 26 loss=6654.662109 err=6654.662109
I 2015-05-22 15:55:48 theanets.trainer:168 RmsProp 27 loss=6563.872559 err=6563.872559
I 2015-05-22 15:55:55 theanets.trainer:168 RmsProp 28 loss=6386.937012 err=6386.937012
I 2015-05-22 15:56:03 theanets.trainer:168 RmsProp 29 loss=6118.185547 err=6118.185547
I 2015-05-22 15:56:11 theanets.trainer:168 RmsProp 30 loss=6009.787109 err=6009.787109
I 2015-05-22 15:56:12 theanets.trainer:168 validation 3 loss=7185.979980 err=7185.979980 *
I 2015-05-22 15:56:18 theanets.trainer:168 RmsProp 31 loss=5853.059082 err=5853.059082
I 2015-05-22 15:56:25 theanets.trainer:168 RmsProp 32 loss=5700.497559 err=5700.497559
I 2015-05-22 15:56:32 theanets.trainer:168 RmsProp 33 loss=5543.304199 err=5543.304199
I 2015-05-22 15:56:39 theanets.trainer:168 RmsProp 34 loss=5405.447754 err=5405.447754
I 2015-05-22 15:56:46 theanets.trainer:168 RmsProp 35 loss=5309.325195 err=5309.325195
I 2015-05-22 15:56:52 theanets.trainer:168 RmsProp 36 loss=5206.022949 err=5206.022949
I 2015-05-22 15:57:00 theanets.trainer:168 RmsProp 37 loss=5136.395020 err=5136.395020
I 2015-05-22 15:57:08 theanets.trainer:168 RmsProp 38 loss=4922.854492 err=4922.854492
I 2015-05-22 15:57:16 theanets.trainer:168 RmsProp 39 loss=4800.190430 err=4800.190430
I 2015-05-22 15:57:24 theanets.trainer:168 RmsProp 40 loss=4759.121582 err=4759.121582
I 2015-05-22 15:57:25 theanets.trainer:168 validation 4 loss=6330.778809 err=6330.778809 *
I 2015-05-22 15:57:31 theanets.trainer:168 RmsProp 41 loss=4598.891113 err=4598.891113
I 2015-05-22 15:57:38 theanets.trainer:168 RmsProp 42 loss=4504.498535 err=4504.498535
I 2015-05-22 15:57:44 theanets.trainer:168 RmsProp 43 loss=4443.266602 err=4443.266602
I 2015-05-22 15:57:51 theanets.trainer:168 RmsProp 44 loss=4307.305664 err=4307.305664
I 2015-05-22 15:57:57 theanets.trainer:168 RmsProp 45 loss=4222.383301 err=4222.383301
I 2015-05-22 15:58:04 theanets.trainer:168 RmsProp 46 loss=4171.906250 err=4171.906250
I 2015-05-22 15:58:12 theanets.trainer:168 RmsProp 47 loss=4113.503906 err=4113.503906
I 2015-05-22 15:58:20 theanets.trainer:168 RmsProp 48 loss=4022.629150 err=4022.629150
I 2015-05-22 15:58:28 theanets.trainer:168 RmsProp 49 loss=3903.170166 err=3903.170166
I 2015-05-22 15:58:35 theanets.trainer:168 RmsProp 50 loss=3820.559570 err=3820.559570
I 2015-05-22 15:58:36 theanets.trainer:168 validation 5 loss=5407.274902 err=5407.274902 *
I 2015-05-22 15:58:43 theanets.trainer:168 RmsProp 51 loss=3765.886963 err=3765.886963
I 2015-05-22 15:58:51 theanets.trainer:168 RmsProp 52 loss=3715.223633 err=3715.223633
I 2015-05-22 15:58:58 theanets.trainer:168 RmsProp 53 loss=3620.679199 err=3620.679199
I 2015-05-22 15:59:05 theanets.trainer:168 RmsProp 54 loss=3559.782471 err=3559.782471
I 2015-05-22 15:59:12 theanets.trainer:168 RmsProp 55 loss=3480.760742 err=3480.760742
I 2015-05-22 15:59:18 theanets.trainer:168 RmsProp 56 loss=3383.835938 err=3383.835938
I 2015-05-22 15:59:26 theanets.trainer:168 RmsProp 57 loss=3436.680176 err=3436.680176
I 2015-05-22 15:59:33 theanets.trainer:168 RmsProp 58 loss=3298.683350 err=3298.683350
I 2015-05-22 15:59:39 theanets.trainer:168 RmsProp 59 loss=3248.022461 err=3248.022461
I 2015-05-22 15:59:46 theanets.trainer:168 RmsProp 60 loss=3127.540771 err=3127.540771
I 2015-05-22 15:59:46 theanets.trainer:168 validation 6 loss=4955.565918 err=4955.565918 *
I 2015-05-22 15:59:53 theanets.trainer:168 RmsProp 61 loss=3171.292969 err=3171.292969
I 2015-05-22 16:00:00 theanets.trainer:168 RmsProp 62 loss=3058.387695 err=3058.387695
I 2015-05-22 16:00:06 theanets.trainer:168 RmsProp 63 loss=2992.007812 err=2992.007812
I 2015-05-22 16:00:13 theanets.trainer:168 RmsProp 64 loss=2952.054199 err=2952.054199
I 2015-05-22 16:00:20 theanets.trainer:168 RmsProp 65 loss=2896.699707 err=2896.699707
I 2015-05-22 16:00:28 theanets.trainer:168 RmsProp 66 loss=2846.430908 err=2846.430908
I 2015-05-22 16:00:36 theanets.trainer:168 RmsProp 67 loss=2830.692139 err=2830.692139
I 2015-05-22 16:00:42 theanets.trainer:168 RmsProp 68 loss=2761.789062 err=2761.789062
I 2015-05-22 16:00:49 theanets.trainer:168 RmsProp 69 loss=2739.952881 err=2739.952881
I 2015-05-22 16:00:56 theanets.trainer:168 RmsProp 70 loss=2661.816162 err=2661.816162
I 2015-05-22 16:00:56 theanets.trainer:168 validation 7 loss=4452.417969 err=4452.417969 *
I 2015-05-22 16:01:04 theanets.trainer:168 RmsProp 71 loss=2677.147461 err=2677.147461
I 2015-05-22 16:01:12 theanets.trainer:168 RmsProp 72 loss=2605.145752 err=2605.145752
I 2015-05-22 16:01:20 theanets.trainer:168 RmsProp 73 loss=2561.082275 err=2561.082275
I 2015-05-22 16:01:28 theanets.trainer:168 RmsProp 74 loss=2514.239014 err=2514.239014
I 2015-05-22 16:01:36 theanets.trainer:168 RmsProp 75 loss=2498.368164 err=2498.368164
I 2015-05-22 16:01:44 theanets.trainer:168 RmsProp 76 loss=2432.827881 err=2432.827881
I 2015-05-22 16:01:50 theanets.trainer:168 RmsProp 77 loss=2365.683838 err=2365.683838
I 2015-05-22 16:01:57 theanets.trainer:168 RmsProp 78 loss=2318.343750 err=2318.343750
I 2015-05-22 16:02:04 theanets.trainer:168 RmsProp 79 loss=2355.719238 err=2355.719238
I 2015-05-22 16:02:10 theanets.trainer:168 RmsProp 80 loss=2283.079346 err=2283.079346
I 2015-05-22 16:02:11 theanets.trainer:168 validation 8 loss=4212.976562 err=4212.976562 *
I 2015-05-22 16:02:18 theanets.trainer:168 RmsProp 81 loss=2264.646729 err=2264.646729
I 2015-05-22 16:02:24 theanets.trainer:168 RmsProp 82 loss=2253.932373 err=2253.932373
I 2015-05-22 16:02:31 theanets.trainer:168 RmsProp 83 loss=2256.700684 err=2256.700684
I 2015-05-22 16:02:37 theanets.trainer:168 RmsProp 84 loss=2186.965820 err=2186.965820
I 2015-05-22 16:02:44 theanets.trainer:168 RmsProp 85 loss=2115.139648 err=2115.139648
I 2015-05-22 16:02:50 theanets.trainer:168 RmsProp 86 loss=2123.294434 err=2123.294434
I 2015-05-22 16:02:58 theanets.trainer:168 RmsProp 87 loss=2048.083740 err=2048.083740
I 2015-05-22 16:03:06 theanets.trainer:168 RmsProp 88 loss=2010.620605 err=2010.620605
I 2015-05-22 16:03:14 theanets.trainer:168 RmsProp 89 loss=2006.363403 err=2006.363403
I 2015-05-22 16:03:22 theanets.trainer:168 RmsProp 90 loss=1957.117554 err=1957.117554
I 2015-05-22 16:03:22 theanets.trainer:168 validation 9 loss=4001.208740 err=4001.208740 *
I 2015-05-22 16:03:29 theanets.trainer:168 RmsProp 91 loss=1975.267212 err=1975.267212
I 2015-05-22 16:03:36 theanets.trainer:168 RmsProp 92 loss=1950.748657 err=1950.748657
I 2015-05-22 16:03:42 theanets.trainer:168 RmsProp 93 loss=1907.356201 err=1907.356201
I 2015-05-22 16:03:49 theanets.trainer:168 RmsProp 94 loss=1948.683594 err=1948.683594
I 2015-05-22 16:03:56 theanets.trainer:168 RmsProp 95 loss=1881.378540 err=1881.378540
I 2015-05-22 16:04:03 theanets.trainer:168 RmsProp 96 loss=1818.683838 err=1818.683838
I 2015-05-22 16:04:09 theanets.trainer:168 RmsProp 97 loss=1802.701416 err=1802.701416
I 2015-05-22 16:04:16 theanets.trainer:168 RmsProp 98 loss=1775.412598 err=1775.412598
I 2015-05-22 16:04:23 theanets.trainer:168 RmsProp 99 loss=1745.812622 err=1745.812622
I 2015-05-22 16:04:29 theanets.trainer:168 RmsProp 100 loss=1752.271606 err=1752.271606
I 2015-05-22 16:04:29 theanets.trainer:168 validation 10 loss=3793.379639 err=3793.379639 *
I 2015-05-22 16:04:36 theanets.trainer:168 RmsProp 101 loss=1712.911865 err=1712.911865
I 2015-05-22 16:04:45 theanets.trainer:168 RmsProp 102 loss=1672.715942 err=1672.715942
I 2015-05-22 16:04:53 theanets.trainer:168 RmsProp 103 loss=1669.458984 err=1669.458984
I 2015-05-22 16:05:01 theanets.trainer:168 RmsProp 104 loss=1678.874146 err=1678.874146
I 2015-05-22 16:05:09 theanets.trainer:168 RmsProp 105 loss=1622.550537 err=1622.550537
I 2015-05-22 16:05:17 theanets.trainer:168 RmsProp 106 loss=1593.614380 err=1593.614380
I 2015-05-22 16:05:24 theanets.trainer:168 RmsProp 107 loss=1540.243652 err=1540.243652
I 2015-05-22 16:05:31 theanets.trainer:168 RmsProp 108 loss=1561.000000 err=1561.000000
I 2015-05-22 16:05:37 theanets.trainer:168 RmsProp 109 loss=1547.674316 err=1547.674316
I 2015-05-22 16:05:45 theanets.trainer:168 RmsProp 110 loss=1507.475708 err=1507.475708
I 2015-05-22 16:05:45 theanets.trainer:168 validation 11 loss=3653.272705 err=3653.272705 *
I 2015-05-22 16:05:53 theanets.trainer:168 RmsProp 111 loss=1504.995239 err=1504.995239
I 2015-05-22 16:06:01 theanets.trainer:168 RmsProp 112 loss=1474.015503 err=1474.015503
I 2015-05-22 16:06:09 theanets.trainer:168 RmsProp 113 loss=1439.697754 err=1439.697754
I 2015-05-22 16:06:16 theanets.trainer:168 RmsProp 114 loss=1485.968628 err=1485.968628
I 2015-05-22 16:06:23 theanets.trainer:168 RmsProp 115 loss=1416.335815 err=1416.335815
I 2015-05-22 16:06:29 theanets.trainer:168 RmsProp 116 loss=1443.231079 err=1443.231079
I 2015-05-22 16:06:36 theanets.trainer:168 RmsProp 117 loss=1430.822998 err=1430.822998
I 2015-05-22 16:06:44 theanets.trainer:168 RmsProp 118 loss=1414.922729 err=1414.922729
I 2015-05-22 16:06:52 theanets.trainer:168 RmsProp 119 loss=1431.559448 err=1431.559448
I 2015-05-22 16:07:00 theanets.trainer:168 RmsProp 120 loss=1391.973511 err=1391.973511
I 2015-05-22 16:07:01 theanets.trainer:168 validation 12 loss=3492.136475 err=3492.136475 *
I 2015-05-22 16:07:08 theanets.trainer:168 RmsProp 121 loss=1381.607544 err=1381.607544
I 2015-05-22 16:07:14 theanets.trainer:168 RmsProp 122 loss=1383.062988 err=1383.062988
I 2015-05-22 16:07:21 theanets.trainer:168 RmsProp 123 loss=1408.600464 err=1408.600464
I 2015-05-22 16:07:27 theanets.trainer:168 RmsProp 124 loss=1339.074707 err=1339.074707
I 2015-05-22 16:07:33 theanets.trainer:168 RmsProp 125 loss=1462.585693 err=1462.585693
I 2015-05-22 16:07:40 theanets.trainer:168 RmsProp 126 loss=1325.601074 err=1325.601074
I 2015-05-22 16:07:47 theanets.trainer:168 RmsProp 127 loss=1256.650024 err=1256.650024
I 2015-05-22 16:07:55 theanets.trainer:168 RmsProp 128 loss=1262.862915 err=1262.862915
I 2015-05-22 16:08:03 theanets.trainer:168 RmsProp 129 loss=1221.543091 err=1221.543091
I 2015-05-22 16:08:11 theanets.trainer:168 RmsProp 130 loss=1187.511841 err=1187.511841
I 2015-05-22 16:08:11 theanets.trainer:168 validation 13 loss=3417.830811 err=3417.830811 *
I 2015-05-22 16:08:17 theanets.trainer:168 RmsProp 131 loss=1178.941528 err=1178.941528
I 2015-05-22 16:08:25 theanets.trainer:168 RmsProp 132 loss=1168.954834 err=1168.954834
I 2015-05-22 16:08:33 theanets.trainer:168 RmsProp 133 loss=1149.677856 err=1149.677856
I 2015-05-22 16:08:41 theanets.trainer:168 RmsProp 134 loss=1136.374146 err=1136.374146
I 2015-05-22 16:08:49 theanets.trainer:168 RmsProp 135 loss=1134.283936 err=1134.283936
I 2015-05-22 16:08:57 theanets.trainer:168 RmsProp 136 loss=1115.274536 err=1115.274536
I 2015-05-22 16:09:04 theanets.trainer:168 RmsProp 137 loss=1076.245850 err=1076.245850
I 2015-05-22 16:09:11 theanets.trainer:168 RmsProp 138 loss=1135.770264 err=1135.770264
I 2015-05-22 16:09:17 theanets.trainer:168 RmsProp 139 loss=1143.230225 err=1143.230225
I 2015-05-22 16:09:24 theanets.trainer:168 RmsProp 140 loss=1122.448608 err=1122.448608
I 2015-05-22 16:09:24 theanets.trainer:168 validation 14 loss=3490.427490 err=3490.427490
I 2015-05-22 16:09:30 theanets.trainer:168 RmsProp 141 loss=1070.684692 err=1070.684692
I 2015-05-22 16:09:37 theanets.trainer:168 RmsProp 142 loss=1074.598145 err=1074.598145
I 2015-05-22 16:09:45 theanets.trainer:168 RmsProp 143 loss=1086.005249 err=1086.005249
I 2015-05-22 16:09:52 theanets.trainer:168 RmsProp 144 loss=1069.234863 err=1069.234863
I 2015-05-22 16:09:59 theanets.trainer:168 RmsProp 145 loss=1049.921021 err=1049.921021
I 2015-05-22 16:10:07 theanets.trainer:168 RmsProp 146 loss=1038.164673 err=1038.164673
I 2015-05-22 16:10:14 theanets.trainer:168 RmsProp 147 loss=1005.678040 err=1005.678040
I 2015-05-22 16:10:21 theanets.trainer:168 RmsProp 148 loss=984.429749 err=984.429749
I 2015-05-22 16:10:27 theanets.trainer:168 RmsProp 149 loss=998.900146 err=998.900146
I 2015-05-22 16:10:34 theanets.trainer:168 RmsProp 150 loss=976.903992 err=976.903992
I 2015-05-22 16:10:34 theanets.trainer:168 validation 15 loss=3297.287109 err=3297.287109 *
I 2015-05-22 16:10:41 theanets.trainer:168 RmsProp 151 loss=970.152649 err=970.152649
I 2015-05-22 16:10:48 theanets.trainer:168 RmsProp 152 loss=988.030640 err=988.030640
I 2015-05-22 16:10:56 theanets.trainer:168 RmsProp 153 loss=975.621887 err=975.621887
I 2015-05-22 16:11:04 theanets.trainer:168 RmsProp 154 loss=962.365601 err=962.365601
I 2015-05-22 16:11:12 theanets.trainer:168 RmsProp 155 loss=958.591431 err=958.591431
I 2015-05-22 16:11:19 theanets.trainer:168 RmsProp 156 loss=934.444275 err=934.444275
I 2015-05-22 16:11:26 theanets.trainer:168 RmsProp 157 loss=925.109314 err=925.109314
I 2015-05-22 16:11:34 theanets.trainer:168 RmsProp 158 loss=910.134155 err=910.134155
I 2015-05-22 16:11:42 theanets.trainer:168 RmsProp 159 loss=916.744751 err=916.744751
I 2015-05-22 16:11:50 theanets.trainer:168 RmsProp 160 loss=895.946655 err=895.946655
I 2015-05-22 16:11:50 theanets.trainer:168 validation 16 loss=3243.926758 err=3243.926758 *
I 2015-05-22 16:11:57 theanets.trainer:168 RmsProp 161 loss=880.056702 err=880.056702
I 2015-05-22 16:12:05 theanets.trainer:168 RmsProp 162 loss=898.287292 err=898.287292
I 2015-05-22 16:12:13 theanets.trainer:168 RmsProp 163 loss=890.102173 err=890.102173
I 2015-05-22 16:12:21 theanets.trainer:168 RmsProp 164 loss=867.788818 err=867.788818
I 2015-05-22 16:12:29 theanets.trainer:168 RmsProp 165 loss=882.558777 err=882.558777
I 2015-05-22 16:12:36 theanets.trainer:168 RmsProp 166 loss=877.501587 err=877.501587
I 2015-05-22 16:12:44 theanets.trainer:168 RmsProp 167 loss=866.326904 err=866.326904
I 2015-05-22 16:12:50 theanets.trainer:168 RmsProp 168 loss=873.592957 err=873.592957
I 2015-05-22 16:12:56 theanets.trainer:168 RmsProp 169 loss=876.088745 err=876.088745
I 2015-05-22 16:13:03 theanets.trainer:168 RmsProp 170 loss=878.940979 err=878.940979
I 2015-05-22 16:13:03 theanets.trainer:168 validation 17 loss=3179.859375 err=3179.859375 *
I 2015-05-22 16:13:11 theanets.trainer:168 RmsProp 171 loss=842.860657 err=842.860657
I 2015-05-22 16:13:19 theanets.trainer:168 RmsProp 172 loss=875.003845 err=875.003845
I 2015-05-22 16:13:27 theanets.trainer:168 RmsProp 173 loss=825.893921 err=825.893921
I 2015-05-22 16:13:35 theanets.trainer:168 RmsProp 174 loss=823.777039 err=823.777039
I 2015-05-22 16:13:42 theanets.trainer:168 RmsProp 175 loss=847.595276 err=847.595276
I 2015-05-22 16:13:50 theanets.trainer:168 RmsProp 176 loss=829.783386 err=829.783386
I 2015-05-22 16:13:58 theanets.trainer:168 RmsProp 177 loss=814.558594 err=814.558594
I 2015-05-22 16:14:06 theanets.trainer:168 RmsProp 178 loss=823.534241 err=823.534241
I 2015-05-22 16:14:13 theanets.trainer:168 RmsProp 179 loss=811.849365 err=811.849365
I 2015-05-22 16:14:21 theanets.trainer:168 RmsProp 180 loss=813.145569 err=813.145569
I 2015-05-22 16:14:22 theanets.trainer:168 validation 18 loss=3184.310547 err=3184.310547
I 2015-05-22 16:14:29 theanets.trainer:168 RmsProp 181 loss=816.160828 err=816.160828
I 2015-05-22 16:14:37 theanets.trainer:168 RmsProp 182 loss=792.431030 err=792.431030
I 2015-05-22 16:14:45 theanets.trainer:168 RmsProp 183 loss=804.594727 err=804.594727
I 2015-05-22 16:14:53 theanets.trainer:168 RmsProp 184 loss=778.601685 err=778.601685
I 2015-05-22 16:15:00 theanets.trainer:168 RmsProp 185 loss=780.638306 err=780.638306
I 2015-05-22 16:15:07 theanets.trainer:168 RmsProp 186 loss=755.302612 err=755.302612
I 2015-05-22 16:15:15 theanets.trainer:168 RmsProp 187 loss=765.637024 err=765.637024
I 2015-05-22 16:15:23 theanets.trainer:168 RmsProp 188 loss=752.967773 err=752.967773
I 2015-05-22 16:15:31 theanets.trainer:168 RmsProp 189 loss=754.094666 err=754.094666
I 2015-05-22 16:15:39 theanets.trainer:168 RmsProp 190 loss=759.525391 err=759.525391
I 2015-05-22 16:15:39 theanets.trainer:168 validation 19 loss=3138.009766 err=3138.009766 *
I 2015-05-22 16:15:45 theanets.trainer:168 RmsProp 191 loss=749.603271 err=749.603271
I 2015-05-22 16:15:52 theanets.trainer:168 RmsProp 192 loss=756.800964 err=756.800964
I 2015-05-22 16:16:00 theanets.trainer:168 RmsProp 193 loss=728.433655 err=728.433655
I 2015-05-22 16:16:08 theanets.trainer:168 RmsProp 194 loss=713.314270 err=713.314270
I 2015-05-22 16:16:16 theanets.trainer:168 RmsProp 195 loss=695.800964 err=695.800964
I 2015-05-22 16:16:24 theanets.trainer:168 RmsProp 196 loss=711.811218 err=711.811218
I 2015-05-22 16:16:31 theanets.trainer:168 RmsProp 197 loss=709.908142 err=709.908142
I 2015-05-22 16:16:39 theanets.trainer:168 RmsProp 198 loss=692.490540 err=692.490540
I 2015-05-22 16:16:47 theanets.trainer:168 RmsProp 199 loss=696.883423 err=696.883423
I 2015-05-22 16:16:55 theanets.trainer:168 RmsProp 200 loss=689.598999 err=689.598999
I 2015-05-22 16:16:56 theanets.trainer:168 validation 20 loss=3080.489014 err=3080.489014 *
I 2015-05-22 16:17:02 theanets.trainer:168 RmsProp 201 loss=675.018494 err=675.018494
I 2015-05-22 16:17:08 theanets.trainer:168 RmsProp 202 loss=678.898621 err=678.898621
I 2015-05-22 16:17:15 theanets.trainer:168 RmsProp 203 loss=664.489990 err=664.489990
I 2015-05-22 16:17:22 theanets.trainer:168 RmsProp 204 loss=680.053650 err=680.053650
I 2015-05-22 16:17:29 theanets.trainer:168 RmsProp 205 loss=664.509094 err=664.509094
I 2015-05-22 16:17:37 theanets.trainer:168 RmsProp 206 loss=700.343445 err=700.343445
I 2015-05-22 16:17:44 theanets.trainer:168 RmsProp 207 loss=681.874146 err=681.874146
I 2015-05-22 16:17:50 theanets.trainer:168 RmsProp 208 loss=656.682617 err=656.682617
I 2015-05-22 16:17:56 theanets.trainer:168 RmsProp 209 loss=641.240173 err=641.240173
I 2015-05-22 16:18:03 theanets.trainer:168 RmsProp 210 loss=636.496094 err=636.496094
I 2015-05-22 16:18:03 theanets.trainer:168 validation 21 loss=3008.385498 err=3008.385498 *
I 2015-05-22 16:18:11 theanets.trainer:168 RmsProp 211 loss=643.760803 err=643.760803
I 2015-05-22 16:18:18 theanets.trainer:168 RmsProp 212 loss=655.967346 err=655.967346
I 2015-05-22 16:18:25 theanets.trainer:168 RmsProp 213 loss=657.686096 err=657.686096
I 2015-05-22 16:18:32 theanets.trainer:168 RmsProp 214 loss=627.112061 err=627.112061
I 2015-05-22 16:18:39 theanets.trainer:168 RmsProp 215 loss=629.226746 err=629.226746
I 2015-05-22 16:18:46 theanets.trainer:168 RmsProp 216 loss=654.505615 err=654.505615
I 2015-05-22 16:18:53 theanets.trainer:168 RmsProp 217 loss=621.718689 err=621.718689
I 2015-05-22 16:19:00 theanets.trainer:168 RmsProp 218 loss=622.263672 err=622.263672
I 2015-05-22 16:19:08 theanets.trainer:168 RmsProp 219 loss=611.254517 err=611.254517
I 2015-05-22 16:19:16 theanets.trainer:168 RmsProp 220 loss=609.654053 err=609.654053
I 2015-05-22 16:19:16 theanets.trainer:168 validation 22 loss=3047.869873 err=3047.869873
I 2015-05-22 16:19:22 theanets.trainer:168 RmsProp 221 loss=603.711731 err=603.711731
I 2015-05-22 16:19:29 theanets.trainer:168 RmsProp 222 loss=607.850830 err=607.850830
I 2015-05-22 16:19:37 theanets.trainer:168 RmsProp 223 loss=593.242554 err=593.242554
I 2015-05-22 16:19:45 theanets.trainer:168 RmsProp 224 loss=578.150696 err=578.150696
I 2015-05-22 16:19:51 theanets.trainer:168 RmsProp 225 loss=588.553467 err=588.553467
I 2015-05-22 16:19:59 theanets.trainer:168 RmsProp 226 loss=583.317444 err=583.317444
I 2015-05-22 16:20:05 theanets.trainer:168 RmsProp 227 loss=585.743042 err=585.743042
I 2015-05-22 16:20:12 theanets.trainer:168 RmsProp 228 loss=573.919983 err=573.919983
I 2015-05-22 16:20:18 theanets.trainer:168 RmsProp 229 loss=567.679138 err=567.679138
I 2015-05-22 16:20:24 theanets.trainer:168 RmsProp 230 loss=566.735474 err=566.735474
I 2015-05-22 16:20:25 theanets.trainer:168 validation 23 loss=3001.756104 err=3001.756104 *
I 2015-05-22 16:20:33 theanets.trainer:168 RmsProp 231 loss=562.984253 err=562.984253
I 2015-05-22 16:20:40 theanets.trainer:168 RmsProp 232 loss=557.024475 err=557.024475
I 2015-05-22 16:20:48 theanets.trainer:168 RmsProp 233 loss=582.860107 err=582.860107
I 2015-05-22 16:20:55 theanets.trainer:168 RmsProp 234 loss=585.419189 err=585.419189
I 2015-05-22 16:21:03 theanets.trainer:168 RmsProp 235 loss=568.577698 err=568.577698
I 2015-05-22 16:21:11 theanets.trainer:168 RmsProp 236 loss=556.712524 err=556.712524
I 2015-05-22 16:21:17 theanets.trainer:168 RmsProp 237 loss=537.821899 err=537.821899
I 2015-05-22 16:21:25 theanets.trainer:168 RmsProp 238 loss=532.234009 err=532.234009
I 2015-05-22 16:21:33 theanets.trainer:168 RmsProp 239 loss=532.329224 err=532.329224
I 2015-05-22 16:21:41 theanets.trainer:168 RmsProp 240 loss=532.910767 err=532.910767
I 2015-05-22 16:21:41 theanets.trainer:168 validation 24 loss=2926.044678 err=2926.044678 *
I 2015-05-22 16:21:48 theanets.trainer:168 RmsProp 241 loss=524.314087 err=524.314087
I 2015-05-22 16:21:54 theanets.trainer:168 RmsProp 242 loss=536.058899 err=536.058899
I 2015-05-22 16:22:00 theanets.trainer:168 RmsProp 243 loss=540.498779 err=540.498779
I 2015-05-22 16:22:07 theanets.trainer:168 RmsProp 244 loss=534.377014 err=534.377014
I 2015-05-22 16:22:15 theanets.trainer:168 RmsProp 245 loss=529.272034 err=529.272034
I 2015-05-22 16:22:23 theanets.trainer:168 RmsProp 246 loss=523.745544 err=523.745544
I 2015-05-22 16:22:29 theanets.trainer:168 RmsProp 247 loss=525.891724 err=525.891724
I 2015-05-22 16:22:36 theanets.trainer:168 RmsProp 248 loss=510.863708 err=510.863708
I 2015-05-22 16:22:42 theanets.trainer:168 RmsProp 249 loss=501.644196 err=501.644196
I 2015-05-22 16:22:49 theanets.trainer:168 RmsProp 250 loss=514.416077 err=514.416077
I 2015-05-22 16:22:49 theanets.trainer:168 validation 25 loss=2920.530518 err=2920.530518 *
I 2015-05-22 16:22:57 theanets.trainer:168 RmsProp 251 loss=507.741241 err=507.741241
I 2015-05-22 16:23:05 theanets.trainer:168 RmsProp 252 loss=509.464691 err=509.464691
I 2015-05-22 16:23:13 theanets.trainer:168 RmsProp 253 loss=499.116241 err=499.116241
I 2015-05-22 16:23:21 theanets.trainer:168 RmsProp 254 loss=500.577606 err=500.577606
I 2015-05-22 16:23:28 theanets.trainer:168 RmsProp 255 loss=495.495880 err=495.495880
I 2015-05-22 16:23:36 theanets.trainer:168 RmsProp 256 loss=500.714203 err=500.714203
I 2015-05-22 16:23:43 theanets.trainer:168 RmsProp 257 loss=491.017578 err=491.017578
I 2015-05-22 16:23:49 theanets.trainer:168 RmsProp 258 loss=483.577759 err=483.577759
I 2015-05-22 16:23:56 theanets.trainer:168 RmsProp 259 loss=476.857941 err=476.857941
I 2015-05-22 16:24:02 theanets.trainer:168 RmsProp 260 loss=504.451447 err=504.451447
I 2015-05-22 16:24:02 theanets.trainer:168 validation 26 loss=2932.775635 err=2932.775635
I 2015-05-22 16:24:10 theanets.trainer:168 RmsProp 261 loss=502.916992 err=502.916992
I 2015-05-22 16:24:18 theanets.trainer:168 RmsProp 262 loss=492.172180 err=492.172180
I 2015-05-22 16:24:24 theanets.trainer:168 RmsProp 263 loss=496.344727 err=496.344727
I 2015-05-22 16:24:31 theanets.trainer:168 RmsProp 264 loss=483.145416 err=483.145416
I 2015-05-22 16:24:38 theanets.trainer:168 RmsProp 265 loss=482.388153 err=482.388153
I 2015-05-22 16:24:44 theanets.trainer:168 RmsProp 266 loss=492.119751 err=492.119751
I 2015-05-22 16:24:51 theanets.trainer:168 RmsProp 267 loss=476.161530 err=476.161530
I 2015-05-22 16:24:58 theanets.trainer:168 RmsProp 268 loss=456.775574 err=456.775574
I 2015-05-22 16:25:06 theanets.trainer:168 RmsProp 269 loss=461.524017 err=461.524017
I 2015-05-22 16:25:14 theanets.trainer:168 RmsProp 270 loss=461.060608 err=461.060608
I 2015-05-22 16:25:14 theanets.trainer:168 validation 27 loss=2993.868408 err=2993.868408
I 2015-05-22 16:25:21 theanets.trainer:168 RmsProp 271 loss=470.919800 err=470.919800
I 2015-05-22 16:25:28 theanets.trainer:168 RmsProp 272 loss=466.114532 err=466.114532
I 2015-05-22 16:25:35 theanets.trainer:168 RmsProp 273 loss=450.771393 err=450.771393
I 2015-05-22 16:25:42 theanets.trainer:168 RmsProp 274 loss=458.729767 err=458.729767
I 2015-05-22 16:25:48 theanets.trainer:168 RmsProp 275 loss=447.359100 err=447.359100
I 2015-05-22 16:25:55 theanets.trainer:168 RmsProp 276 loss=452.071320 err=452.071320
I 2015-05-22 16:26:02 theanets.trainer:168 RmsProp 277 loss=474.519745 err=474.519745
I 2015-05-22 16:26:10 theanets.trainer:168 RmsProp 278 loss=473.359833 err=473.359833
I 2015-05-22 16:26:18 theanets.trainer:168 RmsProp 279 loss=457.286407 err=457.286407
I 2015-05-22 16:26:26 theanets.trainer:168 RmsProp 280 loss=449.699615 err=449.699615
I 2015-05-22 16:26:26 theanets.trainer:168 validation 28 loss=2916.841553 err=2916.841553 *
I 2015-05-22 16:26:33 theanets.trainer:168 RmsProp 281 loss=448.303040 err=448.303040
I 2015-05-22 16:26:40 theanets.trainer:168 RmsProp 282 loss=445.467957 err=445.467957
I 2015-05-22 16:26:48 theanets.trainer:168 RmsProp 283 loss=445.607849 err=445.607849
I 2015-05-22 16:26:56 theanets.trainer:168 RmsProp 284 loss=436.271362 err=436.271362
I 2015-05-22 16:27:04 theanets.trainer:168 RmsProp 285 loss=435.506683 err=435.506683
I 2015-05-22 16:27:12 theanets.trainer:168 RmsProp 286 loss=442.348816 err=442.348816
I 2015-05-22 16:27:19 theanets.trainer:168 RmsProp 287 loss=449.655579 err=449.655579
I 2015-05-22 16:27:27 theanets.trainer:168 RmsProp 288 loss=434.373077 err=434.373077
I 2015-05-22 16:27:33 theanets.trainer:168 RmsProp 289 loss=432.571930 err=432.571930
I 2015-05-22 16:27:41 theanets.trainer:168 RmsProp 290 loss=437.434235 err=437.434235
I 2015-05-22 16:27:41 theanets.trainer:168 validation 29 loss=2939.028564 err=2939.028564
I 2015-05-22 16:27:48 theanets.trainer:168 RmsProp 291 loss=438.044220 err=438.044220
I 2015-05-22 16:27:56 theanets.trainer:168 RmsProp 292 loss=432.756226 err=432.756226
I 2015-05-22 16:28:04 theanets.trainer:168 RmsProp 293 loss=418.856415 err=418.856415
I 2015-05-22 16:28:12 theanets.trainer:168 RmsProp 294 loss=426.484253 err=426.484253
I 2015-05-22 16:28:20 theanets.trainer:168 RmsProp 295 loss=406.508606 err=406.508606
I 2015-05-22 16:28:27 theanets.trainer:168 RmsProp 296 loss=429.669586 err=429.669586
I 2015-05-22 16:28:34 theanets.trainer:168 RmsProp 297 loss=418.416077 err=418.416077
I 2015-05-22 16:28:41 theanets.trainer:168 RmsProp 298 loss=406.216339 err=406.216339
I 2015-05-22 16:28:48 theanets.trainer:168 RmsProp 299 loss=404.858917 err=404.858917
I 2015-05-22 16:28:54 theanets.trainer:168 RmsProp 300 loss=415.433655 err=415.433655
I 2015-05-22 16:28:55 theanets.trainer:168 validation 30 loss=2928.825195 err=2928.825195
I 2015-05-22 16:29:02 theanets.trainer:168 RmsProp 301 loss=407.220917 err=407.220917
I 2015-05-22 16:29:09 theanets.trainer:168 RmsProp 302 loss=401.440369 err=401.440369
I 2015-05-22 16:29:15 theanets.trainer:168 RmsProp 303 loss=403.410767 err=403.410767
I 2015-05-22 16:29:21 theanets.trainer:168 RmsProp 304 loss=391.570618 err=391.570618
I 2015-05-22 16:29:28 theanets.trainer:168 RmsProp 305 loss=390.152802 err=390.152802
I 2015-05-22 16:29:34 theanets.trainer:168 RmsProp 306 loss=388.863770 err=388.863770
I 2015-05-22 16:29:42 theanets.trainer:168 RmsProp 307 loss=394.123077 err=394.123077
I 2015-05-22 16:29:50 theanets.trainer:168 RmsProp 308 loss=404.102020 err=404.102020
I 2015-05-22 16:29:58 theanets.trainer:168 RmsProp 309 loss=391.725372 err=391.725372
I 2015-05-22 16:30:05 theanets.trainer:168 RmsProp 310 loss=398.446838 err=398.446838
I 2015-05-22 16:30:06 theanets.trainer:168 validation 31 loss=3017.889893 err=3017.889893
I 2015-05-22 16:30:13 theanets.trainer:168 RmsProp 311 loss=397.871979 err=397.871979
I 2015-05-22 16:30:21 theanets.trainer:168 RmsProp 312 loss=407.613678 err=407.613678
I 2015-05-22 16:30:28 theanets.trainer:168 RmsProp 313 loss=407.265961 err=407.265961
I 2015-05-22 16:30:35 theanets.trainer:168 RmsProp 314 loss=391.811676 err=391.811676
I 2015-05-22 16:30:41 theanets.trainer:168 RmsProp 315 loss=395.492828 err=395.492828
I 2015-05-22 16:30:48 theanets.trainer:168 RmsProp 316 loss=390.257782 err=390.257782
I 2015-05-22 16:30:55 theanets.trainer:168 RmsProp 317 loss=380.594421 err=380.594421
I 2015-05-22 16:31:03 theanets.trainer:168 RmsProp 318 loss=376.824097 err=376.824097
I 2015-05-22 16:31:11 theanets.trainer:168 RmsProp 319 loss=379.166595 err=379.166595
I 2015-05-22 16:31:19 theanets.trainer:168 RmsProp 320 loss=377.026215 err=377.026215
I 2015-05-22 16:31:19 theanets.trainer:168 validation 32 loss=2950.437256 err=2950.437256
I 2015-05-22 16:31:25 theanets.trainer:168 RmsProp 321 loss=372.342560 err=372.342560
I 2015-05-22 16:31:32 theanets.trainer:168 RmsProp 322 loss=362.232239 err=362.232239
I 2015-05-22 16:31:40 theanets.trainer:168 RmsProp 323 loss=368.924255 err=368.924255
I 2015-05-22 16:31:48 theanets.trainer:168 RmsProp 324 loss=362.364136 err=362.364136
I 2015-05-22 16:31:55 theanets.trainer:168 RmsProp 325 loss=361.783661 err=361.783661
I 2015-05-22 16:32:02 theanets.trainer:168 RmsProp 326 loss=365.074738 err=365.074738
I 2015-05-22 16:32:09 theanets.trainer:168 RmsProp 327 loss=366.207123 err=366.207123
I 2015-05-22 16:32:17 theanets.trainer:168 RmsProp 328 loss=366.129486 err=366.129486
I 2015-05-22 16:32:25 theanets.trainer:168 RmsProp 329 loss=362.442108 err=362.442108
I 2015-05-22 16:32:33 theanets.trainer:168 RmsProp 330 loss=350.950653 err=350.950653
I 2015-05-22 16:32:33 theanets.trainer:168 validation 33 loss=2900.480713 err=2900.480713 *
I 2015-05-22 16:32:39 theanets.trainer:168 RmsProp 331 loss=355.187622 err=355.187622
I 2015-05-22 16:32:46 theanets.trainer:168 RmsProp 332 loss=356.926758 err=356.926758
I 2015-05-22 16:32:52 theanets.trainer:168 RmsProp 333 loss=349.668579 err=349.668579
I 2015-05-22 16:32:58 theanets.trainer:168 RmsProp 334 loss=347.639069 err=347.639069
I 2015-05-22 16:33:06 theanets.trainer:168 RmsProp 335 loss=357.731049 err=357.731049
I 2015-05-22 16:33:14 theanets.trainer:168 RmsProp 336 loss=350.488159 err=350.488159
I 2015-05-22 16:33:20 theanets.trainer:168 RmsProp 337 loss=354.108185 err=354.108185
I 2015-05-22 16:33:27 theanets.trainer:168 RmsProp 338 loss=344.832123 err=344.832123
I 2015-05-22 16:33:33 theanets.trainer:168 RmsProp 339 loss=338.308777 err=338.308777
I 2015-05-22 16:33:40 theanets.trainer:168 RmsProp 340 loss=337.969513 err=337.969513
I 2015-05-22 16:33:40 theanets.trainer:168 validation 34 loss=2922.383057 err=2922.383057
I 2015-05-22 16:33:47 theanets.trainer:168 RmsProp 341 loss=341.606476 err=341.606476
I 2015-05-22 16:33:55 theanets.trainer:168 RmsProp 342 loss=339.705475 err=339.705475
I 2015-05-22 16:34:03 theanets.trainer:168 RmsProp 343 loss=343.380859 err=343.380859
I 2015-05-22 16:34:11 theanets.trainer:168 RmsProp 344 loss=342.023621 err=342.023621
I 2015-05-22 16:34:19 theanets.trainer:168 RmsProp 345 loss=337.463440 err=337.463440
I 2015-05-22 16:34:27 theanets.trainer:168 RmsProp 346 loss=327.884094 err=327.884094
I 2015-05-22 16:34:33 theanets.trainer:168 RmsProp 347 loss=331.719269 err=331.719269
I 2015-05-22 16:34:39 theanets.trainer:168 RmsProp 348 loss=323.560272 err=323.560272
I 2015-05-22 16:34:46 theanets.trainer:168 RmsProp 349 loss=331.610992 err=331.610992
I 2015-05-22 16:34:54 theanets.trainer:168 RmsProp 350 loss=322.334595 err=322.334595
I 2015-05-22 16:34:54 theanets.trainer:168 validation 35 loss=2931.328857 err=2931.328857
I 2015-05-22 16:35:01 theanets.trainer:168 RmsProp 351 loss=323.860168 err=323.860168
I 2015-05-22 16:35:09 theanets.trainer:168 RmsProp 352 loss=330.023804 err=330.023804
I 2015-05-22 16:35:16 theanets.trainer:168 RmsProp 353 loss=338.967285 err=338.967285
I 2015-05-22 16:35:23 theanets.trainer:168 RmsProp 354 loss=340.980377 err=340.980377
I 2015-05-22 16:35:30 theanets.trainer:168 RmsProp 355 loss=323.499878 err=323.499878
I 2015-05-22 16:35:37 theanets.trainer:168 RmsProp 356 loss=329.465363 err=329.465363
I 2015-05-22 16:35:44 theanets.trainer:168 RmsProp 357 loss=315.987244 err=315.987244
I 2015-05-22 16:35:51 theanets.trainer:168 RmsProp 358 loss=352.022736 err=352.022736
I 2015-05-22 16:35:59 theanets.trainer:168 RmsProp 359 loss=376.364807 err=376.364807
I 2015-05-22 16:36:07 theanets.trainer:168 RmsProp 360 loss=334.702301 err=334.702301
I 2015-05-22 16:36:07 theanets.trainer:168 validation 36 loss=2891.016357 err=2891.016357 *
I 2015-05-22 16:36:13 theanets.trainer:168 RmsProp 361 loss=320.172974 err=320.172974
I 2015-05-22 16:36:20 theanets.trainer:168 RmsProp 362 loss=309.841370 err=309.841370
I 2015-05-22 16:36:26 theanets.trainer:168 RmsProp 363 loss=312.185028 err=312.185028
I 2015-05-22 16:36:33 theanets.trainer:168 RmsProp 364 loss=304.525879 err=304.525879
I 2015-05-22 16:36:39 theanets.trainer:168 RmsProp 365 loss=301.679413 err=301.679413
I 2015-05-22 16:36:47 theanets.trainer:168 RmsProp 366 loss=298.057739 err=298.057739
I 2015-05-22 16:36:53 theanets.trainer:168 RmsProp 367 loss=292.037903 err=292.037903
I 2015-05-22 16:37:00 theanets.trainer:168 RmsProp 368 loss=292.059387 err=292.059387
I 2015-05-22 16:37:06 theanets.trainer:168 RmsProp 369 loss=292.697327 err=292.697327
I 2015-05-22 16:37:12 theanets.trainer:168 RmsProp 370 loss=282.127411 err=282.127411
I 2015-05-22 16:37:13 theanets.trainer:168 validation 37 loss=2819.789795 err=2819.789795 *
I 2015-05-22 16:37:21 theanets.trainer:168 RmsProp 371 loss=288.413727 err=288.413727
I 2015-05-22 16:37:29 theanets.trainer:168 RmsProp 372 loss=287.551208 err=287.551208
I 2015-05-22 16:37:37 theanets.trainer:168 RmsProp 373 loss=289.228363 err=289.228363
I 2015-05-22 16:37:44 theanets.trainer:168 RmsProp 374 loss=298.882782 err=298.882782
I 2015-05-22 16:37:52 theanets.trainer:168 RmsProp 375 loss=295.891174 err=295.891174
I 2015-05-22 16:37:59 theanets.trainer:168 RmsProp 376 loss=288.590027 err=288.590027
I 2015-05-22 16:38:07 theanets.trainer:168 RmsProp 377 loss=279.235443 err=279.235443
I 2015-05-22 16:38:15 theanets.trainer:168 RmsProp 378 loss=272.791199 err=272.791199
I 2015-05-22 16:38:23 theanets.trainer:168 RmsProp 379 loss=280.179291 err=280.179291
I 2015-05-22 16:38:30 theanets.trainer:168 RmsProp 380 loss=272.269348 err=272.269348
I 2015-05-22 16:38:31 theanets.trainer:168 validation 38 loss=2815.655029 err=2815.655029 *
I 2015-05-22 16:38:37 theanets.trainer:168 RmsProp 381 loss=274.556152 err=274.556152
I 2015-05-22 16:38:45 theanets.trainer:168 RmsProp 382 loss=276.196350 err=276.196350
I 2015-05-22 16:38:53 theanets.trainer:168 RmsProp 383 loss=267.820892 err=267.820892
I 2015-05-22 16:39:01 theanets.trainer:168 RmsProp 384 loss=266.049652 err=266.049652
I 2015-05-22 16:39:08 theanets.trainer:168 RmsProp 385 loss=273.373322 err=273.373322
I 2015-05-22 16:39:14 theanets.trainer:168 RmsProp 386 loss=273.533569 err=273.533569
I 2015-05-22 16:39:21 theanets.trainer:168 RmsProp 387 loss=264.221741 err=264.221741
I 2015-05-22 16:39:28 theanets.trainer:168 RmsProp 388 loss=264.471802 err=264.471802
I 2015-05-22 16:39:34 theanets.trainer:168 RmsProp 389 loss=256.185638 err=256.185638
I 2015-05-22 16:39:41 theanets.trainer:168 RmsProp 390 loss=262.380646 err=262.380646
I 2015-05-22 16:39:41 theanets.trainer:168 validation 39 loss=2810.668701 err=2810.668701 *
I 2015-05-22 16:39:48 theanets.trainer:168 RmsProp 391 loss=254.689697 err=254.689697
I 2015-05-22 16:39:56 theanets.trainer:168 RmsProp 392 loss=255.566101 err=255.566101
I 2015-05-22 16:40:04 theanets.trainer:168 RmsProp 393 loss=259.025391 err=259.025391
I 2015-05-22 16:40:12 theanets.trainer:168 RmsProp 394 loss=252.445526 err=252.445526
I 2015-05-22 16:40:20 theanets.trainer:168 RmsProp 395 loss=252.857529 err=252.857529
I 2015-05-22 16:40:28 theanets.trainer:168 RmsProp 396 loss=250.548309 err=250.548309
I 2015-05-22 16:40:34 theanets.trainer:168 RmsProp 397 loss=247.995682 err=247.995682
I 2015-05-22 16:40:41 theanets.trainer:168 RmsProp 398 loss=246.417999 err=246.417999
I 2015-05-22 16:40:47 theanets.trainer:168 RmsProp 399 loss=250.042831 err=250.042831
I 2015-05-22 16:40:54 theanets.trainer:168 RmsProp 400 loss=245.350922 err=245.350922
I 2015-05-22 16:40:54 theanets.trainer:168 validation 40 loss=2781.083252 err=2781.083252 *
I 2015-05-22 16:41:02 theanets.trainer:168 RmsProp 401 loss=246.028122 err=246.028122
I 2015-05-22 16:41:10 theanets.trainer:168 RmsProp 402 loss=249.313400 err=249.313400
I 2015-05-22 16:41:18 theanets.trainer:168 RmsProp 403 loss=247.474396 err=247.474396
I 2015-05-22 16:41:26 theanets.trainer:168 RmsProp 404 loss=246.823593 err=246.823593
I 2015-05-22 16:41:34 theanets.trainer:168 RmsProp 405 loss=249.948547 err=249.948547
I 2015-05-22 16:41:42 theanets.trainer:168 RmsProp 406 loss=239.913788 err=239.913788
I 2015-05-22 16:41:48 theanets.trainer:168 RmsProp 407 loss=239.708572 err=239.708572
I 2015-05-22 16:41:54 theanets.trainer:168 RmsProp 408 loss=235.951263 err=235.951263
I 2015-05-22 16:42:01 theanets.trainer:168 RmsProp 409 loss=238.625916 err=238.625916
I 2015-05-22 16:42:07 theanets.trainer:168 RmsProp 410 loss=244.979401 err=244.979401
I 2015-05-22 16:42:07 theanets.trainer:168 validation 41 loss=2799.167725 err=2799.167725
I 2015-05-22 16:42:15 theanets.trainer:168 RmsProp 411 loss=236.858154 err=236.858154
I 2015-05-22 16:42:23 theanets.trainer:168 RmsProp 412 loss=237.121307 err=237.121307
I 2015-05-22 16:42:31 theanets.trainer:168 RmsProp 413 loss=237.219833 err=237.219833
I 2015-05-22 16:42:39 theanets.trainer:168 RmsProp 414 loss=233.966202 err=233.966202
I 2015-05-22 16:42:47 theanets.trainer:168 RmsProp 415 loss=230.118469 err=230.118469
I 2015-05-22 16:42:55 theanets.trainer:168 RmsProp 416 loss=227.018463 err=227.018463
I 2015-05-22 16:43:02 theanets.trainer:168 RmsProp 417 loss=229.883957 err=229.883957
I 2015-05-22 16:43:08 theanets.trainer:168 RmsProp 418 loss=227.243134 err=227.243134
I 2015-05-22 16:43:14 theanets.trainer:168 RmsProp 419 loss=234.014297 err=234.014297
I 2015-05-22 16:43:21 theanets.trainer:168 RmsProp 420 loss=248.960632 err=248.960632
I 2015-05-22 16:43:21 theanets.trainer:168 validation 42 loss=2909.574951 err=2909.574951
I 2015-05-22 16:43:29 theanets.trainer:168 RmsProp 421 loss=242.239990 err=242.239990
I 2015-05-22 16:43:37 theanets.trainer:168 RmsProp 422 loss=234.841507 err=234.841507
I 2015-05-22 16:43:45 theanets.trainer:168 RmsProp 423 loss=229.714264 err=229.714264
I 2015-05-22 16:43:53 theanets.trainer:168 RmsProp 424 loss=227.612244 err=227.612244
I 2015-05-22 16:44:01 theanets.trainer:168 RmsProp 425 loss=225.095200 err=225.095200
I 2015-05-22 16:44:09 theanets.trainer:168 RmsProp 426 loss=225.635223 err=225.635223
I 2015-05-22 16:44:16 theanets.trainer:168 RmsProp 427 loss=220.240387 err=220.240387
I 2015-05-22 16:44:22 theanets.trainer:168 RmsProp 428 loss=222.296249 err=222.296249
I 2015-05-22 16:44:28 theanets.trainer:168 RmsProp 429 loss=220.360382 err=220.360382
I 2015-05-22 16:44:35 theanets.trainer:168 RmsProp 430 loss=225.234390 err=225.234390
I 2015-05-22 16:44:36 theanets.trainer:168 validation 43 loss=2791.549805 err=2791.549805
I 2015-05-22 16:44:42 theanets.trainer:168 RmsProp 431 loss=229.766800 err=229.766800
I 2015-05-22 16:44:48 theanets.trainer:168 RmsProp 432 loss=220.768860 err=220.768860
I 2015-05-22 16:44:55 theanets.trainer:168 RmsProp 433 loss=218.315903 err=218.315903
I 2015-05-22 16:45:02 theanets.trainer:168 RmsProp 434 loss=222.285583 err=222.285583
I 2015-05-22 16:45:08 theanets.trainer:168 RmsProp 435 loss=223.076370 err=223.076370
I 2015-05-22 16:45:15 theanets.trainer:168 RmsProp 436 loss=217.278030 err=217.278030
I 2015-05-22 16:45:22 theanets.trainer:168 RmsProp 437 loss=214.750031 err=214.750031
I 2015-05-22 16:45:30 theanets.trainer:168 RmsProp 438 loss=212.002884 err=212.002884
I 2015-05-22 16:45:38 theanets.trainer:168 RmsProp 439 loss=212.039993 err=212.039993
I 2015-05-22 16:45:46 theanets.trainer:168 RmsProp 440 loss=209.063431 err=209.063431
I 2015-05-22 16:45:47 theanets.trainer:168 validation 44 loss=2759.359619 err=2759.359619 *
I 2015-05-22 16:45:53 theanets.trainer:168 RmsProp 441 loss=207.093658 err=207.093658
I 2015-05-22 16:45:59 theanets.trainer:168 RmsProp 442 loss=209.630630 err=209.630630
I 2015-05-22 16:46:06 theanets.trainer:168 RmsProp 443 loss=204.529968 err=204.529968
I 2015-05-22 16:46:12 theanets.trainer:168 RmsProp 444 loss=206.112839 err=206.112839
I 2015-05-22 16:46:18 theanets.trainer:168 RmsProp 445 loss=204.292175 err=204.292175
I 2015-05-22 16:46:25 theanets.trainer:168 RmsProp 446 loss=203.458221 err=203.458221
I 2015-05-22 16:46:32 theanets.trainer:168 RmsProp 447 loss=208.470795 err=208.470795
I 2015-05-22 16:46:40 theanets.trainer:168 RmsProp 448 loss=203.596863 err=203.596863
I 2015-05-22 16:46:49 theanets.trainer:168 RmsProp 449 loss=206.640350 err=206.640350
I 2015-05-22 16:46:57 theanets.trainer:168 RmsProp 450 loss=207.346390 err=207.346390
I 2015-05-22 16:46:57 theanets.trainer:168 validation 45 loss=2762.786865 err=2762.786865
I 2015-05-22 16:47:03 theanets.trainer:168 RmsProp 451 loss=202.244308 err=202.244308
I 2015-05-22 16:47:11 theanets.trainer:168 RmsProp 452 loss=195.511047 err=195.511047
I 2015-05-22 16:47:19 theanets.trainer:168 RmsProp 453 loss=205.485123 err=205.485123
I 2015-05-22 16:47:26 theanets.trainer:168 RmsProp 454 loss=204.309906 err=204.309906
I 2015-05-22 16:47:34 theanets.trainer:168 RmsProp 455 loss=209.067291 err=209.067291
I 2015-05-22 16:47:42 theanets.trainer:168 RmsProp 456 loss=204.057602 err=204.057602
I 2015-05-22 16:47:48 theanets.trainer:168 RmsProp 457 loss=202.238068 err=202.238068
I 2015-05-22 16:47:55 theanets.trainer:168 RmsProp 458 loss=199.970154 err=199.970154
I 2015-05-22 16:48:01 theanets.trainer:168 RmsProp 459 loss=197.719742 err=197.719742
I 2015-05-22 16:48:07 theanets.trainer:168 RmsProp 460 loss=199.969894 err=199.969894
I 2015-05-22 16:48:08 theanets.trainer:168 validation 46 loss=2784.461670 err=2784.461670
I 2015-05-22 16:48:16 theanets.trainer:168 RmsProp 461 loss=198.745178 err=198.745178
I 2015-05-22 16:48:23 theanets.trainer:168 RmsProp 462 loss=194.673004 err=194.673004
I 2015-05-22 16:48:31 theanets.trainer:168 RmsProp 463 loss=191.070221 err=191.070221
I 2015-05-22 16:48:39 theanets.trainer:168 RmsProp 464 loss=197.681732 err=197.681732
I 2015-05-22 16:48:46 theanets.trainer:168 RmsProp 465 loss=273.190155 err=273.190155
I 2015-05-22 16:48:54 theanets.trainer:168 RmsProp 466 loss=397.207153 err=397.207153
I 2015-05-22 16:49:01 theanets.trainer:168 RmsProp 467 loss=347.435394 err=347.435394
I 2015-05-22 16:49:08 theanets.trainer:168 RmsProp 468 loss=338.563080 err=338.563080
I 2015-05-22 16:49:14 theanets.trainer:168 RmsProp 469 loss=339.861176 err=339.861176
I 2015-05-22 16:49:22 theanets.trainer:168 RmsProp 470 loss=329.033295 err=329.033295
I 2015-05-22 16:49:22 theanets.trainer:168 validation 47 loss=2901.681641 err=2901.681641
I 2015-05-22 16:49:29 theanets.trainer:168 RmsProp 471 loss=317.277771 err=317.277771
I 2015-05-22 16:49:35 theanets.trainer:168 RmsProp 472 loss=369.178284 err=369.178284
I 2015-05-22 16:49:42 theanets.trainer:168 RmsProp 473 loss=330.748566 err=330.748566
I 2015-05-22 16:49:48 theanets.trainer:168 RmsProp 474 loss=319.755310 err=319.755310
I 2015-05-22 16:49:54 theanets.trainer:168 RmsProp 475 loss=315.012817 err=315.012817
I 2015-05-22 16:50:01 theanets.trainer:168 RmsProp 476 loss=308.528046 err=308.528046
I 2015-05-22 16:50:08 theanets.trainer:168 RmsProp 477 loss=301.544250 err=301.544250
I 2015-05-22 16:50:16 theanets.trainer:168 RmsProp 478 loss=320.540100 err=320.540100
I 2015-05-22 16:50:23 theanets.trainer:168 RmsProp 479 loss=329.084351 err=329.084351
I 2015-05-22 16:50:30 theanets.trainer:168 RmsProp 480 loss=312.330994 err=312.330994
I 2015-05-22 16:50:30 theanets.trainer:168 validation 48 loss=2959.475342 err=2959.475342
I 2015-05-22 16:50:37 theanets.trainer:168 RmsProp 481 loss=314.682434 err=314.682434
I 2015-05-22 16:50:44 theanets.trainer:168 RmsProp 482 loss=309.315338 err=309.315338
I 2015-05-22 16:50:52 theanets.trainer:168 RmsProp 483 loss=319.245300 err=319.245300
I 2015-05-22 16:50:59 theanets.trainer:168 RmsProp 484 loss=312.484955 err=312.484955
I 2015-05-22 16:51:06 theanets.trainer:168 RmsProp 485 loss=320.379547 err=320.379547
I 2015-05-22 16:51:13 theanets.trainer:168 RmsProp 486 loss=301.781769 err=301.781769
I 2015-05-22 16:51:20 theanets.trainer:168 RmsProp 487 loss=234.535339 err=234.535339
I 2015-05-22 16:51:27 theanets.trainer:168 RmsProp 488 loss=212.668427 err=212.668427
I 2015-05-22 16:51:34 theanets.trainer:168 RmsProp 489 loss=203.967697 err=203.967697
I 2015-05-22 16:51:41 theanets.trainer:168 RmsProp 490 loss=206.938248 err=206.938248
I 2015-05-22 16:51:41 theanets.trainer:168 validation 49 loss=2759.320312 err=2759.320312 *
I 2015-05-22 16:51:48 theanets.trainer:168 RmsProp 491 loss=196.193344 err=196.193344
I 2015-05-22 16:51:55 theanets.trainer:168 RmsProp 492 loss=187.097809 err=187.097809
I 2015-05-22 16:52:02 theanets.trainer:168 RmsProp 493 loss=181.007401 err=181.007401
I 2015-05-22 16:52:09 theanets.trainer:168 RmsProp 494 loss=179.598984 err=179.598984
I 2015-05-22 16:52:17 theanets.trainer:168 RmsProp 495 loss=181.458847 err=181.458847
I 2015-05-22 16:52:24 theanets.trainer:168 RmsProp 496 loss=175.569870 err=175.569870
I 2015-05-22 16:52:29 theanets.trainer:168 RmsProp 497 loss=172.974915 err=172.974915
I 2015-05-22 16:52:35 theanets.trainer:168 RmsProp 498 loss=172.029099 err=172.029099
I 2015-05-22 16:52:41 theanets.trainer:168 RmsProp 499 loss=170.467072 err=170.467072
I 2015-05-22 16:52:47 theanets.trainer:168 RmsProp 500 loss=172.081970 err=172.081970
I 2015-05-22 16:52:47 theanets.trainer:168 validation 50 loss=2719.807861 err=2719.807861 *
I 2015-05-22 16:52:53 theanets.trainer:168 RmsProp 501 loss=167.891632 err=167.891632
I 2015-05-22 16:52:58 theanets.trainer:168 RmsProp 502 loss=169.188461 err=169.188461
I 2015-05-22 16:53:04 theanets.trainer:168 RmsProp 503 loss=174.302979 err=174.302979
I 2015-05-22 16:53:09 theanets.trainer:168 RmsProp 504 loss=167.990219 err=167.990219
I 2015-05-22 16:53:15 theanets.trainer:168 RmsProp 505 loss=165.673553 err=165.673553
I 2015-05-22 16:53:21 theanets.trainer:168 RmsProp 506 loss=163.406342 err=163.406342
I 2015-05-22 16:53:26 theanets.trainer:168 RmsProp 507 loss=167.985092 err=167.985092
I 2015-05-22 16:53:32 theanets.trainer:168 RmsProp 508 loss=161.375992 err=161.375992
I 2015-05-22 16:53:37 theanets.trainer:168 RmsProp 509 loss=163.509766 err=163.509766
I 2015-05-22 16:53:43 theanets.trainer:168 RmsProp 510 loss=159.264709 err=159.264709
I 2015-05-22 16:53:43 theanets.trainer:168 validation 51 loss=2740.630859 err=2740.630859
I 2015-05-22 16:53:49 theanets.trainer:168 RmsProp 511 loss=161.278824 err=161.278824
I 2015-05-22 16:53:54 theanets.trainer:168 RmsProp 512 loss=158.421921 err=158.421921
I 2015-05-22 16:54:00 theanets.trainer:168 RmsProp 513 loss=158.037003 err=158.037003
I 2015-05-22 16:54:05 theanets.trainer:168 RmsProp 514 loss=159.355209 err=159.355209
I 2015-05-22 16:54:11 theanets.trainer:168 RmsProp 515 loss=153.179443 err=153.179443
I 2015-05-22 16:54:16 theanets.trainer:168 RmsProp 516 loss=154.879181 err=154.879181
I 2015-05-22 16:54:22 theanets.trainer:168 RmsProp 517 loss=153.905945 err=153.905945
I 2015-05-22 16:54:27 theanets.trainer:168 RmsProp 518 loss=157.910309 err=157.910309
I 2015-05-22 16:54:33 theanets.trainer:168 RmsProp 519 loss=150.162003 err=150.162003
I 2015-05-22 16:54:38 theanets.trainer:168 RmsProp 520 loss=151.210602 err=151.210602
I 2015-05-22 16:54:39 theanets.trainer:168 validation 52 loss=2749.584717 err=2749.584717
I 2015-05-22 16:54:44 theanets.trainer:168 RmsProp 521 loss=148.104584 err=148.104584
I 2015-05-22 16:54:50 theanets.trainer:168 RmsProp 522 loss=150.384033 err=150.384033
I 2015-05-22 16:54:55 theanets.trainer:168 RmsProp 523 loss=149.096924 err=149.096924
I 2015-05-22 16:55:01 theanets.trainer:168 RmsProp 524 loss=152.862518 err=152.862518
I 2015-05-22 16:55:06 theanets.trainer:168 RmsProp 525 loss=152.525330 err=152.525330
I 2015-05-22 16:55:12 theanets.trainer:168 RmsProp 526 loss=150.798996 err=150.798996
I 2015-05-22 16:55:18 theanets.trainer:168 RmsProp 527 loss=148.612442 err=148.612442
I 2015-05-22 16:55:23 theanets.trainer:168 RmsProp 528 loss=149.918350 err=149.918350
I 2015-05-22 16:55:28 theanets.trainer:168 RmsProp 529 loss=149.170563 err=149.170563
I 2015-05-22 16:55:33 theanets.trainer:168 RmsProp 530 loss=148.550476 err=148.550476
I 2015-05-22 16:55:34 theanets.trainer:168 validation 53 loss=2733.514404 err=2733.514404
I 2015-05-22 16:55:39 theanets.trainer:168 RmsProp 531 loss=155.099869 err=155.099869
I 2015-05-22 16:55:44 theanets.trainer:168 RmsProp 532 loss=148.468002 err=148.468002
I 2015-05-22 16:55:50 theanets.trainer:168 RmsProp 533 loss=149.843353 err=149.843353
I 2015-05-22 16:55:55 theanets.trainer:168 RmsProp 534 loss=147.205185 err=147.205185
I 2015-05-22 16:56:00 theanets.trainer:168 RmsProp 535 loss=148.843521 err=148.843521
I 2015-05-22 16:56:05 theanets.trainer:168 RmsProp 536 loss=142.119766 err=142.119766
I 2015-05-22 16:56:11 theanets.trainer:168 RmsProp 537 loss=145.175842 err=145.175842
I 2015-05-22 16:56:16 theanets.trainer:168 RmsProp 538 loss=141.279861 err=141.279861
I 2015-05-22 16:56:21 theanets.trainer:168 RmsProp 539 loss=143.227600 err=143.227600
I 2015-05-22 16:56:27 theanets.trainer:168 RmsProp 540 loss=148.186218 err=148.186218
I 2015-05-22 16:56:27 theanets.trainer:168 validation 54 loss=2748.053467 err=2748.053467
I 2015-05-22 16:56:32 theanets.trainer:168 RmsProp 541 loss=142.255844 err=142.255844
I 2015-05-22 16:56:37 theanets.trainer:168 RmsProp 542 loss=142.210648 err=142.210648
I 2015-05-22 16:56:43 theanets.trainer:168 RmsProp 543 loss=145.581619 err=145.581619
I 2015-05-22 16:56:48 theanets.trainer:168 RmsProp 544 loss=137.276886 err=137.276886
I 2015-05-22 16:56:53 theanets.trainer:168 RmsProp 545 loss=138.446106 err=138.446106
I 2015-05-22 16:56:59 theanets.trainer:168 RmsProp 546 loss=140.000214 err=140.000214
I 2015-05-22 16:57:04 theanets.trainer:168 RmsProp 547 loss=134.190796 err=134.190796
I 2015-05-22 16:57:09 theanets.trainer:168 RmsProp 548 loss=133.069412 err=133.069412
I 2015-05-22 16:57:14 theanets.trainer:168 RmsProp 549 loss=133.644760 err=133.644760
I 2015-05-22 16:57:20 theanets.trainer:168 RmsProp 550 loss=137.306229 err=137.306229
I 2015-05-22 16:57:20 theanets.trainer:168 validation 55 loss=2717.287109 err=2717.287109 *
I 2015-05-22 16:57:25 theanets.trainer:168 RmsProp 551 loss=136.286179 err=136.286179
I 2015-05-22 16:57:31 theanets.trainer:168 RmsProp 552 loss=133.860825 err=133.860825
I 2015-05-22 16:57:36 theanets.trainer:168 RmsProp 553 loss=135.093506 err=135.093506
I 2015-05-22 16:57:41 theanets.trainer:168 RmsProp 554 loss=133.141510 err=133.141510
I 2015-05-22 16:57:46 theanets.trainer:168 RmsProp 555 loss=134.664963 err=134.664963
I 2015-05-22 16:57:52 theanets.trainer:168 RmsProp 556 loss=132.196487 err=132.196487
I 2015-05-22 16:57:57 theanets.trainer:168 RmsProp 557 loss=134.018845 err=134.018845
I 2015-05-22 16:58:02 theanets.trainer:168 RmsProp 558 loss=132.133759 err=132.133759
I 2015-05-22 16:58:08 theanets.trainer:168 RmsProp 559 loss=137.681854 err=137.681854
I 2015-05-22 16:58:13 theanets.trainer:168 RmsProp 560 loss=134.729401 err=134.729401
I 2015-05-22 16:58:13 theanets.trainer:168 validation 56 loss=2746.109131 err=2746.109131
I 2015-05-22 16:58:18 theanets.trainer:168 RmsProp 561 loss=128.555710 err=128.555710
I 2015-05-22 16:58:24 theanets.trainer:168 RmsProp 562 loss=125.859726 err=125.859726
I 2015-05-22 16:58:29 theanets.trainer:168 RmsProp 563 loss=130.850525 err=130.850525
I 2015-05-22 16:58:34 theanets.trainer:168 RmsProp 564 loss=127.365303 err=127.365303
I 2015-05-22 16:58:40 theanets.trainer:168 RmsProp 565 loss=134.035736 err=134.035736
I 2015-05-22 16:58:45 theanets.trainer:168 RmsProp 566 loss=147.770477 err=147.770477
I 2015-05-22 16:58:50 theanets.trainer:168 RmsProp 567 loss=141.760391 err=141.760391
I 2015-05-22 16:58:55 theanets.trainer:168 RmsProp 568 loss=144.584457 err=144.584457
I 2015-05-22 16:59:01 theanets.trainer:168 RmsProp 569 loss=141.521957 err=141.521957
I 2015-05-22 16:59:06 theanets.trainer:168 RmsProp 570 loss=133.498779 err=133.498779
I 2015-05-22 16:59:06 theanets.trainer:168 validation 57 loss=2723.431641 err=2723.431641
I 2015-05-22 16:59:12 theanets.trainer:168 RmsProp 571 loss=129.677444 err=129.677444
I 2015-05-22 16:59:17 theanets.trainer:168 RmsProp 572 loss=127.646881 err=127.646881
I 2015-05-22 16:59:22 theanets.trainer:168 RmsProp 573 loss=121.997047 err=121.997047
I 2015-05-22 16:59:27 theanets.trainer:168 RmsProp 574 loss=125.254784 err=125.254784
I 2015-05-22 16:59:33 theanets.trainer:168 RmsProp 575 loss=124.002762 err=124.002762
I 2015-05-22 16:59:38 theanets.trainer:168 RmsProp 576 loss=121.177071 err=121.177071
I 2015-05-22 16:59:43 theanets.trainer:168 RmsProp 577 loss=118.793411 err=118.793411
I 2015-05-22 16:59:48 theanets.trainer:168 RmsProp 578 loss=120.410286 err=120.410286
I 2015-05-22 16:59:54 theanets.trainer:168 RmsProp 579 loss=119.993988 err=119.993988
I 2015-05-22 16:59:59 theanets.trainer:168 RmsProp 580 loss=122.593193 err=122.593193
I 2015-05-22 16:59:59 theanets.trainer:168 validation 58 loss=2778.837158 err=2778.837158
I 2015-05-22 17:00:05 theanets.trainer:168 RmsProp 581 loss=126.591042 err=126.591042
I 2015-05-22 17:00:10 theanets.trainer:168 RmsProp 582 loss=122.223709 err=122.223709
I 2015-05-22 17:00:15 theanets.trainer:168 RmsProp 583 loss=120.377975 err=120.377975
I 2015-05-22 17:00:20 theanets.trainer:168 RmsProp 584 loss=121.857010 err=121.857010
I 2015-05-22 17:00:26 theanets.trainer:168 RmsProp 585 loss=121.256538 err=121.256538
I 2015-05-22 17:00:31 theanets.trainer:168 RmsProp 586 loss=125.056671 err=125.056671
I 2015-05-22 17:00:36 theanets.trainer:168 RmsProp 587 loss=121.866493 err=121.866493
I 2015-05-22 17:00:41 theanets.trainer:168 RmsProp 588 loss=119.229538 err=119.229538
I 2015-05-22 17:00:47 theanets.trainer:168 RmsProp 589 loss=123.575226 err=123.575226
I 2015-05-22 17:00:52 theanets.trainer:168 RmsProp 590 loss=117.634773 err=117.634773
I 2015-05-22 17:00:52 theanets.trainer:168 validation 59 loss=2752.446045 err=2752.446045
I 2015-05-22 17:00:57 theanets.trainer:168 RmsProp 591 loss=115.049461 err=115.049461
I 2015-05-22 17:01:03 theanets.trainer:168 RmsProp 592 loss=116.987839 err=116.987839
I 2015-05-22 17:01:08 theanets.trainer:168 RmsProp 593 loss=113.498276 err=113.498276
I 2015-05-22 17:01:13 theanets.trainer:168 RmsProp 594 loss=111.446587 err=111.446587
I 2015-05-22 17:01:19 theanets.trainer:168 RmsProp 595 loss=115.850861 err=115.850861
I 2015-05-22 17:01:24 theanets.trainer:168 RmsProp 596 loss=116.668602 err=116.668602
I 2015-05-22 17:01:29 theanets.trainer:168 RmsProp 597 loss=114.360092 err=114.360092
I 2015-05-22 17:01:34 theanets.trainer:168 RmsProp 598 loss=113.651154 err=113.651154
I 2015-05-22 17:01:40 theanets.trainer:168 RmsProp 599 loss=113.495026 err=113.495026
I 2015-05-22 17:01:45 theanets.trainer:168 RmsProp 600 loss=111.002571 err=111.002571
I 2015-05-22 17:01:45 theanets.trainer:168 validation 60 loss=2756.833984 err=2756.833984
I 2015-05-22 17:01:45 theanets.trainer:252 patience elapsed!
I 2015-05-22 17:01:45 theanets.main:237 models_deep/models-100-50-1024-None-None-None.pkl: saving model
I 2015-05-22 17:01:45 theanets.graph:477 models_deep/models-100-50-1024-None-None-None.pkl: saved model parameters

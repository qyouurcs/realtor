I 2015-05-26 03:35:25 theanets.layers:465 layer in: out:0 -> 1000, linear, 0 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm1_fw: in.out:1000 -> 100, logistic, 440700 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm1_bw: in.out:1000 -> 100, logistic, 440700 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm1: in.out:1000 -> 200, logistic, 881400 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm2_fw: bdlstm1.out:200 -> 50, logistic, 50350 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm2_bw: bdlstm1.out:200 -> 50, logistic, 50350 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm2: bdlstm1.out:200 -> 100, logistic, 100700 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer out: bdlstm2.out:100 -> 1, linear, 101 parameters
I 2015-05-26 03:35:26 theanets.graph:145 network has 982201 total parameters
models_deep_post_code_sep/95127-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl
I 2015-05-26 03:35:26 theanets.dataset:133 valid: 3 mini-batches from callable
I 2015-05-26 03:35:26 theanets.dataset:133 train: 30 mini-batches from callable
I 2015-05-26 03:35:26 theanets.main:87 creating trainer <class 'theanets.trainer.RmsProp'>
I 2015-05-26 03:35:26 theanets.main:89 --batch_size = 1024
I 2015-05-26 03:35:26 theanets.main:89 --gradient_clip = 1
I 2015-05-26 03:35:26 theanets.main:89 --hidden_l1 = None
I 2015-05-26 03:35:26 theanets.main:89 --learning_rate = 0.001
I 2015-05-26 03:35:26 theanets.main:89 --train_batches = 30
I 2015-05-26 03:35:26 theanets.main:89 --valid_batches = 3
I 2015-05-26 03:35:26 theanets.main:89 --weight_l1 = None
I 2015-05-26 03:35:26 theanets.main:89 --weight_l2 = None
I 2015-05-26 03:35:26 theanets.trainer:134 compiling evaluation function
I 2015-05-26 03:35:41 theanets.trainer:294 compiling RmsProp learning function
I 2015-05-26 03:38:57 theanets.trainer:168 validation 0 loss=14151.203125 err=14151.203125 *
I 2015-05-26 03:39:56 theanets.trainer:168 RmsProp 1 loss=13079.986328 err=13079.986328
I 2015-05-26 03:40:57 theanets.trainer:168 RmsProp 2 loss=13131.314453 err=13131.314453
I 2015-05-26 03:41:57 theanets.trainer:168 RmsProp 3 loss=13096.683594 err=13096.683594
I 2015-05-26 03:42:57 theanets.trainer:168 RmsProp 4 loss=13166.362305 err=13166.362305
I 2015-05-26 03:43:56 theanets.trainer:168 RmsProp 5 loss=12446.033203 err=12446.033203
I 2015-05-26 03:44:55 theanets.trainer:168 RmsProp 6 loss=10985.920898 err=10985.920898
I 2015-05-26 03:45:55 theanets.trainer:168 RmsProp 7 loss=10340.432617 err=10340.432617
I 2015-05-26 03:46:56 theanets.trainer:168 RmsProp 8 loss=9708.169922 err=9708.169922
I 2015-05-26 03:47:57 theanets.trainer:168 RmsProp 9 loss=9315.417969 err=9315.417969
I 2015-05-26 03:48:57 theanets.trainer:168 RmsProp 10 loss=8705.659180 err=8705.659180
I 2015-05-26 03:48:58 theanets.trainer:168 validation 1 loss=8758.900391 err=8758.900391 *
I 2015-05-26 03:49:59 theanets.trainer:168 RmsProp 11 loss=8057.396484 err=8057.396484
I 2015-05-26 03:50:59 theanets.trainer:168 RmsProp 12 loss=7563.604980 err=7563.604980
I 2015-05-26 03:52:00 theanets.trainer:168 RmsProp 13 loss=7016.755371 err=7016.755371
I 2015-05-26 03:53:02 theanets.trainer:168 RmsProp 14 loss=6574.290527 err=6574.290527
I 2015-05-26 03:54:03 theanets.trainer:168 RmsProp 15 loss=5799.645020 err=5799.645020
I 2015-05-26 03:55:04 theanets.trainer:168 RmsProp 16 loss=5415.713379 err=5415.713379
I 2015-05-26 03:56:06 theanets.trainer:168 RmsProp 17 loss=4823.532715 err=4823.532715
I 2015-05-26 03:57:07 theanets.trainer:168 RmsProp 18 loss=4282.823242 err=4282.823242
I 2015-05-26 03:58:08 theanets.trainer:168 RmsProp 19 loss=3804.601318 err=3804.601318
I 2015-05-26 03:59:09 theanets.trainer:168 RmsProp 20 loss=3473.322510 err=3473.322510
I 2015-05-26 03:59:10 theanets.trainer:168 validation 2 loss=2953.888428 err=2953.888428 *
I 2015-05-26 04:00:11 theanets.trainer:168 RmsProp 21 loss=3191.561035 err=3191.561035
I 2015-05-26 04:01:12 theanets.trainer:168 RmsProp 22 loss=2880.122803 err=2880.122803
I 2015-05-26 04:02:13 theanets.trainer:168 RmsProp 23 loss=2763.924316 err=2763.924316
I 2015-05-26 04:03:14 theanets.trainer:168 RmsProp 24 loss=2547.679688 err=2547.679688
I 2015-05-26 04:04:15 theanets.trainer:168 RmsProp 25 loss=2386.673340 err=2386.673340
I 2015-05-26 04:05:16 theanets.trainer:168 RmsProp 26 loss=2265.961426 err=2265.961426
I 2015-05-26 04:06:17 theanets.trainer:168 RmsProp 27 loss=2134.464355 err=2134.464355
I 2015-05-26 04:07:18 theanets.trainer:168 RmsProp 28 loss=1970.644531 err=1970.644531
I 2015-05-26 04:08:19 theanets.trainer:168 RmsProp 29 loss=1840.324463 err=1840.324463
I 2015-05-26 04:09:20 theanets.trainer:168 RmsProp 30 loss=1743.915161 err=1743.915161
I 2015-05-26 04:09:21 theanets.trainer:168 validation 3 loss=2223.434570 err=2223.434570 *
I 2015-05-26 04:10:21 theanets.trainer:168 RmsProp 31 loss=1620.330688 err=1620.330688
I 2015-05-26 04:11:21 theanets.trainer:168 RmsProp 32 loss=1537.312256 err=1537.312256
I 2015-05-26 04:12:21 theanets.trainer:168 RmsProp 33 loss=1443.611572 err=1443.611572
I 2015-05-26 04:13:20 theanets.trainer:168 RmsProp 34 loss=1425.923706 err=1425.923706
I 2015-05-26 04:14:18 theanets.trainer:168 RmsProp 35 loss=1323.040405 err=1323.040405
I 2015-05-26 04:15:14 theanets.trainer:168 RmsProp 36 loss=1267.025879 err=1267.025879
I 2015-05-26 04:16:11 theanets.trainer:168 RmsProp 37 loss=1253.751343 err=1253.751343
I 2015-05-26 04:17:07 theanets.trainer:168 RmsProp 38 loss=1245.473267 err=1245.473267
I 2015-05-26 04:18:04 theanets.trainer:168 RmsProp 39 loss=1188.888062 err=1188.888062
I 2015-05-26 04:19:00 theanets.trainer:168 RmsProp 40 loss=1125.061646 err=1125.061646
I 2015-05-26 04:19:01 theanets.trainer:168 validation 4 loss=2090.327637 err=2090.327637 *
I 2015-05-26 04:19:57 theanets.trainer:168 RmsProp 41 loss=1215.981689 err=1215.981689
I 2015-05-26 04:20:54 theanets.trainer:168 RmsProp 42 loss=1431.495361 err=1431.495361
I 2015-05-26 04:21:52 theanets.trainer:168 RmsProp 43 loss=2001.596191 err=2001.596191
I 2015-05-26 04:22:46 theanets.trainer:168 RmsProp 44 loss=1714.461548 err=1714.461548
I 2015-05-26 04:23:39 theanets.trainer:168 RmsProp 45 loss=1398.089233 err=1398.089233
I 2015-05-26 04:24:33 theanets.trainer:168 RmsProp 46 loss=1234.766479 err=1234.766479
I 2015-05-26 04:25:26 theanets.trainer:168 RmsProp 47 loss=1094.820068 err=1094.820068
I 2015-05-26 04:26:19 theanets.trainer:168 RmsProp 48 loss=942.954651 err=942.954651
I 2015-05-26 04:27:13 theanets.trainer:168 RmsProp 49 loss=879.666687 err=879.666687
I 2015-05-26 04:28:06 theanets.trainer:168 RmsProp 50 loss=808.265198 err=808.265198
I 2015-05-26 04:28:07 theanets.trainer:168 validation 5 loss=1655.750366 err=1655.750366 *
I 2015-05-26 04:29:00 theanets.trainer:168 RmsProp 51 loss=755.226685 err=755.226685
I 2015-05-26 04:29:53 theanets.trainer:168 RmsProp 52 loss=706.893188 err=706.893188
I 2015-05-26 04:30:46 theanets.trainer:168 RmsProp 53 loss=678.533813 err=678.533813
I 2015-05-26 04:31:39 theanets.trainer:168 RmsProp 54 loss=647.481628 err=647.481628
I 2015-05-26 04:32:32 theanets.trainer:168 RmsProp 55 loss=650.937744 err=650.937744
I 2015-05-26 04:33:25 theanets.trainer:168 RmsProp 56 loss=611.844055 err=611.844055
I 2015-05-26 04:34:18 theanets.trainer:168 RmsProp 57 loss=585.423584 err=585.423584
I 2015-05-26 04:35:12 theanets.trainer:168 RmsProp 58 loss=550.313660 err=550.313660
I 2015-05-26 04:36:06 theanets.trainer:168 RmsProp 59 loss=536.346191 err=536.346191
I 2015-05-26 04:37:00 theanets.trainer:168 RmsProp 60 loss=537.507202 err=537.507202
I 2015-05-26 04:37:01 theanets.trainer:168 validation 6 loss=1562.199097 err=1562.199097 *
I 2015-05-26 04:37:55 theanets.trainer:168 RmsProp 61 loss=502.060242 err=502.060242
I 2015-05-26 04:38:49 theanets.trainer:168 RmsProp 62 loss=479.887848 err=479.887848
I 2015-05-26 04:39:43 theanets.trainer:168 RmsProp 63 loss=465.651459 err=465.651459
I 2015-05-26 04:40:38 theanets.trainer:168 RmsProp 64 loss=459.350708 err=459.350708
I 2015-05-26 04:41:32 theanets.trainer:168 RmsProp 65 loss=447.666931 err=447.666931
I 2015-05-26 04:42:26 theanets.trainer:168 RmsProp 66 loss=437.971039 err=437.971039
I 2015-05-26 04:43:20 theanets.trainer:168 RmsProp 67 loss=403.881927 err=403.881927
I 2015-05-26 04:44:15 theanets.trainer:168 RmsProp 68 loss=396.078583 err=396.078583
I 2015-05-26 04:45:08 theanets.trainer:168 RmsProp 69 loss=404.065155 err=404.065155
I 2015-05-26 04:46:03 theanets.trainer:168 RmsProp 70 loss=365.098755 err=365.098755
I 2015-05-26 04:46:04 theanets.trainer:168 validation 7 loss=1463.748657 err=1463.748657 *
I 2015-05-26 04:46:58 theanets.trainer:168 RmsProp 71 loss=361.296051 err=361.296051
I 2015-05-26 04:47:51 theanets.trainer:168 RmsProp 72 loss=356.837372 err=356.837372
I 2015-05-26 04:48:45 theanets.trainer:168 RmsProp 73 loss=354.679321 err=354.679321
I 2015-05-26 04:49:39 theanets.trainer:168 RmsProp 74 loss=336.438934 err=336.438934
I 2015-05-26 04:50:33 theanets.trainer:168 RmsProp 75 loss=316.745575 err=316.745575
I 2015-05-26 04:51:27 theanets.trainer:168 RmsProp 76 loss=326.364868 err=326.364868
I 2015-05-26 04:52:22 theanets.trainer:168 RmsProp 77 loss=314.633423 err=314.633423
I 2015-05-26 04:53:17 theanets.trainer:168 RmsProp 78 loss=299.035004 err=299.035004
I 2015-05-26 04:54:10 theanets.trainer:168 RmsProp 79 loss=289.459747 err=289.459747
I 2015-05-26 04:55:04 theanets.trainer:168 RmsProp 80 loss=284.663452 err=284.663452
I 2015-05-26 04:55:05 theanets.trainer:168 validation 8 loss=1461.653687 err=1461.653687 *
I 2015-05-26 04:55:57 theanets.trainer:168 RmsProp 81 loss=271.005951 err=271.005951
I 2015-05-26 04:56:50 theanets.trainer:168 RmsProp 82 loss=261.046173 err=261.046173
I 2015-05-26 04:57:42 theanets.trainer:168 RmsProp 83 loss=255.865494 err=255.865494
I 2015-05-26 04:58:34 theanets.trainer:168 RmsProp 84 loss=257.464172 err=257.464172
I 2015-05-26 04:59:27 theanets.trainer:168 RmsProp 85 loss=255.170578 err=255.170578
I 2015-05-26 05:00:20 theanets.trainer:168 RmsProp 86 loss=246.838623 err=246.838623
I 2015-05-26 05:01:13 theanets.trainer:168 RmsProp 87 loss=232.414078 err=232.414078
I 2015-05-26 05:02:06 theanets.trainer:168 RmsProp 88 loss=213.609665 err=213.609665
I 2015-05-26 05:02:58 theanets.trainer:168 RmsProp 89 loss=242.677017 err=242.677017
I 2015-05-26 05:03:51 theanets.trainer:168 RmsProp 90 loss=227.768204 err=227.768204
I 2015-05-26 05:03:52 theanets.trainer:168 validation 9 loss=1466.992065 err=1466.992065
I 2015-05-26 05:04:45 theanets.trainer:168 RmsProp 91 loss=202.657242 err=202.657242
I 2015-05-26 05:05:39 theanets.trainer:168 RmsProp 92 loss=207.946533 err=207.946533
I 2015-05-26 05:06:32 theanets.trainer:168 RmsProp 93 loss=207.999863 err=207.999863
I 2015-05-26 05:07:25 theanets.trainer:168 RmsProp 94 loss=207.888504 err=207.888504
I 2015-05-26 05:08:16 theanets.trainer:168 RmsProp 95 loss=185.920319 err=185.920319
I 2015-05-26 05:09:07 theanets.trainer:168 RmsProp 96 loss=190.838821 err=190.838821
I 2015-05-26 05:09:58 theanets.trainer:168 RmsProp 97 loss=185.446274 err=185.446274
I 2015-05-26 05:10:48 theanets.trainer:168 RmsProp 98 loss=179.214905 err=179.214905
I 2015-05-26 05:11:38 theanets.trainer:168 RmsProp 99 loss=179.149704 err=179.149704
I 2015-05-26 05:12:28 theanets.trainer:168 RmsProp 100 loss=174.446198 err=174.446198
I 2015-05-26 05:12:29 theanets.trainer:168 validation 10 loss=1451.781616 err=1451.781616 *
I 2015-05-26 05:13:18 theanets.trainer:168 RmsProp 101 loss=167.504974 err=167.504974
I 2015-05-26 05:14:09 theanets.trainer:168 RmsProp 102 loss=166.553268 err=166.553268
I 2015-05-26 05:14:59 theanets.trainer:168 RmsProp 103 loss=157.335464 err=157.335464
I 2015-05-26 05:15:50 theanets.trainer:168 RmsProp 104 loss=156.995255 err=156.995255
I 2015-05-26 05:16:41 theanets.trainer:168 RmsProp 105 loss=152.290009 err=152.290009
I 2015-05-26 05:17:31 theanets.trainer:168 RmsProp 106 loss=150.932800 err=150.932800
I 2015-05-26 05:18:22 theanets.trainer:168 RmsProp 107 loss=148.815399 err=148.815399
I 2015-05-26 05:19:12 theanets.trainer:168 RmsProp 108 loss=144.200485 err=144.200485
I 2015-05-26 05:20:02 theanets.trainer:168 RmsProp 109 loss=143.530121 err=143.530121
I 2015-05-26 05:20:53 theanets.trainer:168 RmsProp 110 loss=137.800919 err=137.800919
I 2015-05-26 05:20:54 theanets.trainer:168 validation 11 loss=1459.856812 err=1459.856812
I 2015-05-26 05:21:45 theanets.trainer:168 RmsProp 111 loss=133.059357 err=133.059357
I 2015-05-26 05:22:35 theanets.trainer:168 RmsProp 112 loss=135.368958 err=135.368958
I 2015-05-26 05:23:26 theanets.trainer:168 RmsProp 113 loss=131.721939 err=131.721939
I 2015-05-26 05:24:17 theanets.trainer:168 RmsProp 114 loss=124.771309 err=124.771309
I 2015-05-26 05:25:08 theanets.trainer:168 RmsProp 115 loss=122.759552 err=122.759552
I 2015-05-26 05:25:58 theanets.trainer:168 RmsProp 116 loss=120.966599 err=120.966599
I 2015-05-26 05:26:48 theanets.trainer:168 RmsProp 117 loss=116.854042 err=116.854042
I 2015-05-26 05:27:39 theanets.trainer:168 RmsProp 118 loss=110.994804 err=110.994804
I 2015-05-26 05:28:30 theanets.trainer:168 RmsProp 119 loss=116.828247 err=116.828247
I 2015-05-26 05:29:20 theanets.trainer:168 RmsProp 120 loss=114.375763 err=114.375763
I 2015-05-26 05:29:22 theanets.trainer:168 validation 12 loss=1440.178589 err=1440.178589 *
I 2015-05-26 05:30:12 theanets.trainer:168 RmsProp 121 loss=110.360146 err=110.360146
I 2015-05-26 05:31:03 theanets.trainer:168 RmsProp 122 loss=105.315331 err=105.315331
I 2015-05-26 05:31:54 theanets.trainer:168 RmsProp 123 loss=102.902191 err=102.902191
I 2015-05-26 05:32:45 theanets.trainer:168 RmsProp 124 loss=102.230034 err=102.230034
I 2015-05-26 05:33:35 theanets.trainer:168 RmsProp 125 loss=100.715019 err=100.715019
I 2015-05-26 05:34:26 theanets.trainer:168 RmsProp 126 loss=97.832253 err=97.832253
I 2015-05-26 05:35:17 theanets.trainer:168 RmsProp 127 loss=98.662941 err=98.662941
I 2015-05-26 05:36:08 theanets.trainer:168 RmsProp 128 loss=96.593178 err=96.593178
I 2015-05-26 05:36:58 theanets.trainer:168 RmsProp 129 loss=91.547615 err=91.547615
I 2015-05-26 05:37:48 theanets.trainer:168 RmsProp 130 loss=91.651123 err=91.651123
I 2015-05-26 05:37:49 theanets.trainer:168 validation 13 loss=1421.441040 err=1421.441040 *
I 2015-05-26 05:38:37 theanets.trainer:168 RmsProp 131 loss=89.277611 err=89.277611
I 2015-05-26 05:39:25 theanets.trainer:168 RmsProp 132 loss=89.131256 err=89.131256
I 2015-05-26 05:40:13 theanets.trainer:168 RmsProp 133 loss=89.595711 err=89.595711
I 2015-05-26 05:41:02 theanets.trainer:168 RmsProp 134 loss=85.239861 err=85.239861
I 2015-05-26 05:41:51 theanets.trainer:168 RmsProp 135 loss=83.933594 err=83.933594
I 2015-05-26 05:42:39 theanets.trainer:168 RmsProp 136 loss=82.691154 err=82.691154
I 2015-05-26 05:43:29 theanets.trainer:168 RmsProp 137 loss=82.103218 err=82.103218
I 2015-05-26 05:44:18 theanets.trainer:168 RmsProp 138 loss=79.532951 err=79.532951
I 2015-05-26 05:45:07 theanets.trainer:168 RmsProp 139 loss=77.535683 err=77.535683
I 2015-05-26 05:45:57 theanets.trainer:168 RmsProp 140 loss=76.408684 err=76.408684
I 2015-05-26 05:45:58 theanets.trainer:168 validation 14 loss=1421.611694 err=1421.611694
I 2015-05-26 05:46:47 theanets.trainer:168 RmsProp 141 loss=76.618439 err=76.618439
I 2015-05-26 05:47:37 theanets.trainer:168 RmsProp 142 loss=75.079628 err=75.079628
I 2015-05-26 05:48:27 theanets.trainer:168 RmsProp 143 loss=71.590355 err=71.590355
I 2015-05-26 05:49:16 theanets.trainer:168 RmsProp 144 loss=71.764580 err=71.764580
I 2015-05-26 05:50:06 theanets.trainer:168 RmsProp 145 loss=69.897110 err=69.897110
I 2015-05-26 05:50:56 theanets.trainer:168 RmsProp 146 loss=69.417992 err=69.417992
I 2015-05-26 05:51:45 theanets.trainer:168 RmsProp 147 loss=70.539894 err=70.539894
I 2015-05-26 05:52:34 theanets.trainer:168 RmsProp 148 loss=67.563858 err=67.563858
I 2015-05-26 05:53:23 theanets.trainer:168 RmsProp 149 loss=65.405235 err=65.405235
I 2015-05-26 05:54:13 theanets.trainer:168 RmsProp 150 loss=66.527008 err=66.527008
I 2015-05-26 05:54:14 theanets.trainer:168 validation 15 loss=1434.523804 err=1434.523804
I 2015-05-26 05:55:03 theanets.trainer:168 RmsProp 151 loss=63.164799 err=63.164799
I 2015-05-26 05:55:53 theanets.trainer:168 RmsProp 152 loss=62.413570 err=62.413570
I 2015-05-26 05:56:42 theanets.trainer:168 RmsProp 153 loss=60.390869 err=60.390869
I 2015-05-26 05:57:32 theanets.trainer:168 RmsProp 154 loss=60.502071 err=60.502071
I 2015-05-26 05:58:21 theanets.trainer:168 RmsProp 155 loss=60.650688 err=60.650688
I 2015-05-26 05:59:10 theanets.trainer:168 RmsProp 156 loss=58.761787 err=58.761787
I 2015-05-26 06:00:00 theanets.trainer:168 RmsProp 157 loss=56.272255 err=56.272255
I 2015-05-26 06:00:49 theanets.trainer:168 RmsProp 158 loss=56.279930 err=56.279930
I 2015-05-26 06:01:38 theanets.trainer:168 RmsProp 159 loss=54.875256 err=54.875256
I 2015-05-26 06:02:28 theanets.trainer:168 RmsProp 160 loss=55.354568 err=55.354568
I 2015-05-26 06:02:29 theanets.trainer:168 validation 16 loss=1439.554810 err=1439.554810
I 2015-05-26 06:03:19 theanets.trainer:168 RmsProp 161 loss=54.786137 err=54.786137
I 2015-05-26 06:04:08 theanets.trainer:168 RmsProp 162 loss=52.815514 err=52.815514
I 2015-05-26 06:04:58 theanets.trainer:168 RmsProp 163 loss=51.407555 err=51.407555
I 2015-05-26 06:05:47 theanets.trainer:168 RmsProp 164 loss=50.261833 err=50.261833
I 2015-05-26 06:06:36 theanets.trainer:168 RmsProp 165 loss=47.533886 err=47.533886
I 2015-05-26 06:07:23 theanets.trainer:168 RmsProp 166 loss=47.780258 err=47.780258
I 2015-05-26 06:08:11 theanets.trainer:168 RmsProp 167 loss=48.132023 err=48.132023
I 2015-05-26 06:09:00 theanets.trainer:168 RmsProp 168 loss=49.254631 err=49.254631
I 2015-05-26 06:09:49 theanets.trainer:168 RmsProp 169 loss=46.730324 err=46.730324
I 2015-05-26 06:10:38 theanets.trainer:168 RmsProp 170 loss=45.101456 err=45.101456
I 2015-05-26 06:10:39 theanets.trainer:168 validation 17 loss=1436.786743 err=1436.786743
I 2015-05-26 06:11:28 theanets.trainer:168 RmsProp 171 loss=42.737148 err=42.737148
I 2015-05-26 06:12:18 theanets.trainer:168 RmsProp 172 loss=43.629299 err=43.629299
I 2015-05-26 06:13:08 theanets.trainer:168 RmsProp 173 loss=44.282101 err=44.282101
I 2015-05-26 06:13:58 theanets.trainer:168 RmsProp 174 loss=41.214249 err=41.214249
I 2015-05-26 06:14:47 theanets.trainer:168 RmsProp 175 loss=45.069881 err=45.069881
I 2015-05-26 06:15:36 theanets.trainer:168 RmsProp 176 loss=48.628338 err=48.628338
I 2015-05-26 06:16:26 theanets.trainer:168 RmsProp 177 loss=41.962952 err=41.962952
I 2015-05-26 06:17:16 theanets.trainer:168 RmsProp 178 loss=40.459084 err=40.459084
I 2015-05-26 06:18:06 theanets.trainer:168 RmsProp 179 loss=39.421757 err=39.421757
I 2015-05-26 06:18:56 theanets.trainer:168 RmsProp 180 loss=37.891014 err=37.891014
I 2015-05-26 06:18:57 theanets.trainer:168 validation 18 loss=1436.012329 err=1436.012329
I 2015-05-26 06:18:57 theanets.trainer:252 patience elapsed!
I 2015-05-26 06:18:57 theanets.dataset:133 valid: 2 mini-batches from callable
I 2015-05-26 06:18:57 theanets.dataset:133 train: 10 mini-batches from callable
I 2015-05-26 06:18:57 theanets.main:87 creating trainer <class 'theanets.trainer.RmsProp'>
I 2015-05-26 06:18:57 theanets.main:89 --algorithms = rmsprop
I 2015-05-26 06:18:57 theanets.main:89 --batch_size = 1024
I 2015-05-26 06:18:57 theanets.main:89 --gradient_clip = 100000.0
I 2015-05-26 06:18:57 theanets.main:89 --hidden_l1 = None
I 2015-05-26 06:18:57 theanets.main:89 --learning_rate = 0.0001
I 2015-05-26 06:18:57 theanets.main:89 --train_batches = 10
I 2015-05-26 06:18:57 theanets.main:89 --valid_batches = 2
I 2015-05-26 06:18:57 theanets.main:89 --weight_l1 = None
I 2015-05-26 06:18:57 theanets.main:89 --weight_l2 = None
I 2015-05-26 06:18:57 theanets.trainer:134 compiling evaluation function
I 2015-05-26 06:19:07 theanets.trainer:294 compiling RmsProp learning function
I 2015-05-26 06:20:50 theanets.trainer:168 validation 0 loss=889.957642 err=889.957642 *
I 2015-05-26 06:21:06 theanets.trainer:168 RmsProp 1 loss=58.063019 err=58.063019
I 2015-05-26 06:21:22 theanets.trainer:168 RmsProp 2 loss=40.435986 err=40.435986
I 2015-05-26 06:21:39 theanets.trainer:168 RmsProp 3 loss=30.891968 err=30.891968
I 2015-05-26 06:21:55 theanets.trainer:168 RmsProp 4 loss=25.930161 err=25.930161
I 2015-05-26 06:22:12 theanets.trainer:168 RmsProp 5 loss=23.143223 err=23.143223
I 2015-05-26 06:22:29 theanets.trainer:168 RmsProp 6 loss=20.410965 err=20.410965
I 2015-05-26 06:22:45 theanets.trainer:168 RmsProp 7 loss=18.576796 err=18.576796
I 2015-05-26 06:23:01 theanets.trainer:168 RmsProp 8 loss=16.381212 err=16.381212
I 2015-05-26 06:23:17 theanets.trainer:168 RmsProp 9 loss=14.825388 err=14.825388
I 2015-05-26 06:23:34 theanets.trainer:168 RmsProp 10 loss=13.425726 err=13.425726
I 2015-05-26 06:23:34 theanets.trainer:168 validation 1 loss=772.093445 err=772.093445 *
I 2015-05-26 06:23:51 theanets.trainer:168 RmsProp 11 loss=12.216354 err=12.216354
I 2015-05-26 06:24:07 theanets.trainer:168 RmsProp 12 loss=11.236214 err=11.236214
I 2015-05-26 06:24:23 theanets.trainer:168 RmsProp 13 loss=10.722495 err=10.722495
I 2015-05-26 06:24:39 theanets.trainer:168 RmsProp 14 loss=9.605722 err=9.605722
I 2015-05-26 06:24:55 theanets.trainer:168 RmsProp 15 loss=9.433885 err=9.433885
I 2015-05-26 06:25:12 theanets.trainer:168 RmsProp 16 loss=8.661304 err=8.661304
I 2015-05-26 06:25:28 theanets.trainer:168 RmsProp 17 loss=8.246618 err=8.246618
I 2015-05-26 06:25:45 theanets.trainer:168 RmsProp 18 loss=7.990476 err=7.990476
I 2015-05-26 06:26:01 theanets.trainer:168 RmsProp 19 loss=7.155967 err=7.155967
I 2015-05-26 06:26:18 theanets.trainer:168 RmsProp 20 loss=7.240596 err=7.240596
I 2015-05-26 06:26:18 theanets.trainer:168 validation 2 loss=744.286804 err=744.286804 *
I 2015-05-26 06:26:35 theanets.trainer:168 RmsProp 21 loss=6.599256 err=6.599256
I 2015-05-26 06:26:51 theanets.trainer:168 RmsProp 22 loss=6.363046 err=6.363046
I 2015-05-26 06:27:08 theanets.trainer:168 RmsProp 23 loss=6.297481 err=6.297481
I 2015-05-26 06:27:24 theanets.trainer:168 RmsProp 24 loss=6.030497 err=6.030497
I 2015-05-26 06:27:41 theanets.trainer:168 RmsProp 25 loss=5.702817 err=5.702817
I 2015-05-26 06:27:57 theanets.trainer:168 RmsProp 26 loss=5.578579 err=5.578579
I 2015-05-26 06:28:13 theanets.trainer:168 RmsProp 27 loss=5.421565 err=5.421565
I 2015-05-26 06:28:30 theanets.trainer:168 RmsProp 28 loss=5.108624 err=5.108624
I 2015-05-26 06:28:46 theanets.trainer:168 RmsProp 29 loss=4.948576 err=4.948576
I 2015-05-26 06:29:03 theanets.trainer:168 RmsProp 30 loss=4.902576 err=4.902576
I 2015-05-26 06:29:04 theanets.trainer:168 validation 3 loss=728.223511 err=728.223511 *
I 2015-05-26 06:29:20 theanets.trainer:168 RmsProp 31 loss=4.646382 err=4.646382
I 2015-05-26 06:29:37 theanets.trainer:168 RmsProp 32 loss=4.455371 err=4.455371
I 2015-05-26 06:29:53 theanets.trainer:168 RmsProp 33 loss=4.469289 err=4.469289
I 2015-05-26 06:30:10 theanets.trainer:168 RmsProp 34 loss=4.231297 err=4.231297
I 2015-05-26 06:30:26 theanets.trainer:168 RmsProp 35 loss=4.108326 err=4.108326
I 2015-05-26 06:30:43 theanets.trainer:168 RmsProp 36 loss=4.042442 err=4.042442
I 2015-05-26 06:30:59 theanets.trainer:168 RmsProp 37 loss=3.871988 err=3.871988
I 2015-05-26 06:31:16 theanets.trainer:168 RmsProp 38 loss=3.879318 err=3.879318
I 2015-05-26 06:31:32 theanets.trainer:168 RmsProp 39 loss=3.613853 err=3.613853
I 2015-05-26 06:31:49 theanets.trainer:168 RmsProp 40 loss=3.678497 err=3.678497
I 2015-05-26 06:31:50 theanets.trainer:168 validation 4 loss=718.544128 err=718.544128 *
I 2015-05-26 06:32:06 theanets.trainer:168 RmsProp 41 loss=3.493118 err=3.493118
I 2015-05-26 06:32:23 theanets.trainer:168 RmsProp 42 loss=3.461953 err=3.461953
I 2015-05-26 06:32:40 theanets.trainer:168 RmsProp 43 loss=3.435990 err=3.435990
I 2015-05-26 06:32:56 theanets.trainer:168 RmsProp 44 loss=3.390559 err=3.390559
I 2015-05-26 06:33:13 theanets.trainer:168 RmsProp 45 loss=3.243394 err=3.243394
I 2015-05-26 06:33:29 theanets.trainer:168 RmsProp 46 loss=3.232475 err=3.232475
I 2015-05-26 06:33:46 theanets.trainer:168 RmsProp 47 loss=3.040428 err=3.040428
I 2015-05-26 06:34:02 theanets.trainer:168 RmsProp 48 loss=2.991972 err=2.991972
I 2015-05-26 06:34:18 theanets.trainer:168 RmsProp 49 loss=3.058065 err=3.058065
I 2015-05-26 06:34:34 theanets.trainer:168 RmsProp 50 loss=2.899504 err=2.899504
I 2015-05-26 06:34:34 theanets.trainer:168 validation 5 loss=711.738464 err=711.738464 *
I 2015-05-26 06:34:49 theanets.trainer:168 RmsProp 51 loss=2.853086 err=2.853086
I 2015-05-26 06:35:05 theanets.trainer:168 RmsProp 52 loss=2.768236 err=2.768236
I 2015-05-26 06:35:20 theanets.trainer:168 RmsProp 53 loss=2.680559 err=2.680559
I 2015-05-26 06:35:35 theanets.trainer:168 RmsProp 54 loss=2.757131 err=2.757131
I 2015-05-26 06:35:50 theanets.trainer:168 RmsProp 55 loss=2.617638 err=2.617638
I 2015-05-26 06:36:06 theanets.trainer:168 RmsProp 56 loss=2.630053 err=2.630053
I 2015-05-26 06:36:20 theanets.trainer:168 RmsProp 57 loss=2.489161 err=2.489161
I 2015-05-26 06:36:36 theanets.trainer:168 RmsProp 58 loss=2.465099 err=2.465099
I 2015-05-26 06:36:51 theanets.trainer:168 RmsProp 59 loss=2.544544 err=2.544544
I 2015-05-26 06:37:07 theanets.trainer:168 RmsProp 60 loss=2.457537 err=2.457537
I 2015-05-26 06:37:08 theanets.trainer:168 validation 6 loss=706.569763 err=706.569763 *
I 2015-05-26 06:37:23 theanets.trainer:168 RmsProp 61 loss=2.325598 err=2.325598
I 2015-05-26 06:37:38 theanets.trainer:168 RmsProp 62 loss=2.335960 err=2.335960
I 2015-05-26 06:37:54 theanets.trainer:168 RmsProp 63 loss=2.276457 err=2.276457
I 2015-05-26 06:38:09 theanets.trainer:168 RmsProp 64 loss=2.254682 err=2.254682
I 2015-05-26 06:38:25 theanets.trainer:168 RmsProp 65 loss=2.197511 err=2.197511
I 2015-05-26 06:38:41 theanets.trainer:168 RmsProp 66 loss=2.164691 err=2.164691
I 2015-05-26 06:38:56 theanets.trainer:168 RmsProp 67 loss=2.125098 err=2.125098
I 2015-05-26 06:39:12 theanets.trainer:168 RmsProp 68 loss=2.161903 err=2.161903
I 2015-05-26 06:39:28 theanets.trainer:168 RmsProp 69 loss=2.070950 err=2.070950
I 2015-05-26 06:39:43 theanets.trainer:168 RmsProp 70 loss=2.048890 err=2.048890
I 2015-05-26 06:39:44 theanets.trainer:168 validation 7 loss=701.274841 err=701.274841 *
I 2015-05-26 06:39:59 theanets.trainer:168 RmsProp 71 loss=2.033252 err=2.033252
I 2015-05-26 06:40:14 theanets.trainer:168 RmsProp 72 loss=2.058674 err=2.058674
I 2015-05-26 06:40:29 theanets.trainer:168 RmsProp 73 loss=2.012344 err=2.012344
I 2015-05-26 06:40:44 theanets.trainer:168 RmsProp 74 loss=1.963633 err=1.963633
I 2015-05-26 06:41:00 theanets.trainer:168 RmsProp 75 loss=1.875537 err=1.875537
I 2015-05-26 06:41:15 theanets.trainer:168 RmsProp 76 loss=1.933599 err=1.933599
I 2015-05-26 06:41:30 theanets.trainer:168 RmsProp 77 loss=1.831486 err=1.831486
I 2015-05-26 06:41:45 theanets.trainer:168 RmsProp 78 loss=1.801952 err=1.801952
I 2015-05-26 06:42:00 theanets.trainer:168 RmsProp 79 loss=1.886558 err=1.886558
I 2015-05-26 06:42:15 theanets.trainer:168 RmsProp 80 loss=1.799513 err=1.799513
I 2015-05-26 06:42:16 theanets.trainer:168 validation 8 loss=697.738464 err=697.738464 *
I 2015-05-26 06:42:31 theanets.trainer:168 RmsProp 81 loss=1.745165 err=1.745165
I 2015-05-26 06:42:46 theanets.trainer:168 RmsProp 82 loss=1.737055 err=1.737055
I 2015-05-26 06:43:01 theanets.trainer:168 RmsProp 83 loss=1.749774 err=1.749774
I 2015-05-26 06:43:16 theanets.trainer:168 RmsProp 84 loss=1.682662 err=1.682662
I 2015-05-26 06:43:31 theanets.trainer:168 RmsProp 85 loss=1.689567 err=1.689567
I 2015-05-26 06:43:45 theanets.trainer:168 RmsProp 86 loss=1.634958 err=1.634958
I 2015-05-26 06:44:00 theanets.trainer:168 RmsProp 87 loss=1.672779 err=1.672779
I 2015-05-26 06:44:15 theanets.trainer:168 RmsProp 88 loss=1.610376 err=1.610376
I 2015-05-26 06:44:30 theanets.trainer:168 RmsProp 89 loss=1.584350 err=1.584350
I 2015-05-26 06:44:45 theanets.trainer:168 RmsProp 90 loss=1.553470 err=1.553470
I 2015-05-26 06:44:46 theanets.trainer:168 validation 9 loss=694.492493 err=694.492493 *
I 2015-05-26 06:45:01 theanets.trainer:168 RmsProp 91 loss=1.569452 err=1.569452
I 2015-05-26 06:45:16 theanets.trainer:168 RmsProp 92 loss=1.566824 err=1.566824
I 2015-05-26 06:45:31 theanets.trainer:168 RmsProp 93 loss=1.519467 err=1.519467
I 2015-05-26 06:45:46 theanets.trainer:168 RmsProp 94 loss=1.490563 err=1.490563
I 2015-05-26 06:46:01 theanets.trainer:168 RmsProp 95 loss=1.532793 err=1.532793
I 2015-05-26 06:46:17 theanets.trainer:168 RmsProp 96 loss=1.456575 err=1.456575
I 2015-05-26 06:46:32 theanets.trainer:168 RmsProp 97 loss=1.471932 err=1.471932
I 2015-05-26 06:46:47 theanets.trainer:168 RmsProp 98 loss=1.502288 err=1.502288
I 2015-05-26 06:47:02 theanets.trainer:168 RmsProp 99 loss=1.463325 err=1.463325
I 2015-05-26 06:47:17 theanets.trainer:168 RmsProp 100 loss=1.399492 err=1.399492
I 2015-05-26 06:47:18 theanets.trainer:168 validation 10 loss=690.983093 err=690.983093 *
I 2015-05-26 06:47:32 theanets.trainer:168 RmsProp 101 loss=1.372151 err=1.372151
I 2015-05-26 06:47:47 theanets.trainer:168 RmsProp 102 loss=1.372928 err=1.372928
I 2015-05-26 06:48:02 theanets.trainer:168 RmsProp 103 loss=1.381503 err=1.381503
I 2015-05-26 06:48:17 theanets.trainer:168 RmsProp 104 loss=1.379317 err=1.379317
I 2015-05-26 06:48:32 theanets.trainer:168 RmsProp 105 loss=1.348835 err=1.348835
I 2015-05-26 06:48:47 theanets.trainer:168 RmsProp 106 loss=1.331899 err=1.331899
I 2015-05-26 06:49:02 theanets.trainer:168 RmsProp 107 loss=1.289617 err=1.289617
I 2015-05-26 06:49:17 theanets.trainer:168 RmsProp 108 loss=1.331881 err=1.331881
I 2015-05-26 06:49:32 theanets.trainer:168 RmsProp 109 loss=1.297468 err=1.297468
I 2015-05-26 06:49:46 theanets.trainer:168 RmsProp 110 loss=1.259735 err=1.259735
I 2015-05-26 06:49:47 theanets.trainer:168 validation 11 loss=689.247009 err=689.247009 *
I 2015-05-26 06:50:02 theanets.trainer:168 RmsProp 111 loss=1.293009 err=1.293009
I 2015-05-26 06:50:17 theanets.trainer:168 RmsProp 112 loss=1.263101 err=1.263101
I 2015-05-26 06:50:32 theanets.trainer:168 RmsProp 113 loss=1.212275 err=1.212275
I 2015-05-26 06:50:47 theanets.trainer:168 RmsProp 114 loss=1.222376 err=1.222376
I 2015-05-26 06:51:02 theanets.trainer:168 RmsProp 115 loss=1.232703 err=1.232703
I 2015-05-26 06:51:17 theanets.trainer:168 RmsProp 116 loss=1.218227 err=1.218227
I 2015-05-26 06:51:32 theanets.trainer:168 RmsProp 117 loss=1.198329 err=1.198329
I 2015-05-26 06:51:47 theanets.trainer:168 RmsProp 118 loss=1.202320 err=1.202320
I 2015-05-26 06:52:03 theanets.trainer:168 RmsProp 119 loss=1.170612 err=1.170612
I 2015-05-26 06:52:17 theanets.trainer:168 RmsProp 120 loss=1.170352 err=1.170352
I 2015-05-26 06:52:18 theanets.trainer:168 validation 12 loss=687.396851 err=687.396851 *
I 2015-05-26 06:52:33 theanets.trainer:168 RmsProp 121 loss=1.160402 err=1.160402
I 2015-05-26 06:52:48 theanets.trainer:168 RmsProp 122 loss=1.108637 err=1.108637
I 2015-05-26 06:53:03 theanets.trainer:168 RmsProp 123 loss=1.191129 err=1.191129
I 2015-05-26 06:53:18 theanets.trainer:168 RmsProp 124 loss=1.151279 err=1.151279
I 2015-05-26 06:53:32 theanets.trainer:168 RmsProp 125 loss=1.163592 err=1.163592
I 2015-05-26 06:53:47 theanets.trainer:168 RmsProp 126 loss=1.088968 err=1.088968
I 2015-05-26 06:54:02 theanets.trainer:168 RmsProp 127 loss=1.072212 err=1.072212
I 2015-05-26 06:54:17 theanets.trainer:168 RmsProp 128 loss=1.161730 err=1.161730
I 2015-05-26 06:54:32 theanets.trainer:168 RmsProp 129 loss=1.077577 err=1.077577
I 2015-05-26 06:54:47 theanets.trainer:168 RmsProp 130 loss=1.048373 err=1.048373
I 2015-05-26 06:54:48 theanets.trainer:168 validation 13 loss=685.878418 err=685.878418 *
I 2015-05-26 06:55:03 theanets.trainer:168 RmsProp 131 loss=1.070557 err=1.070557
I 2015-05-26 06:55:18 theanets.trainer:168 RmsProp 132 loss=1.086207 err=1.086207
I 2015-05-26 06:55:33 theanets.trainer:168 RmsProp 133 loss=1.044563 err=1.044563
I 2015-05-26 06:55:48 theanets.trainer:168 RmsProp 134 loss=1.033762 err=1.033762
I 2015-05-26 06:56:03 theanets.trainer:168 RmsProp 135 loss=1.037079 err=1.037079
I 2015-05-26 06:56:19 theanets.trainer:168 RmsProp 136 loss=1.016384 err=1.016384
I 2015-05-26 06:56:34 theanets.trainer:168 RmsProp 137 loss=1.004328 err=1.004328
I 2015-05-26 06:56:49 theanets.trainer:168 RmsProp 138 loss=0.995665 err=0.995665
I 2015-05-26 06:57:04 theanets.trainer:168 RmsProp 139 loss=1.019878 err=1.019878
I 2015-05-26 06:57:19 theanets.trainer:168 RmsProp 140 loss=0.996700 err=0.996700
I 2015-05-26 06:57:19 theanets.trainer:168 validation 14 loss=683.868286 err=683.868286 *
I 2015-05-26 06:57:34 theanets.trainer:168 RmsProp 141 loss=0.990528 err=0.990528
I 2015-05-26 06:57:48 theanets.trainer:168 RmsProp 142 loss=0.950429 err=0.950429
I 2015-05-26 06:58:03 theanets.trainer:168 RmsProp 143 loss=0.964684 err=0.964684
I 2015-05-26 06:58:16 theanets.trainer:168 RmsProp 144 loss=0.933358 err=0.933358
I 2015-05-26 06:58:29 theanets.trainer:168 RmsProp 145 loss=0.938540 err=0.938540
I 2015-05-26 06:58:43 theanets.trainer:168 RmsProp 146 loss=1.005923 err=1.005923
I 2015-05-26 06:58:57 theanets.trainer:168 RmsProp 147 loss=0.955471 err=0.955471
I 2015-05-26 06:59:10 theanets.trainer:168 RmsProp 148 loss=0.894911 err=0.894911
I 2015-05-26 06:59:23 theanets.trainer:168 RmsProp 149 loss=0.927781 err=0.927781
I 2015-05-26 06:59:37 theanets.trainer:168 RmsProp 150 loss=0.915691 err=0.915691
I 2015-05-26 06:59:38 theanets.trainer:168 validation 15 loss=682.441528 err=682.441528 *
I 2015-05-26 06:59:51 theanets.trainer:168 RmsProp 151 loss=0.904948 err=0.904948
I 2015-05-26 07:00:05 theanets.trainer:168 RmsProp 152 loss=0.928416 err=0.928416
I 2015-05-26 07:00:18 theanets.trainer:168 RmsProp 153 loss=0.899272 err=0.899272
I 2015-05-26 07:00:32 theanets.trainer:168 RmsProp 154 loss=0.910035 err=0.910035
I 2015-05-26 07:00:45 theanets.trainer:168 RmsProp 155 loss=0.886716 err=0.886716
I 2015-05-26 07:00:59 theanets.trainer:168 RmsProp 156 loss=0.878755 err=0.878755
I 2015-05-26 07:01:13 theanets.trainer:168 RmsProp 157 loss=0.867860 err=0.867860
I 2015-05-26 07:01:26 theanets.trainer:168 RmsProp 158 loss=0.899506 err=0.899506
I 2015-05-26 07:01:40 theanets.trainer:168 RmsProp 159 loss=0.841994 err=0.841994
I 2015-05-26 07:01:53 theanets.trainer:168 RmsProp 160 loss=0.885051 err=0.885051
I 2015-05-26 07:01:54 theanets.trainer:168 validation 16 loss=680.937683 err=680.937683 *
I 2015-05-26 07:02:08 theanets.trainer:168 RmsProp 161 loss=0.849217 err=0.849217
I 2015-05-26 07:02:22 theanets.trainer:168 RmsProp 162 loss=0.815735 err=0.815735
I 2015-05-26 07:02:35 theanets.trainer:168 RmsProp 163 loss=0.850059 err=0.850059
I 2015-05-26 07:02:49 theanets.trainer:168 RmsProp 164 loss=0.846290 err=0.846290
I 2015-05-26 07:03:03 theanets.trainer:168 RmsProp 165 loss=0.835514 err=0.835514
I 2015-05-26 07:03:17 theanets.trainer:168 RmsProp 166 loss=0.797002 err=0.797002
I 2015-05-26 07:03:31 theanets.trainer:168 RmsProp 167 loss=0.840791 err=0.840791
I 2015-05-26 07:03:44 theanets.trainer:168 RmsProp 168 loss=0.829357 err=0.829357
I 2015-05-26 07:03:58 theanets.trainer:168 RmsProp 169 loss=0.803151 err=0.803151
I 2015-05-26 07:04:12 theanets.trainer:168 RmsProp 170 loss=0.839284 err=0.839284
I 2015-05-26 07:04:13 theanets.trainer:168 validation 17 loss=681.454529 err=681.454529
I 2015-05-26 07:04:26 theanets.trainer:168 RmsProp 171 loss=0.778120 err=0.778120
I 2015-05-26 07:04:40 theanets.trainer:168 RmsProp 172 loss=0.793842 err=0.793842
I 2015-05-26 07:04:53 theanets.trainer:168 RmsProp 173 loss=0.790173 err=0.790173
I 2015-05-26 07:05:06 theanets.trainer:168 RmsProp 174 loss=0.792217 err=0.792217
I 2015-05-26 07:05:19 theanets.trainer:168 RmsProp 175 loss=0.788565 err=0.788565
I 2015-05-26 07:05:32 theanets.trainer:168 RmsProp 176 loss=0.778006 err=0.778006
I 2015-05-26 07:05:45 theanets.trainer:168 RmsProp 177 loss=0.780895 err=0.780895
I 2015-05-26 07:05:58 theanets.trainer:168 RmsProp 178 loss=0.792311 err=0.792311
I 2015-05-26 07:06:11 theanets.trainer:168 RmsProp 179 loss=0.773924 err=0.773924
I 2015-05-26 07:06:24 theanets.trainer:168 RmsProp 180 loss=0.759084 err=0.759084
I 2015-05-26 07:06:25 theanets.trainer:168 validation 18 loss=679.815430 err=679.815430 *
I 2015-05-26 07:06:38 theanets.trainer:168 RmsProp 181 loss=0.740972 err=0.740972
I 2015-05-26 07:06:52 theanets.trainer:168 RmsProp 182 loss=0.744957 err=0.744957
I 2015-05-26 07:07:06 theanets.trainer:168 RmsProp 183 loss=0.748859 err=0.748859
I 2015-05-26 07:07:20 theanets.trainer:168 RmsProp 184 loss=0.761608 err=0.761608
I 2015-05-26 07:07:34 theanets.trainer:168 RmsProp 185 loss=0.781143 err=0.781143
I 2015-05-26 07:07:47 theanets.trainer:168 RmsProp 186 loss=0.726859 err=0.726859
I 2015-05-26 07:08:01 theanets.trainer:168 RmsProp 187 loss=0.692021 err=0.692021
I 2015-05-26 07:08:15 theanets.trainer:168 RmsProp 188 loss=0.710292 err=0.710292
I 2015-05-26 07:08:29 theanets.trainer:168 RmsProp 189 loss=0.719045 err=0.719045
I 2015-05-26 07:08:43 theanets.trainer:168 RmsProp 190 loss=0.719214 err=0.719214
I 2015-05-26 07:08:43 theanets.trainer:168 validation 19 loss=677.241089 err=677.241089 *
I 2015-05-26 07:08:57 theanets.trainer:168 RmsProp 191 loss=0.737268 err=0.737268
I 2015-05-26 07:09:10 theanets.trainer:168 RmsProp 192 loss=0.695895 err=0.695895
I 2015-05-26 07:09:24 theanets.trainer:168 RmsProp 193 loss=0.722566 err=0.722566
I 2015-05-26 07:09:38 theanets.trainer:168 RmsProp 194 loss=0.697658 err=0.697658
I 2015-05-26 07:09:51 theanets.trainer:168 RmsProp 195 loss=0.679362 err=0.679362
I 2015-05-26 07:10:05 theanets.trainer:168 RmsProp 196 loss=0.742132 err=0.742132
I 2015-05-26 07:10:19 theanets.trainer:168 RmsProp 197 loss=0.728080 err=0.728080
I 2015-05-26 07:10:33 theanets.trainer:168 RmsProp 198 loss=0.645424 err=0.645424
I 2015-05-26 07:10:46 theanets.trainer:168 RmsProp 199 loss=0.698107 err=0.698107
I 2015-05-26 07:11:00 theanets.trainer:168 RmsProp 200 loss=0.683709 err=0.683709
I 2015-05-26 07:11:01 theanets.trainer:168 validation 20 loss=676.726257 err=676.726257 *
I 2015-05-26 07:11:14 theanets.trainer:168 RmsProp 201 loss=0.653340 err=0.653340
I 2015-05-26 07:11:28 theanets.trainer:168 RmsProp 202 loss=0.673770 err=0.673770
I 2015-05-26 07:11:41 theanets.trainer:168 RmsProp 203 loss=0.658039 err=0.658039
I 2015-05-26 07:11:55 theanets.trainer:168 RmsProp 204 loss=0.663963 err=0.663963
I 2015-05-26 07:12:07 theanets.trainer:168 RmsProp 205 loss=0.653453 err=0.653453
I 2015-05-26 07:12:20 theanets.trainer:168 RmsProp 206 loss=0.640683 err=0.640683
I 2015-05-26 07:12:33 theanets.trainer:168 RmsProp 207 loss=0.644931 err=0.644931
I 2015-05-26 07:12:46 theanets.trainer:168 RmsProp 208 loss=0.657923 err=0.657923
I 2015-05-26 07:12:59 theanets.trainer:168 RmsProp 209 loss=0.649585 err=0.649585
I 2015-05-26 07:13:12 theanets.trainer:168 RmsProp 210 loss=0.616194 err=0.616194
I 2015-05-26 07:13:13 theanets.trainer:168 validation 21 loss=676.522217 err=676.522217 *
I 2015-05-26 07:13:25 theanets.trainer:168 RmsProp 211 loss=0.655658 err=0.655658
I 2015-05-26 07:13:39 theanets.trainer:168 RmsProp 212 loss=0.666035 err=0.666035
I 2015-05-26 07:13:52 theanets.trainer:168 RmsProp 213 loss=0.628800 err=0.628800
I 2015-05-26 07:14:05 theanets.trainer:168 RmsProp 214 loss=0.629316 err=0.629316
I 2015-05-26 07:14:19 theanets.trainer:168 RmsProp 215 loss=0.612424 err=0.612424
I 2015-05-26 07:14:33 theanets.trainer:168 RmsProp 216 loss=0.633987 err=0.633987
I 2015-05-26 07:14:46 theanets.trainer:168 RmsProp 217 loss=0.614521 err=0.614521
I 2015-05-26 07:15:00 theanets.trainer:168 RmsProp 218 loss=0.605921 err=0.605921
I 2015-05-26 07:15:14 theanets.trainer:168 RmsProp 219 loss=0.628876 err=0.628876
I 2015-05-26 07:15:27 theanets.trainer:168 RmsProp 220 loss=0.603373 err=0.603373
I 2015-05-26 07:15:28 theanets.trainer:168 validation 22 loss=674.846985 err=674.846985 *
I 2015-05-26 07:15:42 theanets.trainer:168 RmsProp 221 loss=0.632408 err=0.632408
I 2015-05-26 07:15:55 theanets.trainer:168 RmsProp 222 loss=0.599982 err=0.599982
I 2015-05-26 07:16:09 theanets.trainer:168 RmsProp 223 loss=0.607412 err=0.607412
I 2015-05-26 07:16:22 theanets.trainer:168 RmsProp 224 loss=0.594991 err=0.594991
I 2015-05-26 07:16:36 theanets.trainer:168 RmsProp 225 loss=0.600993 err=0.600993
I 2015-05-26 07:16:50 theanets.trainer:168 RmsProp 226 loss=0.580985 err=0.580985
I 2015-05-26 07:17:03 theanets.trainer:168 RmsProp 227 loss=0.588036 err=0.588036
I 2015-05-26 07:17:17 theanets.trainer:168 RmsProp 228 loss=0.586103 err=0.586103
I 2015-05-26 07:17:30 theanets.trainer:168 RmsProp 229 loss=0.580392 err=0.580392
I 2015-05-26 07:17:44 theanets.trainer:168 RmsProp 230 loss=0.587953 err=0.587953
I 2015-05-26 07:17:44 theanets.trainer:168 validation 23 loss=673.195984 err=673.195984 *
I 2015-05-26 07:17:58 theanets.trainer:168 RmsProp 231 loss=0.556043 err=0.556043
I 2015-05-26 07:18:12 theanets.trainer:168 RmsProp 232 loss=0.586487 err=0.586487
I 2015-05-26 07:18:25 theanets.trainer:168 RmsProp 233 loss=0.579758 err=0.579758
I 2015-05-26 07:18:38 theanets.trainer:168 RmsProp 234 loss=0.563885 err=0.563885
I 2015-05-26 07:18:52 theanets.trainer:168 RmsProp 235 loss=0.554861 err=0.554861
I 2015-05-26 07:19:06 theanets.trainer:168 RmsProp 236 loss=0.567526 err=0.567526
I 2015-05-26 07:19:19 theanets.trainer:168 RmsProp 237 loss=0.566711 err=0.566711
I 2015-05-26 07:19:33 theanets.trainer:168 RmsProp 238 loss=0.546822 err=0.546822
I 2015-05-26 07:19:47 theanets.trainer:168 RmsProp 239 loss=0.630629 err=0.630629
I 2015-05-26 07:20:01 theanets.trainer:168 RmsProp 240 loss=0.582232 err=0.582232
I 2015-05-26 07:20:01 theanets.trainer:168 validation 24 loss=672.096924 err=672.096924 *
I 2015-05-26 07:20:15 theanets.trainer:168 RmsProp 241 loss=0.555936 err=0.555936
I 2015-05-26 07:20:29 theanets.trainer:168 RmsProp 242 loss=0.548227 err=0.548227
I 2015-05-26 07:20:42 theanets.trainer:168 RmsProp 243 loss=0.550103 err=0.550103
I 2015-05-26 07:20:56 theanets.trainer:168 RmsProp 244 loss=0.546317 err=0.546317
I 2015-05-26 07:21:10 theanets.trainer:168 RmsProp 245 loss=0.565604 err=0.565604
I 2015-05-26 07:21:23 theanets.trainer:168 RmsProp 246 loss=0.540906 err=0.540906
I 2015-05-26 07:21:37 theanets.trainer:168 RmsProp 247 loss=0.532897 err=0.532897
I 2015-05-26 07:21:51 theanets.trainer:168 RmsProp 248 loss=0.536833 err=0.536833
I 2015-05-26 07:22:04 theanets.trainer:168 RmsProp 249 loss=0.530029 err=0.530029
I 2015-05-26 07:22:18 theanets.trainer:168 RmsProp 250 loss=0.526211 err=0.526211
I 2015-05-26 07:22:18 theanets.trainer:168 validation 25 loss=672.486450 err=672.486450
I 2015-05-26 07:22:32 theanets.trainer:168 RmsProp 251 loss=0.546833 err=0.546833
I 2015-05-26 07:22:45 theanets.trainer:168 RmsProp 252 loss=0.530115 err=0.530115
I 2015-05-26 07:22:59 theanets.trainer:168 RmsProp 253 loss=0.513830 err=0.513830
I 2015-05-26 07:23:13 theanets.trainer:168 RmsProp 254 loss=0.513637 err=0.513637
I 2015-05-26 07:23:27 theanets.trainer:168 RmsProp 255 loss=0.524890 err=0.524890
I 2015-05-26 07:23:41 theanets.trainer:168 RmsProp 256 loss=0.513238 err=0.513238
I 2015-05-26 07:23:55 theanets.trainer:168 RmsProp 257 loss=0.519700 err=0.519700
I 2015-05-26 07:24:08 theanets.trainer:168 RmsProp 258 loss=0.518761 err=0.518761
I 2015-05-26 07:24:22 theanets.trainer:168 RmsProp 259 loss=0.501430 err=0.501430
I 2015-05-26 07:24:36 theanets.trainer:168 RmsProp 260 loss=0.486579 err=0.486579
I 2015-05-26 07:24:37 theanets.trainer:168 validation 26 loss=670.511169 err=670.511169 *
I 2015-05-26 07:24:50 theanets.trainer:168 RmsProp 261 loss=0.534652 err=0.534652
I 2015-05-26 07:25:03 theanets.trainer:168 RmsProp 262 loss=0.490828 err=0.490828
I 2015-05-26 07:25:16 theanets.trainer:168 RmsProp 263 loss=0.520716 err=0.520716
I 2015-05-26 07:25:29 theanets.trainer:168 RmsProp 264 loss=0.489037 err=0.489037
I 2015-05-26 07:25:42 theanets.trainer:168 RmsProp 265 loss=0.512643 err=0.512643
I 2015-05-26 07:25:55 theanets.trainer:168 RmsProp 266 loss=0.514597 err=0.514597
I 2015-05-26 07:26:08 theanets.trainer:168 RmsProp 267 loss=0.473258 err=0.473258
I 2015-05-26 07:26:21 theanets.trainer:168 RmsProp 268 loss=0.483797 err=0.483797
I 2015-05-26 07:26:34 theanets.trainer:168 RmsProp 269 loss=0.499269 err=0.499269
I 2015-05-26 07:26:46 theanets.trainer:168 RmsProp 270 loss=0.466665 err=0.466665
I 2015-05-26 07:26:47 theanets.trainer:168 validation 27 loss=670.862793 err=670.862793
I 2015-05-26 07:27:00 theanets.trainer:168 RmsProp 271 loss=0.500029 err=0.500029
I 2015-05-26 07:27:12 theanets.trainer:168 RmsProp 272 loss=0.496801 err=0.496801
I 2015-05-26 07:27:25 theanets.trainer:168 RmsProp 273 loss=0.478670 err=0.478670
I 2015-05-26 07:27:37 theanets.trainer:168 RmsProp 274 loss=0.466085 err=0.466085
I 2015-05-26 07:27:50 theanets.trainer:168 RmsProp 275 loss=0.487656 err=0.487656
I 2015-05-26 07:28:02 theanets.trainer:168 RmsProp 276 loss=0.461660 err=0.461660
I 2015-05-26 07:28:15 theanets.trainer:168 RmsProp 277 loss=0.474151 err=0.474151
I 2015-05-26 07:28:27 theanets.trainer:168 RmsProp 278 loss=0.491881 err=0.491881
I 2015-05-26 07:28:40 theanets.trainer:168 RmsProp 279 loss=0.459812 err=0.459812
I 2015-05-26 07:28:53 theanets.trainer:168 RmsProp 280 loss=0.498308 err=0.498308
I 2015-05-26 07:28:53 theanets.trainer:168 validation 28 loss=669.910828 err=669.910828 *
I 2015-05-26 07:29:05 theanets.trainer:168 RmsProp 281 loss=0.461480 err=0.461480
I 2015-05-26 07:29:18 theanets.trainer:168 RmsProp 282 loss=0.475179 err=0.475179
I 2015-05-26 07:29:30 theanets.trainer:168 RmsProp 283 loss=0.455325 err=0.455325
I 2015-05-26 07:29:43 theanets.trainer:168 RmsProp 284 loss=0.484187 err=0.484187
I 2015-05-26 07:29:56 theanets.trainer:168 RmsProp 285 loss=0.477499 err=0.477499
I 2015-05-26 07:30:08 theanets.trainer:168 RmsProp 286 loss=0.447159 err=0.447159
I 2015-05-26 07:30:21 theanets.trainer:168 RmsProp 287 loss=0.465449 err=0.465449
I 2015-05-26 07:30:34 theanets.trainer:168 RmsProp 288 loss=0.441550 err=0.441550
I 2015-05-26 07:30:46 theanets.trainer:168 RmsProp 289 loss=0.461351 err=0.461351
I 2015-05-26 07:30:58 theanets.trainer:168 RmsProp 290 loss=0.480171 err=0.480171
I 2015-05-26 07:30:59 theanets.trainer:168 validation 29 loss=668.561646 err=668.561646 *
I 2015-05-26 07:31:12 theanets.trainer:168 RmsProp 291 loss=0.444212 err=0.444212
I 2015-05-26 07:31:24 theanets.trainer:168 RmsProp 292 loss=0.439852 err=0.439852
I 2015-05-26 07:31:36 theanets.trainer:168 RmsProp 293 loss=0.460285 err=0.460285
I 2015-05-26 07:31:48 theanets.trainer:168 RmsProp 294 loss=0.443331 err=0.443331
I 2015-05-26 07:32:00 theanets.trainer:168 RmsProp 295 loss=0.418040 err=0.418040
I 2015-05-26 07:32:12 theanets.trainer:168 RmsProp 296 loss=0.464147 err=0.464147
I 2015-05-26 07:32:24 theanets.trainer:168 RmsProp 297 loss=0.446566 err=0.446566
I 2015-05-26 07:32:36 theanets.trainer:168 RmsProp 298 loss=0.448974 err=0.448974
I 2015-05-26 07:32:48 theanets.trainer:168 RmsProp 299 loss=0.432794 err=0.432794
I 2015-05-26 07:33:00 theanets.trainer:168 RmsProp 300 loss=0.433401 err=0.433401
I 2015-05-26 07:33:00 theanets.trainer:168 validation 30 loss=668.746948 err=668.746948
I 2015-05-26 07:33:12 theanets.trainer:168 RmsProp 301 loss=0.443216 err=0.443216
I 2015-05-26 07:33:24 theanets.trainer:168 RmsProp 302 loss=0.412226 err=0.412226
I 2015-05-26 07:33:37 theanets.trainer:168 RmsProp 303 loss=0.448210 err=0.448210
I 2015-05-26 07:33:49 theanets.trainer:168 RmsProp 304 loss=0.424393 err=0.424393
I 2015-05-26 07:34:02 theanets.trainer:168 RmsProp 305 loss=0.431633 err=0.431633
I 2015-05-26 07:34:14 theanets.trainer:168 RmsProp 306 loss=0.437593 err=0.437593
I 2015-05-26 07:34:26 theanets.trainer:168 RmsProp 307 loss=0.442415 err=0.442415
I 2015-05-26 07:34:39 theanets.trainer:168 RmsProp 308 loss=0.426773 err=0.426773
I 2015-05-26 07:34:51 theanets.trainer:168 RmsProp 309 loss=0.425721 err=0.425721
I 2015-05-26 07:35:03 theanets.trainer:168 RmsProp 310 loss=0.406729 err=0.406729
I 2015-05-26 07:35:04 theanets.trainer:168 validation 31 loss=667.615417 err=667.615417 *
I 2015-05-26 07:35:17 theanets.trainer:168 RmsProp 311 loss=0.433979 err=0.433979
I 2015-05-26 07:35:29 theanets.trainer:168 RmsProp 312 loss=0.399792 err=0.399792
I 2015-05-26 07:35:41 theanets.trainer:168 RmsProp 313 loss=0.457030 err=0.457030
I 2015-05-26 07:35:54 theanets.trainer:168 RmsProp 314 loss=0.417912 err=0.417912
I 2015-05-26 07:36:06 theanets.trainer:168 RmsProp 315 loss=0.407904 err=0.407904
I 2015-05-26 07:36:19 theanets.trainer:168 RmsProp 316 loss=0.414221 err=0.414221
I 2015-05-26 07:36:31 theanets.trainer:168 RmsProp 317 loss=0.419201 err=0.419201
I 2015-05-26 07:36:43 theanets.trainer:168 RmsProp 318 loss=0.446268 err=0.446268
I 2015-05-26 07:36:56 theanets.trainer:168 RmsProp 319 loss=0.414525 err=0.414525
I 2015-05-26 07:37:08 theanets.trainer:168 RmsProp 320 loss=0.402128 err=0.402128
I 2015-05-26 07:37:09 theanets.trainer:168 validation 32 loss=667.772888 err=667.772888
I 2015-05-26 07:37:21 theanets.trainer:168 RmsProp 321 loss=0.401009 err=0.401009
I 2015-05-26 07:37:34 theanets.trainer:168 RmsProp 322 loss=0.397620 err=0.397620
I 2015-05-26 07:37:46 theanets.trainer:168 RmsProp 323 loss=0.416267 err=0.416267
I 2015-05-26 07:37:59 theanets.trainer:168 RmsProp 324 loss=0.408039 err=0.408039
I 2015-05-26 07:38:12 theanets.trainer:168 RmsProp 325 loss=0.390906 err=0.390906
I 2015-05-26 07:38:25 theanets.trainer:168 RmsProp 326 loss=0.402453 err=0.402453
I 2015-05-26 07:38:37 theanets.trainer:168 RmsProp 327 loss=0.400408 err=0.400408
I 2015-05-26 07:38:49 theanets.trainer:168 RmsProp 328 loss=0.410746 err=0.410746
I 2015-05-26 07:39:02 theanets.trainer:168 RmsProp 329 loss=0.395796 err=0.395796
I 2015-05-26 07:39:14 theanets.trainer:168 RmsProp 330 loss=0.400213 err=0.400213
I 2015-05-26 07:39:15 theanets.trainer:168 validation 33 loss=667.073242 err=667.073242 *
I 2015-05-26 07:39:27 theanets.trainer:168 RmsProp 331 loss=0.415070 err=0.415070
I 2015-05-26 07:39:40 theanets.trainer:168 RmsProp 332 loss=0.380561 err=0.380561
I 2015-05-26 07:39:53 theanets.trainer:168 RmsProp 333 loss=0.391341 err=0.391341
I 2015-05-26 07:40:06 theanets.trainer:168 RmsProp 334 loss=0.392147 err=0.392147
I 2015-05-26 07:40:19 theanets.trainer:168 RmsProp 335 loss=0.378197 err=0.378197
I 2015-05-26 07:40:32 theanets.trainer:168 RmsProp 336 loss=0.380328 err=0.380328
I 2015-05-26 07:40:45 theanets.trainer:168 RmsProp 337 loss=0.423311 err=0.423311
I 2015-05-26 07:40:58 theanets.trainer:168 RmsProp 338 loss=0.381381 err=0.381381
I 2015-05-26 07:41:11 theanets.trainer:168 RmsProp 339 loss=0.414476 err=0.414476
I 2015-05-26 07:41:24 theanets.trainer:168 RmsProp 340 loss=0.390993 err=0.390993
I 2015-05-26 07:41:24 theanets.trainer:168 validation 34 loss=667.770752 err=667.770752
I 2015-05-26 07:41:37 theanets.trainer:168 RmsProp 341 loss=0.378084 err=0.378084
I 2015-05-26 07:41:50 theanets.trainer:168 RmsProp 342 loss=0.387865 err=0.387865
I 2015-05-26 07:42:03 theanets.trainer:168 RmsProp 343 loss=0.377982 err=0.377982
I 2015-05-26 07:42:16 theanets.trainer:168 RmsProp 344 loss=0.371381 err=0.371381
I 2015-05-26 07:42:28 theanets.trainer:168 RmsProp 345 loss=0.395738 err=0.395738
I 2015-05-26 07:42:41 theanets.trainer:168 RmsProp 346 loss=0.369911 err=0.369911
I 2015-05-26 07:42:53 theanets.trainer:168 RmsProp 347 loss=0.408727 err=0.408727
I 2015-05-26 07:43:06 theanets.trainer:168 RmsProp 348 loss=0.378451 err=0.378451
I 2015-05-26 07:43:18 theanets.trainer:168 RmsProp 349 loss=0.374965 err=0.374965
I 2015-05-26 07:43:31 theanets.trainer:168 RmsProp 350 loss=0.364807 err=0.364807
I 2015-05-26 07:43:31 theanets.trainer:168 validation 35 loss=666.882263 err=666.882263 *
I 2015-05-26 07:43:44 theanets.trainer:168 RmsProp 351 loss=0.393398 err=0.393398
I 2015-05-26 07:43:56 theanets.trainer:168 RmsProp 352 loss=0.374737 err=0.374737
I 2015-05-26 07:44:08 theanets.trainer:168 RmsProp 353 loss=0.384343 err=0.384343
I 2015-05-26 07:44:20 theanets.trainer:168 RmsProp 354 loss=0.349384 err=0.349384
I 2015-05-26 07:44:33 theanets.trainer:168 RmsProp 355 loss=0.402770 err=0.402770
I 2015-05-26 07:44:45 theanets.trainer:168 RmsProp 356 loss=0.370314 err=0.370314
I 2015-05-26 07:44:57 theanets.trainer:168 RmsProp 357 loss=0.351582 err=0.351582
I 2015-05-26 07:45:09 theanets.trainer:168 RmsProp 358 loss=0.400200 err=0.400200
I 2015-05-26 07:45:22 theanets.trainer:168 RmsProp 359 loss=0.368585 err=0.368585
I 2015-05-26 07:45:34 theanets.trainer:168 RmsProp 360 loss=0.352414 err=0.352414
I 2015-05-26 07:45:35 theanets.trainer:168 validation 36 loss=666.515442 err=666.515442 *
I 2015-05-26 07:45:47 theanets.trainer:168 RmsProp 361 loss=0.389471 err=0.389471
I 2015-05-26 07:45:58 theanets.trainer:168 RmsProp 362 loss=0.373062 err=0.373062
I 2015-05-26 07:46:10 theanets.trainer:168 RmsProp 363 loss=0.385174 err=0.385174
I 2015-05-26 07:46:23 theanets.trainer:168 RmsProp 364 loss=0.338858 err=0.338858
I 2015-05-26 07:46:35 theanets.trainer:168 RmsProp 365 loss=0.354259 err=0.354259
I 2015-05-26 07:46:47 theanets.trainer:168 RmsProp 366 loss=0.365373 err=0.365373
I 2015-05-26 07:46:59 theanets.trainer:168 RmsProp 367 loss=0.368949 err=0.368949
I 2015-05-26 07:47:11 theanets.trainer:168 RmsProp 368 loss=0.339209 err=0.339209
I 2015-05-26 07:47:23 theanets.trainer:168 RmsProp 369 loss=0.360076 err=0.360076
I 2015-05-26 07:47:36 theanets.trainer:168 RmsProp 370 loss=0.353010 err=0.353010
I 2015-05-26 07:47:37 theanets.trainer:168 validation 37 loss=665.672668 err=665.672668 *
I 2015-05-26 07:47:49 theanets.trainer:168 RmsProp 371 loss=0.363721 err=0.363721
I 2015-05-26 07:48:01 theanets.trainer:168 RmsProp 372 loss=0.341256 err=0.341256
I 2015-05-26 07:48:13 theanets.trainer:168 RmsProp 373 loss=0.342543 err=0.342543
I 2015-05-26 07:48:25 theanets.trainer:168 RmsProp 374 loss=0.358525 err=0.358525
I 2015-05-26 07:48:37 theanets.trainer:168 RmsProp 375 loss=0.368172 err=0.368172
I 2015-05-26 07:48:49 theanets.trainer:168 RmsProp 376 loss=0.340233 err=0.340233
I 2015-05-26 07:49:01 theanets.trainer:168 RmsProp 377 loss=0.338425 err=0.338425
I 2015-05-26 07:49:13 theanets.trainer:168 RmsProp 378 loss=0.354175 err=0.354175
I 2015-05-26 07:49:25 theanets.trainer:168 RmsProp 379 loss=0.355064 err=0.355064
I 2015-05-26 07:49:37 theanets.trainer:168 RmsProp 380 loss=0.332924 err=0.332924
I 2015-05-26 07:49:38 theanets.trainer:168 validation 38 loss=665.516907 err=665.516907 *
I 2015-05-26 07:49:50 theanets.trainer:168 RmsProp 381 loss=0.324929 err=0.324929
I 2015-05-26 07:50:02 theanets.trainer:168 RmsProp 382 loss=0.369233 err=0.369233
I 2015-05-26 07:50:13 theanets.trainer:168 RmsProp 383 loss=0.344423 err=0.344423
I 2015-05-26 07:50:24 theanets.trainer:168 RmsProp 384 loss=0.341937 err=0.341937
I 2015-05-26 07:50:35 theanets.trainer:168 RmsProp 385 loss=0.348728 err=0.348728
I 2015-05-26 07:50:47 theanets.trainer:168 RmsProp 386 loss=0.342294 err=0.342294
I 2015-05-26 07:50:58 theanets.trainer:168 RmsProp 387 loss=0.332278 err=0.332278
I 2015-05-26 07:51:09 theanets.trainer:168 RmsProp 388 loss=0.330044 err=0.330044
I 2015-05-26 07:51:21 theanets.trainer:168 RmsProp 389 loss=0.334628 err=0.334628
I 2015-05-26 07:51:32 theanets.trainer:168 RmsProp 390 loss=0.330440 err=0.330440
I 2015-05-26 07:51:32 theanets.trainer:168 validation 39 loss=665.086060 err=665.086060 *
I 2015-05-26 07:51:43 theanets.trainer:168 RmsProp 391 loss=0.331576 err=0.331576
I 2015-05-26 07:51:55 theanets.trainer:168 RmsProp 392 loss=0.327488 err=0.327488
I 2015-05-26 07:52:07 theanets.trainer:168 RmsProp 393 loss=0.339296 err=0.339296
I 2015-05-26 07:52:19 theanets.trainer:168 RmsProp 394 loss=0.342333 err=0.342333
I 2015-05-26 07:52:31 theanets.trainer:168 RmsProp 395 loss=0.301267 err=0.301267
I 2015-05-26 07:52:43 theanets.trainer:168 RmsProp 396 loss=0.371177 err=0.371177
I 2015-05-26 07:52:55 theanets.trainer:168 RmsProp 397 loss=0.346771 err=0.346771
I 2015-05-26 07:53:07 theanets.trainer:168 RmsProp 398 loss=0.317588 err=0.317588
I 2015-05-26 07:53:20 theanets.trainer:168 RmsProp 399 loss=0.304813 err=0.304813
I 2015-05-26 07:53:32 theanets.trainer:168 RmsProp 400 loss=0.357203 err=0.357203
I 2015-05-26 07:53:33 theanets.trainer:168 validation 40 loss=665.149719 err=665.149719
I 2015-05-26 07:53:45 theanets.trainer:168 RmsProp 401 loss=0.329784 err=0.329784
I 2015-05-26 07:53:57 theanets.trainer:168 RmsProp 402 loss=0.328723 err=0.328723
I 2015-05-26 07:54:10 theanets.trainer:168 RmsProp 403 loss=0.331826 err=0.331826
I 2015-05-26 07:54:22 theanets.trainer:168 RmsProp 404 loss=0.321635 err=0.321635
I 2015-05-26 07:54:34 theanets.trainer:168 RmsProp 405 loss=0.323147 err=0.323147
I 2015-05-26 07:54:46 theanets.trainer:168 RmsProp 406 loss=0.305833 err=0.305833
I 2015-05-26 07:54:59 theanets.trainer:168 RmsProp 407 loss=0.325238 err=0.325238
I 2015-05-26 07:55:11 theanets.trainer:168 RmsProp 408 loss=0.310925 err=0.310925
I 2015-05-26 07:55:23 theanets.trainer:168 RmsProp 409 loss=0.325572 err=0.325572
I 2015-05-26 07:55:35 theanets.trainer:168 RmsProp 410 loss=0.317350 err=0.317350
I 2015-05-26 07:55:36 theanets.trainer:168 validation 41 loss=664.875671 err=664.875671 *
I 2015-05-26 07:55:48 theanets.trainer:168 RmsProp 411 loss=0.326125 err=0.326125
I 2015-05-26 07:56:00 theanets.trainer:168 RmsProp 412 loss=0.321212 err=0.321212
I 2015-05-26 07:56:13 theanets.trainer:168 RmsProp 413 loss=0.320193 err=0.320193
I 2015-05-26 07:56:25 theanets.trainer:168 RmsProp 414 loss=0.315453 err=0.315453
I 2015-05-26 07:56:37 theanets.trainer:168 RmsProp 415 loss=0.312548 err=0.312548
I 2015-05-26 07:56:49 theanets.trainer:168 RmsProp 416 loss=0.316472 err=0.316472
I 2015-05-26 07:57:02 theanets.trainer:168 RmsProp 417 loss=0.307318 err=0.307318
I 2015-05-26 07:57:14 theanets.trainer:168 RmsProp 418 loss=0.317265 err=0.317265
I 2015-05-26 07:57:27 theanets.trainer:168 RmsProp 419 loss=0.312200 err=0.312200
I 2015-05-26 07:57:39 theanets.trainer:168 RmsProp 420 loss=0.318005 err=0.318005
I 2015-05-26 07:57:40 theanets.trainer:168 validation 42 loss=664.204834 err=664.204834 *
I 2015-05-26 07:57:52 theanets.trainer:168 RmsProp 421 loss=0.301611 err=0.301611
I 2015-05-26 07:58:03 theanets.trainer:168 RmsProp 422 loss=0.307006 err=0.307006
I 2015-05-26 07:58:15 theanets.trainer:168 RmsProp 423 loss=0.314101 err=0.314101
I 2015-05-26 07:58:27 theanets.trainer:168 RmsProp 424 loss=0.318430 err=0.318430
I 2015-05-26 07:58:38 theanets.trainer:168 RmsProp 425 loss=0.324090 err=0.324090
I 2015-05-26 07:58:50 theanets.trainer:168 RmsProp 426 loss=0.299366 err=0.299366
I 2015-05-26 07:59:02 theanets.trainer:168 RmsProp 427 loss=0.303938 err=0.303938
I 2015-05-26 07:59:13 theanets.trainer:168 RmsProp 428 loss=0.308571 err=0.308571
I 2015-05-26 07:59:25 theanets.trainer:168 RmsProp 429 loss=0.297330 err=0.297330
I 2015-05-26 07:59:36 theanets.trainer:168 RmsProp 430 loss=0.295298 err=0.295298
I 2015-05-26 07:59:37 theanets.trainer:168 validation 43 loss=663.540344 err=663.540344 *
I 2015-05-26 07:59:49 theanets.trainer:168 RmsProp 431 loss=0.283993 err=0.283993
I 2015-05-26 08:00:01 theanets.trainer:168 RmsProp 432 loss=0.311501 err=0.311501
I 2015-05-26 08:00:13 theanets.trainer:168 RmsProp 433 loss=0.288601 err=0.288601
I 2015-05-26 08:00:25 theanets.trainer:168 RmsProp 434 loss=0.320286 err=0.320286
I 2015-05-26 08:00:37 theanets.trainer:168 RmsProp 435 loss=0.309680 err=0.309680
I 2015-05-26 08:00:49 theanets.trainer:168 RmsProp 436 loss=0.309395 err=0.309395
I 2015-05-26 08:01:02 theanets.trainer:168 RmsProp 437 loss=0.286282 err=0.286282
I 2015-05-26 08:01:14 theanets.trainer:168 RmsProp 438 loss=0.281750 err=0.281750
I 2015-05-26 08:01:26 theanets.trainer:168 RmsProp 439 loss=0.299531 err=0.299531
I 2015-05-26 08:01:39 theanets.trainer:168 RmsProp 440 loss=0.297450 err=0.297450
I 2015-05-26 08:01:39 theanets.trainer:168 validation 44 loss=663.949829 err=663.949829
I 2015-05-26 08:01:51 theanets.trainer:168 RmsProp 441 loss=0.290800 err=0.290800
I 2015-05-26 08:02:02 theanets.trainer:168 RmsProp 442 loss=0.288618 err=0.288618
I 2015-05-26 08:02:12 theanets.trainer:168 RmsProp 443 loss=0.296075 err=0.296075
I 2015-05-26 08:02:22 theanets.trainer:168 RmsProp 444 loss=0.290178 err=0.290178
I 2015-05-26 08:02:33 theanets.trainer:168 RmsProp 445 loss=0.290845 err=0.290845
I 2015-05-26 08:02:43 theanets.trainer:168 RmsProp 446 loss=0.286003 err=0.286003
I 2015-05-26 08:02:54 theanets.trainer:168 RmsProp 447 loss=0.279087 err=0.279087
I 2015-05-26 08:03:04 theanets.trainer:168 RmsProp 448 loss=0.284276 err=0.284276
I 2015-05-26 08:03:15 theanets.trainer:168 RmsProp 449 loss=0.291325 err=0.291325
I 2015-05-26 08:03:25 theanets.trainer:168 RmsProp 450 loss=0.303375 err=0.303375
I 2015-05-26 08:03:26 theanets.trainer:168 validation 45 loss=663.997009 err=663.997009
I 2015-05-26 08:03:36 theanets.trainer:168 RmsProp 451 loss=0.291970 err=0.291970
I 2015-05-26 08:03:47 theanets.trainer:168 RmsProp 452 loss=0.298786 err=0.298786
I 2015-05-26 08:03:57 theanets.trainer:168 RmsProp 453 loss=0.274203 err=0.274203
I 2015-05-26 08:04:08 theanets.trainer:168 RmsProp 454 loss=0.279968 err=0.279968
I 2015-05-26 08:04:19 theanets.trainer:168 RmsProp 455 loss=0.294473 err=0.294473
I 2015-05-26 08:04:30 theanets.trainer:168 RmsProp 456 loss=0.289232 err=0.289232
I 2015-05-26 08:04:41 theanets.trainer:168 RmsProp 457 loss=0.288276 err=0.288276
I 2015-05-26 08:04:52 theanets.trainer:168 RmsProp 458 loss=0.268814 err=0.268814
I 2015-05-26 08:05:04 theanets.trainer:168 RmsProp 459 loss=0.269257 err=0.269257
I 2015-05-26 08:05:15 theanets.trainer:168 RmsProp 460 loss=0.297887 err=0.297887
I 2015-05-26 08:05:15 theanets.trainer:168 validation 46 loss=663.204773 err=663.204773 *
I 2015-05-26 08:05:26 theanets.trainer:168 RmsProp 461 loss=0.281416 err=0.281416
I 2015-05-26 08:05:37 theanets.trainer:168 RmsProp 462 loss=0.274452 err=0.274452
I 2015-05-26 08:05:48 theanets.trainer:168 RmsProp 463 loss=0.275019 err=0.275019
I 2015-05-26 08:05:59 theanets.trainer:168 RmsProp 464 loss=0.293487 err=0.293487
I 2015-05-26 08:06:10 theanets.trainer:168 RmsProp 465 loss=0.276278 err=0.276278
I 2015-05-26 08:06:21 theanets.trainer:168 RmsProp 466 loss=0.275353 err=0.275353
I 2015-05-26 08:06:32 theanets.trainer:168 RmsProp 467 loss=0.273783 err=0.273783
I 2015-05-26 08:06:43 theanets.trainer:168 RmsProp 468 loss=0.286141 err=0.286141
I 2015-05-26 08:06:55 theanets.trainer:168 RmsProp 469 loss=0.272486 err=0.272486
I 2015-05-26 08:07:06 theanets.trainer:168 RmsProp 470 loss=0.276417 err=0.276417
I 2015-05-26 08:07:06 theanets.trainer:168 validation 47 loss=662.799438 err=662.799438 *
I 2015-05-26 08:07:17 theanets.trainer:168 RmsProp 471 loss=0.269601 err=0.269601
I 2015-05-26 08:07:28 theanets.trainer:168 RmsProp 472 loss=0.285893 err=0.285893
I 2015-05-26 08:07:38 theanets.trainer:168 RmsProp 473 loss=0.272085 err=0.272085
I 2015-05-26 08:07:49 theanets.trainer:168 RmsProp 474 loss=0.276954 err=0.276954
I 2015-05-26 08:07:59 theanets.trainer:168 RmsProp 475 loss=0.265491 err=0.265491
I 2015-05-26 08:08:09 theanets.trainer:168 RmsProp 476 loss=0.258537 err=0.258537
I 2015-05-26 08:08:20 theanets.trainer:168 RmsProp 477 loss=0.275099 err=0.275099
I 2015-05-26 08:08:30 theanets.trainer:168 RmsProp 478 loss=0.260114 err=0.260114
I 2015-05-26 08:08:40 theanets.trainer:168 RmsProp 479 loss=0.283695 err=0.283695
I 2015-05-26 08:08:51 theanets.trainer:168 RmsProp 480 loss=0.254025 err=0.254025
I 2015-05-26 08:08:51 theanets.trainer:168 validation 48 loss=662.789062 err=662.789062 *
I 2015-05-26 08:09:01 theanets.trainer:168 RmsProp 481 loss=0.288819 err=0.288819
I 2015-05-26 08:09:12 theanets.trainer:168 RmsProp 482 loss=0.261903 err=0.261903
I 2015-05-26 08:09:23 theanets.trainer:168 RmsProp 483 loss=0.268447 err=0.268447
I 2015-05-26 08:09:34 theanets.trainer:168 RmsProp 484 loss=0.303065 err=0.303065
I 2015-05-26 08:09:46 theanets.trainer:168 RmsProp 485 loss=0.258608 err=0.258608
I 2015-05-26 08:09:57 theanets.trainer:168 RmsProp 486 loss=0.270123 err=0.270123
I 2015-05-26 08:10:08 theanets.trainer:168 RmsProp 487 loss=0.306583 err=0.306583
I 2015-05-26 08:10:19 theanets.trainer:168 RmsProp 488 loss=0.262799 err=0.262799
I 2015-05-26 08:10:30 theanets.trainer:168 RmsProp 489 loss=0.268663 err=0.268663
I 2015-05-26 08:10:42 theanets.trainer:168 RmsProp 490 loss=0.264695 err=0.264695
I 2015-05-26 08:10:42 theanets.trainer:168 validation 49 loss=662.044067 err=662.044067 *
I 2015-05-26 08:10:53 theanets.trainer:168 RmsProp 491 loss=0.246409 err=0.246409
I 2015-05-26 08:11:04 theanets.trainer:168 RmsProp 492 loss=0.257686 err=0.257686
I 2015-05-26 08:11:15 theanets.trainer:168 RmsProp 493 loss=0.341549 err=0.341549
I 2015-05-26 08:11:26 theanets.trainer:168 RmsProp 494 loss=0.278918 err=0.278918
I 2015-05-26 08:11:37 theanets.trainer:168 RmsProp 495 loss=0.259881 err=0.259881
I 2015-05-26 08:11:48 theanets.trainer:168 RmsProp 496 loss=0.270498 err=0.270498
I 2015-05-26 08:11:59 theanets.trainer:168 RmsProp 497 loss=0.248150 err=0.248150
I 2015-05-26 08:12:10 theanets.trainer:168 RmsProp 498 loss=0.265135 err=0.265135
I 2015-05-26 08:12:21 theanets.trainer:168 RmsProp 499 loss=0.267607 err=0.267607
I 2015-05-26 08:12:33 theanets.trainer:168 RmsProp 500 loss=0.256863 err=0.256863
I 2015-05-26 08:12:33 theanets.trainer:168 validation 50 loss=660.857788 err=660.857788 *
I 2015-05-26 08:12:44 theanets.trainer:168 RmsProp 501 loss=0.264126 err=0.264126
I 2015-05-26 08:12:55 theanets.trainer:168 RmsProp 502 loss=0.267074 err=0.267074
I 2015-05-26 08:13:06 theanets.trainer:168 RmsProp 503 loss=0.257099 err=0.257099
I 2015-05-26 08:13:18 theanets.trainer:168 RmsProp 504 loss=0.252937 err=0.252937
I 2015-05-26 08:13:29 theanets.trainer:168 RmsProp 505 loss=0.256855 err=0.256855
I 2015-05-26 08:13:41 theanets.trainer:168 RmsProp 506 loss=0.261822 err=0.261822
I 2015-05-26 08:13:52 theanets.trainer:168 RmsProp 507 loss=0.268637 err=0.268637
I 2015-05-26 08:14:03 theanets.trainer:168 RmsProp 508 loss=0.254482 err=0.254482
I 2015-05-26 08:14:13 theanets.trainer:168 RmsProp 509 loss=0.245122 err=0.245122
I 2015-05-26 08:14:25 theanets.trainer:168 RmsProp 510 loss=0.244311 err=0.244311
I 2015-05-26 08:14:25 theanets.trainer:168 validation 51 loss=661.343384 err=661.343384
I 2015-05-26 08:14:36 theanets.trainer:168 RmsProp 511 loss=0.231159 err=0.231159
I 2015-05-26 08:14:47 theanets.trainer:168 RmsProp 512 loss=0.293294 err=0.293294
I 2015-05-26 08:14:58 theanets.trainer:168 RmsProp 513 loss=0.263905 err=0.263905
I 2015-05-26 08:15:10 theanets.trainer:168 RmsProp 514 loss=0.237600 err=0.237600
I 2015-05-26 08:15:21 theanets.trainer:168 RmsProp 515 loss=0.245983 err=0.245983
I 2015-05-26 08:15:32 theanets.trainer:168 RmsProp 516 loss=0.279507 err=0.279507
I 2015-05-26 08:15:44 theanets.trainer:168 RmsProp 517 loss=0.242615 err=0.242615
I 2015-05-26 08:15:55 theanets.trainer:168 RmsProp 518 loss=0.274576 err=0.274576
I 2015-05-26 08:16:06 theanets.trainer:168 RmsProp 519 loss=0.254617 err=0.254617
I 2015-05-26 08:16:17 theanets.trainer:168 RmsProp 520 loss=0.241183 err=0.241183
I 2015-05-26 08:16:18 theanets.trainer:168 validation 52 loss=660.395874 err=660.395874 *
I 2015-05-26 08:16:29 theanets.trainer:168 RmsProp 521 loss=0.240841 err=0.240841
I 2015-05-26 08:16:40 theanets.trainer:168 RmsProp 522 loss=0.248842 err=0.248842
I 2015-05-26 08:16:51 theanets.trainer:168 RmsProp 523 loss=0.255890 err=0.255890
I 2015-05-26 08:17:02 theanets.trainer:168 RmsProp 524 loss=0.251917 err=0.251917
I 2015-05-26 08:17:13 theanets.trainer:168 RmsProp 525 loss=0.245898 err=0.245898
I 2015-05-26 08:17:24 theanets.trainer:168 RmsProp 526 loss=0.241530 err=0.241530
I 2015-05-26 08:17:35 theanets.trainer:168 RmsProp 527 loss=0.230695 err=0.230695
I 2015-05-26 08:17:46 theanets.trainer:168 RmsProp 528 loss=0.241013 err=0.241013
I 2015-05-26 08:17:57 theanets.trainer:168 RmsProp 529 loss=0.247884 err=0.247884
I 2015-05-26 08:18:08 theanets.trainer:168 RmsProp 530 loss=0.255404 err=0.255404
I 2015-05-26 08:18:09 theanets.trainer:168 validation 53 loss=659.979309 err=659.979309 *
I 2015-05-26 08:18:20 theanets.trainer:168 RmsProp 531 loss=0.238273 err=0.238273
I 2015-05-26 08:18:31 theanets.trainer:168 RmsProp 532 loss=0.224868 err=0.224868
I 2015-05-26 08:18:42 theanets.trainer:168 RmsProp 533 loss=0.289899 err=0.289899
I 2015-05-26 08:18:53 theanets.trainer:168 RmsProp 534 loss=0.243436 err=0.243436
I 2015-05-26 08:19:04 theanets.trainer:168 RmsProp 535 loss=0.236468 err=0.236468
I 2015-05-26 08:19:15 theanets.trainer:168 RmsProp 536 loss=0.220682 err=0.220682
I 2015-05-26 08:19:27 theanets.trainer:168 RmsProp 537 loss=0.256596 err=0.256596
I 2015-05-26 08:19:38 theanets.trainer:168 RmsProp 538 loss=0.237391 err=0.237391
I 2015-05-26 08:19:49 theanets.trainer:168 RmsProp 539 loss=0.235486 err=0.235486
I 2015-05-26 08:20:01 theanets.trainer:168 RmsProp 540 loss=0.245347 err=0.245347
I 2015-05-26 08:20:02 theanets.trainer:168 validation 54 loss=659.596802 err=659.596802 *
I 2015-05-26 08:20:13 theanets.trainer:168 RmsProp 541 loss=0.216747 err=0.216747
I 2015-05-26 08:20:24 theanets.trainer:168 RmsProp 542 loss=0.264465 err=0.264465
I 2015-05-26 08:20:36 theanets.trainer:168 RmsProp 543 loss=0.241349 err=0.241349
I 2015-05-26 08:20:48 theanets.trainer:168 RmsProp 544 loss=0.235038 err=0.235038
I 2015-05-26 08:20:59 theanets.trainer:168 RmsProp 545 loss=0.234887 err=0.234887
I 2015-05-26 08:21:10 theanets.trainer:168 RmsProp 546 loss=0.249413 err=0.249413
I 2015-05-26 08:21:22 theanets.trainer:168 RmsProp 547 loss=0.233545 err=0.233545
I 2015-05-26 08:21:33 theanets.trainer:168 RmsProp 548 loss=0.221073 err=0.221073
I 2015-05-26 08:21:44 theanets.trainer:168 RmsProp 549 loss=0.239275 err=0.239275
I 2015-05-26 08:21:56 theanets.trainer:168 RmsProp 550 loss=0.251770 err=0.251770
I 2015-05-26 08:21:56 theanets.trainer:168 validation 55 loss=659.692383 err=659.692383
I 2015-05-26 08:22:08 theanets.trainer:168 RmsProp 551 loss=0.230133 err=0.230133
I 2015-05-26 08:22:19 theanets.trainer:168 RmsProp 552 loss=0.223073 err=0.223073
I 2015-05-26 08:22:30 theanets.trainer:168 RmsProp 553 loss=0.245236 err=0.245236
I 2015-05-26 08:22:42 theanets.trainer:168 RmsProp 554 loss=0.234642 err=0.234642
I 2015-05-26 08:22:53 theanets.trainer:168 RmsProp 555 loss=0.229020 err=0.229020
I 2015-05-26 08:23:05 theanets.trainer:168 RmsProp 556 loss=0.236266 err=0.236266
I 2015-05-26 08:23:16 theanets.trainer:168 RmsProp 557 loss=0.215566 err=0.215566
I 2015-05-26 08:23:28 theanets.trainer:168 RmsProp 558 loss=0.283627 err=0.283627
I 2015-05-26 08:23:39 theanets.trainer:168 RmsProp 559 loss=0.223033 err=0.223033
I 2015-05-26 08:23:50 theanets.trainer:168 RmsProp 560 loss=0.224606 err=0.224606
I 2015-05-26 08:23:51 theanets.trainer:168 validation 56 loss=659.791931 err=659.791931
I 2015-05-26 08:24:02 theanets.trainer:168 RmsProp 561 loss=0.225125 err=0.225125
I 2015-05-26 08:24:13 theanets.trainer:168 RmsProp 562 loss=0.222368 err=0.222368
I 2015-05-26 08:24:25 theanets.trainer:168 RmsProp 563 loss=0.232562 err=0.232562
I 2015-05-26 08:24:36 theanets.trainer:168 RmsProp 564 loss=0.260386 err=0.260386
I 2015-05-26 08:24:47 theanets.trainer:168 RmsProp 565 loss=0.214647 err=0.214647
I 2015-05-26 08:24:59 theanets.trainer:168 RmsProp 566 loss=0.221531 err=0.221531
I 2015-05-26 08:25:10 theanets.trainer:168 RmsProp 567 loss=0.215810 err=0.215810
I 2015-05-26 08:25:21 theanets.trainer:168 RmsProp 568 loss=0.241130 err=0.241130
I 2015-05-26 08:25:33 theanets.trainer:168 RmsProp 569 loss=0.228643 err=0.228643
I 2015-05-26 08:25:44 theanets.trainer:168 RmsProp 570 loss=0.233474 err=0.233474
I 2015-05-26 08:25:45 theanets.trainer:168 validation 57 loss=659.712891 err=659.712891
I 2015-05-26 08:25:56 theanets.trainer:168 RmsProp 571 loss=0.219317 err=0.219317
I 2015-05-26 08:26:08 theanets.trainer:168 RmsProp 572 loss=0.216897 err=0.216897
I 2015-05-26 08:26:20 theanets.trainer:168 RmsProp 573 loss=0.226606 err=0.226606
I 2015-05-26 08:26:31 theanets.trainer:168 RmsProp 574 loss=0.236943 err=0.236943
I 2015-05-26 08:26:43 theanets.trainer:168 RmsProp 575 loss=0.213556 err=0.213556
I 2015-05-26 08:26:54 theanets.trainer:168 RmsProp 576 loss=0.237384 err=0.237384
I 2015-05-26 08:27:06 theanets.trainer:168 RmsProp 577 loss=0.227344 err=0.227344
I 2015-05-26 08:27:17 theanets.trainer:168 RmsProp 578 loss=0.228946 err=0.228946
I 2015-05-26 08:27:29 theanets.trainer:168 RmsProp 579 loss=0.218152 err=0.218152
I 2015-05-26 08:27:41 theanets.trainer:168 RmsProp 580 loss=0.217765 err=0.217765
I 2015-05-26 08:27:42 theanets.trainer:168 validation 58 loss=659.021790 err=659.021790 *
I 2015-05-26 08:27:53 theanets.trainer:168 RmsProp 581 loss=0.210522 err=0.210522
I 2015-05-26 08:28:04 theanets.trainer:168 RmsProp 582 loss=0.228424 err=0.228424
I 2015-05-26 08:28:15 theanets.trainer:168 RmsProp 583 loss=0.219413 err=0.219413
I 2015-05-26 08:28:27 theanets.trainer:168 RmsProp 584 loss=0.208527 err=0.208527
I 2015-05-26 08:28:38 theanets.trainer:168 RmsProp 585 loss=0.222201 err=0.222201
I 2015-05-26 08:28:49 theanets.trainer:168 RmsProp 586 loss=0.235134 err=0.235134
I 2015-05-26 08:29:01 theanets.trainer:168 RmsProp 587 loss=0.217302 err=0.217302
I 2015-05-26 08:29:12 theanets.trainer:168 RmsProp 588 loss=0.213902 err=0.213902
I 2015-05-26 08:29:24 theanets.trainer:168 RmsProp 589 loss=0.209485 err=0.209485
I 2015-05-26 08:29:35 theanets.trainer:168 RmsProp 590 loss=0.237688 err=0.237688
I 2015-05-26 08:29:36 theanets.trainer:168 validation 59 loss=658.598145 err=658.598145 *
I 2015-05-26 08:29:47 theanets.trainer:168 RmsProp 591 loss=0.213332 err=0.213332
I 2015-05-26 08:29:58 theanets.trainer:168 RmsProp 592 loss=0.249356 err=0.249356
I 2015-05-26 08:30:10 theanets.trainer:168 RmsProp 593 loss=0.240947 err=0.240947
I 2015-05-26 08:30:21 theanets.trainer:168 RmsProp 594 loss=0.200410 err=0.200410
I 2015-05-26 08:30:33 theanets.trainer:168 RmsProp 595 loss=0.230975 err=0.230975
I 2015-05-26 08:30:45 theanets.trainer:168 RmsProp 596 loss=0.198842 err=0.198842
I 2015-05-26 08:30:57 theanets.trainer:168 RmsProp 597 loss=0.205606 err=0.205606
I 2015-05-26 08:31:08 theanets.trainer:168 RmsProp 598 loss=0.235812 err=0.235812
I 2015-05-26 08:31:20 theanets.trainer:168 RmsProp 599 loss=0.216033 err=0.216033
I 2015-05-26 08:31:30 theanets.trainer:168 RmsProp 600 loss=0.214797 err=0.214797
I 2015-05-26 08:31:31 theanets.trainer:168 validation 60 loss=658.925720 err=658.925720
I 2015-05-26 08:31:41 theanets.trainer:168 RmsProp 601 loss=0.213344 err=0.213344
I 2015-05-26 08:31:51 theanets.trainer:168 RmsProp 602 loss=0.201446 err=0.201446
I 2015-05-26 08:32:02 theanets.trainer:168 RmsProp 603 loss=0.215550 err=0.215550
I 2015-05-26 08:32:12 theanets.trainer:168 RmsProp 604 loss=0.211579 err=0.211579
I 2015-05-26 08:32:23 theanets.trainer:168 RmsProp 605 loss=0.206458 err=0.206458
I 2015-05-26 08:32:34 theanets.trainer:168 RmsProp 606 loss=0.199638 err=0.199638
I 2015-05-26 08:32:44 theanets.trainer:168 RmsProp 607 loss=0.215199 err=0.215199
I 2015-05-26 08:32:55 theanets.trainer:168 RmsProp 608 loss=0.222150 err=0.222150
I 2015-05-26 08:33:05 theanets.trainer:168 RmsProp 609 loss=0.196972 err=0.196972
I 2015-05-26 08:33:16 theanets.trainer:168 RmsProp 610 loss=0.223229 err=0.223229
I 2015-05-26 08:33:17 theanets.trainer:168 validation 61 loss=658.361755 err=658.361755 *
I 2015-05-26 08:33:27 theanets.trainer:168 RmsProp 611 loss=0.198006 err=0.198006
I 2015-05-26 08:33:37 theanets.trainer:168 RmsProp 612 loss=0.194176 err=0.194176
I 2015-05-26 08:33:47 theanets.trainer:168 RmsProp 613 loss=0.227162 err=0.227162
I 2015-05-26 08:33:58 theanets.trainer:168 RmsProp 614 loss=0.210803 err=0.210803
I 2015-05-26 08:34:08 theanets.trainer:168 RmsProp 615 loss=0.224250 err=0.224250
I 2015-05-26 08:34:18 theanets.trainer:168 RmsProp 616 loss=0.212778 err=0.212778
I 2015-05-26 08:34:29 theanets.trainer:168 RmsProp 617 loss=0.203132 err=0.203132
I 2015-05-26 08:34:39 theanets.trainer:168 RmsProp 618 loss=0.210124 err=0.210124
I 2015-05-26 08:34:50 theanets.trainer:168 RmsProp 619 loss=0.225079 err=0.225079
I 2015-05-26 08:35:00 theanets.trainer:168 RmsProp 620 loss=0.192868 err=0.192868
I 2015-05-26 08:35:01 theanets.trainer:168 validation 62 loss=658.870361 err=658.870361
I 2015-05-26 08:35:12 theanets.trainer:168 RmsProp 621 loss=0.225805 err=0.225805
I 2015-05-26 08:35:22 theanets.trainer:168 RmsProp 622 loss=0.209388 err=0.209388
I 2015-05-26 08:35:33 theanets.trainer:168 RmsProp 623 loss=0.199605 err=0.199605
I 2015-05-26 08:35:43 theanets.trainer:168 RmsProp 624 loss=0.214829 err=0.214829
I 2015-05-26 08:35:53 theanets.trainer:168 RmsProp 625 loss=0.202276 err=0.202276
I 2015-05-26 08:36:04 theanets.trainer:168 RmsProp 626 loss=0.185741 err=0.185741
I 2015-05-26 08:36:14 theanets.trainer:168 RmsProp 627 loss=0.209296 err=0.209296
I 2015-05-26 08:36:24 theanets.trainer:168 RmsProp 628 loss=0.215394 err=0.215394
I 2015-05-26 08:36:35 theanets.trainer:168 RmsProp 629 loss=0.196671 err=0.196671
I 2015-05-26 08:36:45 theanets.trainer:168 RmsProp 630 loss=0.221728 err=0.221728
I 2015-05-26 08:36:46 theanets.trainer:168 validation 63 loss=659.007996 err=659.007996
I 2015-05-26 08:36:56 theanets.trainer:168 RmsProp 631 loss=0.202967 err=0.202967
I 2015-05-26 08:37:07 theanets.trainer:168 RmsProp 632 loss=0.194528 err=0.194528
I 2015-05-26 08:37:17 theanets.trainer:168 RmsProp 633 loss=0.204249 err=0.204249
I 2015-05-26 08:37:28 theanets.trainer:168 RmsProp 634 loss=0.204516 err=0.204516
I 2015-05-26 08:37:38 theanets.trainer:168 RmsProp 635 loss=0.223379 err=0.223379
I 2015-05-26 08:37:49 theanets.trainer:168 RmsProp 636 loss=0.206513 err=0.206513
I 2015-05-26 08:37:59 theanets.trainer:168 RmsProp 637 loss=0.203623 err=0.203623
I 2015-05-26 08:38:10 theanets.trainer:168 RmsProp 638 loss=0.195639 err=0.195639
I 2015-05-26 08:38:20 theanets.trainer:168 RmsProp 639 loss=0.190977 err=0.190977
I 2015-05-26 08:38:31 theanets.trainer:168 RmsProp 640 loss=0.199445 err=0.199445
I 2015-05-26 08:38:31 theanets.trainer:168 validation 64 loss=657.797363 err=657.797363 *
I 2015-05-26 08:38:42 theanets.trainer:168 RmsProp 641 loss=0.240743 err=0.240743
I 2015-05-26 08:38:52 theanets.trainer:168 RmsProp 642 loss=0.194123 err=0.194123
I 2015-05-26 08:39:02 theanets.trainer:168 RmsProp 643 loss=0.195369 err=0.195369
I 2015-05-26 08:39:13 theanets.trainer:168 RmsProp 644 loss=0.185237 err=0.185237
I 2015-05-26 08:39:23 theanets.trainer:168 RmsProp 645 loss=0.203378 err=0.203378
I 2015-05-26 08:39:34 theanets.trainer:168 RmsProp 646 loss=0.189867 err=0.189867
I 2015-05-26 08:39:44 theanets.trainer:168 RmsProp 647 loss=0.226667 err=0.226667
I 2015-05-26 08:39:55 theanets.trainer:168 RmsProp 648 loss=0.213618 err=0.213618
I 2015-05-26 08:40:06 theanets.trainer:168 RmsProp 649 loss=0.194150 err=0.194150
I 2015-05-26 08:40:16 theanets.trainer:168 RmsProp 650 loss=0.188066 err=0.188066
I 2015-05-26 08:40:17 theanets.trainer:168 validation 65 loss=658.543213 err=658.543213
I 2015-05-26 08:40:28 theanets.trainer:168 RmsProp 651 loss=0.198432 err=0.198432
I 2015-05-26 08:40:38 theanets.trainer:168 RmsProp 652 loss=0.196364 err=0.196364
I 2015-05-26 08:40:48 theanets.trainer:168 RmsProp 653 loss=0.199110 err=0.199110
I 2015-05-26 08:40:59 theanets.trainer:168 RmsProp 654 loss=0.202578 err=0.202578
I 2015-05-26 08:41:09 theanets.trainer:168 RmsProp 655 loss=0.192764 err=0.192764
I 2015-05-26 08:41:19 theanets.trainer:168 RmsProp 656 loss=0.191294 err=0.191294
I 2015-05-26 08:41:30 theanets.trainer:168 RmsProp 657 loss=0.204029 err=0.204029
I 2015-05-26 08:41:41 theanets.trainer:168 RmsProp 658 loss=0.199509 err=0.199509
I 2015-05-26 08:41:51 theanets.trainer:168 RmsProp 659 loss=0.192408 err=0.192408
I 2015-05-26 08:42:01 theanets.trainer:168 RmsProp 660 loss=0.190693 err=0.190693
I 2015-05-26 08:42:02 theanets.trainer:168 validation 66 loss=657.597839 err=657.597839 *
I 2015-05-26 08:42:12 theanets.trainer:168 RmsProp 661 loss=0.194386 err=0.194386
I 2015-05-26 08:42:22 theanets.trainer:168 RmsProp 662 loss=0.190266 err=0.190266
I 2015-05-26 08:42:33 theanets.trainer:168 RmsProp 663 loss=0.193920 err=0.193920
I 2015-05-26 08:42:43 theanets.trainer:168 RmsProp 664 loss=0.199935 err=0.199935
I 2015-05-26 08:42:53 theanets.trainer:168 RmsProp 665 loss=0.193440 err=0.193440
I 2015-05-26 08:43:03 theanets.trainer:168 RmsProp 666 loss=0.194763 err=0.194763
I 2015-05-26 08:43:14 theanets.trainer:168 RmsProp 667 loss=0.185871 err=0.185871
I 2015-05-26 08:43:24 theanets.trainer:168 RmsProp 668 loss=0.200054 err=0.200054
I 2015-05-26 08:43:35 theanets.trainer:168 RmsProp 669 loss=0.192571 err=0.192571
I 2015-05-26 08:43:45 theanets.trainer:168 RmsProp 670 loss=0.181264 err=0.181264
I 2015-05-26 08:43:46 theanets.trainer:168 validation 67 loss=657.669434 err=657.669434
I 2015-05-26 08:43:56 theanets.trainer:168 RmsProp 671 loss=0.192292 err=0.192292
I 2015-05-26 08:44:06 theanets.trainer:168 RmsProp 672 loss=0.207060 err=0.207060
I 2015-05-26 08:44:17 theanets.trainer:168 RmsProp 673 loss=0.181454 err=0.181454
I 2015-05-26 08:44:27 theanets.trainer:168 RmsProp 674 loss=0.211431 err=0.211431
I 2015-05-26 08:44:38 theanets.trainer:168 RmsProp 675 loss=0.190490 err=0.190490
I 2015-05-26 08:44:48 theanets.trainer:168 RmsProp 676 loss=0.179964 err=0.179964
I 2015-05-26 08:44:59 theanets.trainer:168 RmsProp 677 loss=0.196938 err=0.196938
I 2015-05-26 08:45:09 theanets.trainer:168 RmsProp 678 loss=0.190042 err=0.190042
I 2015-05-26 08:45:20 theanets.trainer:168 RmsProp 679 loss=0.189994 err=0.189994
I 2015-05-26 08:45:30 theanets.trainer:168 RmsProp 680 loss=0.206971 err=0.206971
I 2015-05-26 08:45:31 theanets.trainer:168 validation 68 loss=657.574158 err=657.574158 *
I 2015-05-26 08:45:41 theanets.trainer:168 RmsProp 681 loss=0.183965 err=0.183965
I 2015-05-26 08:45:52 theanets.trainer:168 RmsProp 682 loss=0.192676 err=0.192676
I 2015-05-26 08:46:02 theanets.trainer:168 RmsProp 683 loss=0.184767 err=0.184767
I 2015-05-26 08:46:12 theanets.trainer:168 RmsProp 684 loss=0.199294 err=0.199294
I 2015-05-26 08:46:22 theanets.trainer:168 RmsProp 685 loss=0.196337 err=0.196337
I 2015-05-26 08:46:32 theanets.trainer:168 RmsProp 686 loss=0.172622 err=0.172622
I 2015-05-26 08:46:42 theanets.trainer:168 RmsProp 687 loss=0.196369 err=0.196369
I 2015-05-26 08:46:52 theanets.trainer:168 RmsProp 688 loss=0.185081 err=0.185081
I 2015-05-26 08:47:02 theanets.trainer:168 RmsProp 689 loss=0.201575 err=0.201575
I 2015-05-26 08:47:12 theanets.trainer:168 RmsProp 690 loss=0.194250 err=0.194250
I 2015-05-26 08:47:12 theanets.trainer:168 validation 69 loss=657.267517 err=657.267517 *
I 2015-05-26 08:47:22 theanets.trainer:168 RmsProp 691 loss=0.167994 err=0.167994
I 2015-05-26 08:47:32 theanets.trainer:168 RmsProp 692 loss=0.195074 err=0.195074
I 2015-05-26 08:47:41 theanets.trainer:168 RmsProp 693 loss=0.174733 err=0.174733
I 2015-05-26 08:47:52 theanets.trainer:168 RmsProp 694 loss=0.195772 err=0.195772
I 2015-05-26 08:48:02 theanets.trainer:168 RmsProp 695 loss=0.177718 err=0.177718
I 2015-05-26 08:48:12 theanets.trainer:168 RmsProp 696 loss=0.193163 err=0.193163
I 2015-05-26 08:48:23 theanets.trainer:168 RmsProp 697 loss=0.184102 err=0.184102
I 2015-05-26 08:48:33 theanets.trainer:168 RmsProp 698 loss=0.192840 err=0.192840
I 2015-05-26 08:48:43 theanets.trainer:168 RmsProp 699 loss=0.175963 err=0.175963
I 2015-05-26 08:48:54 theanets.trainer:168 RmsProp 700 loss=0.181737 err=0.181737
I 2015-05-26 08:48:54 theanets.trainer:168 validation 70 loss=657.152954 err=657.152954 *
I 2015-05-26 08:49:04 theanets.trainer:168 RmsProp 701 loss=0.183479 err=0.183479
I 2015-05-26 08:49:13 theanets.trainer:168 RmsProp 702 loss=0.170958 err=0.170958
I 2015-05-26 08:49:23 theanets.trainer:168 RmsProp 703 loss=0.186083 err=0.186083
I 2015-05-26 08:49:33 theanets.trainer:168 RmsProp 704 loss=0.189817 err=0.189817
I 2015-05-26 08:49:43 theanets.trainer:168 RmsProp 705 loss=0.178599 err=0.178599
I 2015-05-26 08:49:53 theanets.trainer:168 RmsProp 706 loss=0.175421 err=0.175421
I 2015-05-26 08:50:03 theanets.trainer:168 RmsProp 707 loss=0.173252 err=0.173252
I 2015-05-26 08:50:13 theanets.trainer:168 RmsProp 708 loss=0.191606 err=0.191606
I 2015-05-26 08:50:23 theanets.trainer:168 RmsProp 709 loss=0.175092 err=0.175092
I 2015-05-26 08:50:33 theanets.trainer:168 RmsProp 710 loss=0.184361 err=0.184361
I 2015-05-26 08:50:33 theanets.trainer:168 validation 71 loss=657.551392 err=657.551392
I 2015-05-26 08:50:43 theanets.trainer:168 RmsProp 711 loss=0.190060 err=0.190060
I 2015-05-26 08:50:52 theanets.trainer:168 RmsProp 712 loss=0.189229 err=0.189229
I 2015-05-26 08:51:00 theanets.trainer:168 RmsProp 713 loss=0.173288 err=0.173288
I 2015-05-26 08:51:09 theanets.trainer:168 RmsProp 714 loss=0.175742 err=0.175742
I 2015-05-26 08:51:18 theanets.trainer:168 RmsProp 715 loss=0.180794 err=0.180794
I 2015-05-26 08:51:27 theanets.trainer:168 RmsProp 716 loss=0.180209 err=0.180209
I 2015-05-26 08:51:36 theanets.trainer:168 RmsProp 717 loss=0.173651 err=0.173651
I 2015-05-26 08:51:44 theanets.trainer:168 RmsProp 718 loss=0.182774 err=0.182774
I 2015-05-26 08:51:53 theanets.trainer:168 RmsProp 719 loss=0.180588 err=0.180588
I 2015-05-26 08:52:01 theanets.trainer:168 RmsProp 720 loss=0.166288 err=0.166288
I 2015-05-26 08:52:02 theanets.trainer:168 validation 72 loss=657.427185 err=657.427185
I 2015-05-26 08:52:10 theanets.trainer:168 RmsProp 721 loss=0.184806 err=0.184806
I 2015-05-26 08:52:19 theanets.trainer:168 RmsProp 722 loss=0.176170 err=0.176170
I 2015-05-26 08:52:27 theanets.trainer:168 RmsProp 723 loss=0.172897 err=0.172897
I 2015-05-26 08:52:36 theanets.trainer:168 RmsProp 724 loss=0.183262 err=0.183262
I 2015-05-26 08:52:45 theanets.trainer:168 RmsProp 725 loss=0.179493 err=0.179493
I 2015-05-26 08:52:53 theanets.trainer:168 RmsProp 726 loss=0.179042 err=0.179042
I 2015-05-26 08:53:02 theanets.trainer:168 RmsProp 727 loss=0.173040 err=0.173040
I 2015-05-26 08:53:10 theanets.trainer:168 RmsProp 728 loss=0.182386 err=0.182386
I 2015-05-26 08:53:19 theanets.trainer:168 RmsProp 729 loss=0.180992 err=0.180992
I 2015-05-26 08:53:27 theanets.trainer:168 RmsProp 730 loss=0.168319 err=0.168319
I 2015-05-26 08:53:28 theanets.trainer:168 validation 73 loss=656.698853 err=656.698853 *
I 2015-05-26 08:53:36 theanets.trainer:168 RmsProp 731 loss=0.170160 err=0.170160
I 2015-05-26 08:53:45 theanets.trainer:168 RmsProp 732 loss=0.187040 err=0.187040
I 2015-05-26 08:53:53 theanets.trainer:168 RmsProp 733 loss=0.178034 err=0.178034
I 2015-05-26 08:54:02 theanets.trainer:168 RmsProp 734 loss=0.166058 err=0.166058
I 2015-05-26 08:54:09 theanets.trainer:168 RmsProp 735 loss=0.171262 err=0.171262
I 2015-05-26 08:54:17 theanets.trainer:168 RmsProp 736 loss=0.176588 err=0.176588
I 2015-05-26 08:54:24 theanets.trainer:168 RmsProp 737 loss=0.172436 err=0.172436
I 2015-05-26 08:54:31 theanets.trainer:168 RmsProp 738 loss=0.192500 err=0.192500
I 2015-05-26 08:54:39 theanets.trainer:168 RmsProp 739 loss=0.179883 err=0.179883
I 2015-05-26 08:54:46 theanets.trainer:168 RmsProp 740 loss=0.172059 err=0.172059
I 2015-05-26 08:54:47 theanets.trainer:168 validation 74 loss=656.008789 err=656.008789 *
I 2015-05-26 08:54:54 theanets.trainer:168 RmsProp 741 loss=0.163067 err=0.163067
I 2015-05-26 08:55:01 theanets.trainer:168 RmsProp 742 loss=0.170564 err=0.170564
I 2015-05-26 08:55:09 theanets.trainer:168 RmsProp 743 loss=0.163341 err=0.163341
I 2015-05-26 08:55:17 theanets.trainer:168 RmsProp 744 loss=0.169764 err=0.169764
I 2015-05-26 08:55:25 theanets.trainer:168 RmsProp 745 loss=0.188770 err=0.188770
I 2015-05-26 08:55:32 theanets.trainer:168 RmsProp 746 loss=0.172563 err=0.172563
I 2015-05-26 08:55:40 theanets.trainer:168 RmsProp 747 loss=0.180183 err=0.180183
I 2015-05-26 08:55:47 theanets.trainer:168 RmsProp 748 loss=0.160108 err=0.160108
I 2015-05-26 08:55:54 theanets.trainer:168 RmsProp 749 loss=0.178169 err=0.178169
I 2015-05-26 08:56:01 theanets.trainer:168 RmsProp 750 loss=0.167283 err=0.167283
I 2015-05-26 08:56:01 theanets.trainer:168 validation 75 loss=656.846008 err=656.846008
I 2015-05-26 08:56:09 theanets.trainer:168 RmsProp 751 loss=0.167695 err=0.167695
I 2015-05-26 08:56:16 theanets.trainer:168 RmsProp 752 loss=0.174524 err=0.174524
I 2015-05-26 08:56:24 theanets.trainer:168 RmsProp 753 loss=0.162861 err=0.162861
I 2015-05-26 08:56:32 theanets.trainer:168 RmsProp 754 loss=0.182821 err=0.182821
I 2015-05-26 08:56:39 theanets.trainer:168 RmsProp 755 loss=0.199779 err=0.199779
I 2015-05-26 08:56:46 theanets.trainer:168 RmsProp 756 loss=0.182063 err=0.182063
I 2015-05-26 08:56:54 theanets.trainer:168 RmsProp 757 loss=0.164752 err=0.164752
I 2015-05-26 08:57:02 theanets.trainer:168 RmsProp 758 loss=0.151949 err=0.151949
I 2015-05-26 08:57:10 theanets.trainer:168 RmsProp 759 loss=0.179360 err=0.179360
I 2015-05-26 08:57:18 theanets.trainer:168 RmsProp 760 loss=0.180894 err=0.180894
I 2015-05-26 08:57:19 theanets.trainer:168 validation 76 loss=656.530701 err=656.530701
I 2015-05-26 08:57:26 theanets.trainer:168 RmsProp 761 loss=0.161960 err=0.161960
I 2015-05-26 08:57:34 theanets.trainer:168 RmsProp 762 loss=0.157432 err=0.157432
I 2015-05-26 08:57:42 theanets.trainer:168 RmsProp 763 loss=0.176644 err=0.176644
I 2015-05-26 08:57:50 theanets.trainer:168 RmsProp 764 loss=0.156306 err=0.156306
I 2015-05-26 08:57:59 theanets.trainer:168 RmsProp 765 loss=0.178985 err=0.178985
I 2015-05-26 08:58:06 theanets.trainer:168 RmsProp 766 loss=0.160827 err=0.160827
I 2015-05-26 08:58:13 theanets.trainer:168 RmsProp 767 loss=0.188399 err=0.188399
I 2015-05-26 08:58:21 theanets.trainer:168 RmsProp 768 loss=0.165169 err=0.165169
I 2015-05-26 08:58:29 theanets.trainer:168 RmsProp 769 loss=0.147913 err=0.147913
I 2015-05-26 08:58:36 theanets.trainer:168 RmsProp 770 loss=0.206635 err=0.206635
I 2015-05-26 08:58:37 theanets.trainer:168 validation 77 loss=656.295044 err=656.295044
I 2015-05-26 08:58:45 theanets.trainer:168 RmsProp 771 loss=0.168716 err=0.168716
I 2015-05-26 08:58:52 theanets.trainer:168 RmsProp 772 loss=0.162112 err=0.162112
I 2015-05-26 08:59:00 theanets.trainer:168 RmsProp 773 loss=0.159752 err=0.159752
I 2015-05-26 08:59:08 theanets.trainer:168 RmsProp 774 loss=0.172129 err=0.172129
I 2015-05-26 08:59:17 theanets.trainer:168 RmsProp 775 loss=0.175703 err=0.175703
I 2015-05-26 08:59:25 theanets.trainer:168 RmsProp 776 loss=0.174617 err=0.174617
I 2015-05-26 08:59:33 theanets.trainer:168 RmsProp 777 loss=0.158007 err=0.158007
I 2015-05-26 08:59:41 theanets.trainer:168 RmsProp 778 loss=0.155729 err=0.155729
I 2015-05-26 08:59:49 theanets.trainer:168 RmsProp 779 loss=0.173016 err=0.173016
I 2015-05-26 08:59:56 theanets.trainer:168 RmsProp 780 loss=0.168573 err=0.168573
I 2015-05-26 08:59:57 theanets.trainer:168 validation 78 loss=656.303650 err=656.303650
I 2015-05-26 09:00:04 theanets.trainer:168 RmsProp 781 loss=0.158967 err=0.158967
I 2015-05-26 09:00:12 theanets.trainer:168 RmsProp 782 loss=0.161588 err=0.161588
I 2015-05-26 09:00:19 theanets.trainer:168 RmsProp 783 loss=0.170306 err=0.170306
I 2015-05-26 09:00:27 theanets.trainer:168 RmsProp 784 loss=0.159325 err=0.159325
I 2015-05-26 09:00:35 theanets.trainer:168 RmsProp 785 loss=0.158722 err=0.158722
I 2015-05-26 09:00:43 theanets.trainer:168 RmsProp 786 loss=0.162569 err=0.162569
I 2015-05-26 09:00:50 theanets.trainer:168 RmsProp 787 loss=0.158161 err=0.158161
I 2015-05-26 09:00:57 theanets.trainer:168 RmsProp 788 loss=0.169614 err=0.169614
I 2015-05-26 09:01:05 theanets.trainer:168 RmsProp 789 loss=0.160987 err=0.160987
I 2015-05-26 09:01:13 theanets.trainer:168 RmsProp 790 loss=0.156022 err=0.156022
I 2015-05-26 09:01:14 theanets.trainer:168 validation 79 loss=655.524353 err=655.524353 *
I 2015-05-26 09:01:21 theanets.trainer:168 RmsProp 791 loss=0.165934 err=0.165934
I 2015-05-26 09:01:29 theanets.trainer:168 RmsProp 792 loss=0.155182 err=0.155182
I 2015-05-26 09:01:37 theanets.trainer:168 RmsProp 793 loss=0.171768 err=0.171768
I 2015-05-26 09:01:45 theanets.trainer:168 RmsProp 794 loss=0.181745 err=0.181745
I 2015-05-26 09:01:53 theanets.trainer:168 RmsProp 795 loss=0.159878 err=0.159878
I 2015-05-26 09:02:00 theanets.trainer:168 RmsProp 796 loss=0.174747 err=0.174747
I 2015-05-26 09:02:07 theanets.trainer:168 RmsProp 797 loss=0.151650 err=0.151650
I 2015-05-26 09:02:15 theanets.trainer:168 RmsProp 798 loss=0.173526 err=0.173526
I 2015-05-26 09:02:23 theanets.trainer:168 RmsProp 799 loss=0.149157 err=0.149157
I 2015-05-26 09:02:31 theanets.trainer:168 RmsProp 800 loss=0.153170 err=0.153170
I 2015-05-26 09:02:32 theanets.trainer:168 validation 80 loss=655.801208 err=655.801208
I 2015-05-26 09:02:39 theanets.trainer:168 RmsProp 801 loss=0.173867 err=0.173867
I 2015-05-26 09:02:47 theanets.trainer:168 RmsProp 802 loss=0.163256 err=0.163256
I 2015-05-26 09:02:54 theanets.trainer:168 RmsProp 803 loss=0.156979 err=0.156979
I 2015-05-26 09:03:02 theanets.trainer:168 RmsProp 804 loss=0.144010 err=0.144010
I 2015-05-26 09:03:10 theanets.trainer:168 RmsProp 805 loss=0.151475 err=0.151475
I 2015-05-26 09:03:18 theanets.trainer:168 RmsProp 806 loss=0.181107 err=0.181107
I 2015-05-26 09:03:26 theanets.trainer:168 RmsProp 807 loss=0.160967 err=0.160967
I 2015-05-26 09:03:35 theanets.trainer:168 RmsProp 808 loss=0.161030 err=0.161030
I 2015-05-26 09:03:43 theanets.trainer:168 RmsProp 809 loss=0.163809 err=0.163809
I 2015-05-26 09:03:50 theanets.trainer:168 RmsProp 810 loss=0.158921 err=0.158921
I 2015-05-26 09:03:51 theanets.trainer:168 validation 81 loss=655.570435 err=655.570435
I 2015-05-26 09:03:57 theanets.trainer:168 RmsProp 811 loss=0.158073 err=0.158073
I 2015-05-26 09:04:05 theanets.trainer:168 RmsProp 812 loss=0.168168 err=0.168168
I 2015-05-26 09:04:13 theanets.trainer:168 RmsProp 813 loss=0.154001 err=0.154001
I 2015-05-26 09:04:20 theanets.trainer:168 RmsProp 814 loss=0.161254 err=0.161254
I 2015-05-26 09:04:28 theanets.trainer:168 RmsProp 815 loss=0.152367 err=0.152367
I 2015-05-26 09:04:37 theanets.trainer:168 RmsProp 816 loss=0.153630 err=0.153630
I 2015-05-26 09:04:45 theanets.trainer:168 RmsProp 817 loss=0.153753 err=0.153753
I 2015-05-26 09:04:53 theanets.trainer:168 RmsProp 818 loss=0.155852 err=0.155852
I 2015-05-26 09:05:01 theanets.trainer:168 RmsProp 819 loss=0.160545 err=0.160545
I 2015-05-26 09:05:08 theanets.trainer:168 RmsProp 820 loss=0.164896 err=0.164896
I 2015-05-26 09:05:09 theanets.trainer:168 validation 82 loss=655.301575 err=655.301575 *
I 2015-05-26 09:05:16 theanets.trainer:168 RmsProp 821 loss=0.144388 err=0.144388
I 2015-05-26 09:05:24 theanets.trainer:168 RmsProp 822 loss=0.156135 err=0.156135
I 2015-05-26 09:05:31 theanets.trainer:168 RmsProp 823 loss=0.150955 err=0.150955
I 2015-05-26 09:05:39 theanets.trainer:168 RmsProp 824 loss=0.175543 err=0.175543
I 2015-05-26 09:05:46 theanets.trainer:168 RmsProp 825 loss=0.151016 err=0.151016
I 2015-05-26 09:05:53 theanets.trainer:168 RmsProp 826 loss=0.143229 err=0.143229
I 2015-05-26 09:06:01 theanets.trainer:168 RmsProp 827 loss=0.153720 err=0.153720
I 2015-05-26 09:06:08 theanets.trainer:168 RmsProp 828 loss=0.163752 err=0.163752
I 2015-05-26 09:06:15 theanets.trainer:168 RmsProp 829 loss=0.155634 err=0.155634
I 2015-05-26 09:06:23 theanets.trainer:168 RmsProp 830 loss=0.139300 err=0.139300
I 2015-05-26 09:06:23 theanets.trainer:168 validation 83 loss=655.657898 err=655.657898
I 2015-05-26 09:06:30 theanets.trainer:168 RmsProp 831 loss=0.181049 err=0.181049
I 2015-05-26 09:06:37 theanets.trainer:168 RmsProp 832 loss=0.170698 err=0.170698
I 2015-05-26 09:06:45 theanets.trainer:168 RmsProp 833 loss=0.147313 err=0.147313
I 2015-05-26 09:06:52 theanets.trainer:168 RmsProp 834 loss=0.160241 err=0.160241
I 2015-05-26 09:07:00 theanets.trainer:168 RmsProp 835 loss=0.208059 err=0.208059
I 2015-05-26 09:07:07 theanets.trainer:168 RmsProp 836 loss=0.160541 err=0.160541
I 2015-05-26 09:07:14 theanets.trainer:168 RmsProp 837 loss=0.163204 err=0.163204
I 2015-05-26 09:07:21 theanets.trainer:168 RmsProp 838 loss=0.155058 err=0.155058
I 2015-05-26 09:07:28 theanets.trainer:168 RmsProp 839 loss=0.141631 err=0.141631
I 2015-05-26 09:07:36 theanets.trainer:168 RmsProp 840 loss=0.152704 err=0.152704
I 2015-05-26 09:07:36 theanets.trainer:168 validation 84 loss=655.980652 err=655.980652
I 2015-05-26 09:07:44 theanets.trainer:168 RmsProp 841 loss=0.156022 err=0.156022
I 2015-05-26 09:07:52 theanets.trainer:168 RmsProp 842 loss=0.156292 err=0.156292
I 2015-05-26 09:07:59 theanets.trainer:168 RmsProp 843 loss=0.149087 err=0.149087
I 2015-05-26 09:08:07 theanets.trainer:168 RmsProp 844 loss=0.144602 err=0.144602
I 2015-05-26 09:08:15 theanets.trainer:168 RmsProp 845 loss=0.162927 err=0.162927
I 2015-05-26 09:08:23 theanets.trainer:168 RmsProp 846 loss=0.168359 err=0.168359
I 2015-05-26 09:08:30 theanets.trainer:168 RmsProp 847 loss=0.151426 err=0.151426
I 2015-05-26 09:08:38 theanets.trainer:168 RmsProp 848 loss=0.140649 err=0.140649
I 2015-05-26 09:08:46 theanets.trainer:168 RmsProp 849 loss=0.160036 err=0.160036
I 2015-05-26 09:08:55 theanets.trainer:168 RmsProp 850 loss=0.150238 err=0.150238
I 2015-05-26 09:08:55 theanets.trainer:168 validation 85 loss=655.247192 err=655.247192 *
I 2015-05-26 09:09:02 theanets.trainer:168 RmsProp 851 loss=0.150768 err=0.150768
I 2015-05-26 09:09:10 theanets.trainer:168 RmsProp 852 loss=0.150775 err=0.150775
I 2015-05-26 09:09:17 theanets.trainer:168 RmsProp 853 loss=0.154980 err=0.154980
I 2015-05-26 09:09:24 theanets.trainer:168 RmsProp 854 loss=0.151540 err=0.151540
I 2015-05-26 09:09:31 theanets.trainer:168 RmsProp 855 loss=0.160317 err=0.160317
I 2015-05-26 09:09:39 theanets.trainer:168 RmsProp 856 loss=0.140698 err=0.140698
I 2015-05-26 09:09:46 theanets.trainer:168 RmsProp 857 loss=0.152794 err=0.152794
I 2015-05-26 09:09:54 theanets.trainer:168 RmsProp 858 loss=0.142781 err=0.142781
I 2015-05-26 09:10:01 theanets.trainer:168 RmsProp 859 loss=0.145681 err=0.145681
I 2015-05-26 09:10:08 theanets.trainer:168 RmsProp 860 loss=0.154377 err=0.154377
I 2015-05-26 09:10:08 theanets.trainer:168 validation 86 loss=654.193420 err=654.193420 *
I 2015-05-26 09:10:15 theanets.trainer:168 RmsProp 861 loss=0.151685 err=0.151685
I 2015-05-26 09:10:23 theanets.trainer:168 RmsProp 862 loss=0.147018 err=0.147018
I 2015-05-26 09:10:31 theanets.trainer:168 RmsProp 863 loss=0.160304 err=0.160304
I 2015-05-26 09:10:39 theanets.trainer:168 RmsProp 864 loss=0.152068 err=0.152068
I 2015-05-26 09:10:47 theanets.trainer:168 RmsProp 865 loss=0.152689 err=0.152689
I 2015-05-26 09:10:56 theanets.trainer:168 RmsProp 866 loss=0.141498 err=0.141498
I 2015-05-26 09:11:03 theanets.trainer:168 RmsProp 867 loss=0.144889 err=0.144889
I 2015-05-26 09:11:10 theanets.trainer:168 RmsProp 868 loss=0.154182 err=0.154182
I 2015-05-26 09:11:17 theanets.trainer:168 RmsProp 869 loss=0.168092 err=0.168092
I 2015-05-26 09:11:24 theanets.trainer:168 RmsProp 870 loss=0.130201 err=0.130201
I 2015-05-26 09:11:24 theanets.trainer:168 validation 87 loss=654.014465 err=654.014465 *
I 2015-05-26 09:11:30 theanets.trainer:168 RmsProp 871 loss=0.159597 err=0.159597
I 2015-05-26 09:11:36 theanets.trainer:168 RmsProp 872 loss=0.173453 err=0.173453
I 2015-05-26 09:11:42 theanets.trainer:168 RmsProp 873 loss=0.153267 err=0.153267
I 2015-05-26 09:11:48 theanets.trainer:168 RmsProp 874 loss=0.146943 err=0.146943
I 2015-05-26 09:11:54 theanets.trainer:168 RmsProp 875 loss=0.149609 err=0.149609
I 2015-05-26 09:12:00 theanets.trainer:168 RmsProp 876 loss=0.147549 err=0.147549
I 2015-05-26 09:12:07 theanets.trainer:168 RmsProp 877 loss=0.150174 err=0.150174
I 2015-05-26 09:12:13 theanets.trainer:168 RmsProp 878 loss=0.141300 err=0.141300
I 2015-05-26 09:12:20 theanets.trainer:168 RmsProp 879 loss=0.148831 err=0.148831
I 2015-05-26 09:12:26 theanets.trainer:168 RmsProp 880 loss=0.174857 err=0.174857
I 2015-05-26 09:12:26 theanets.trainer:168 validation 88 loss=653.959900 err=653.959900 *
I 2015-05-26 09:12:32 theanets.trainer:168 RmsProp 881 loss=0.146239 err=0.146239
I 2015-05-26 09:12:38 theanets.trainer:168 RmsProp 882 loss=0.144966 err=0.144966
I 2015-05-26 09:12:44 theanets.trainer:168 RmsProp 883 loss=0.139455 err=0.139455
I 2015-05-26 09:12:51 theanets.trainer:168 RmsProp 884 loss=0.141012 err=0.141012
I 2015-05-26 09:12:57 theanets.trainer:168 RmsProp 885 loss=0.150138 err=0.150138
I 2015-05-26 09:13:04 theanets.trainer:168 RmsProp 886 loss=0.145989 err=0.145989
I 2015-05-26 09:13:10 theanets.trainer:168 RmsProp 887 loss=0.141946 err=0.141946
I 2015-05-26 09:13:17 theanets.trainer:168 RmsProp 888 loss=0.139976 err=0.139976
I 2015-05-26 09:13:24 theanets.trainer:168 RmsProp 889 loss=0.146579 err=0.146579
I 2015-05-26 09:13:30 theanets.trainer:168 RmsProp 890 loss=0.148425 err=0.148425
I 2015-05-26 09:13:31 theanets.trainer:168 validation 89 loss=653.501892 err=653.501892 *
I 2015-05-26 09:13:36 theanets.trainer:168 RmsProp 891 loss=0.144881 err=0.144881
I 2015-05-26 09:13:43 theanets.trainer:168 RmsProp 892 loss=0.144533 err=0.144533
I 2015-05-26 09:13:49 theanets.trainer:168 RmsProp 893 loss=0.140770 err=0.140770
I 2015-05-26 09:13:54 theanets.trainer:168 RmsProp 894 loss=0.161897 err=0.161897
I 2015-05-26 09:14:00 theanets.trainer:168 RmsProp 895 loss=0.140447 err=0.140447
I 2015-05-26 09:14:06 theanets.trainer:168 RmsProp 896 loss=0.141424 err=0.141424
I 2015-05-26 09:14:12 theanets.trainer:168 RmsProp 897 loss=0.139229 err=0.139229
I 2015-05-26 09:14:17 theanets.trainer:168 RmsProp 898 loss=0.138082 err=0.138082
I 2015-05-26 09:14:23 theanets.trainer:168 RmsProp 899 loss=0.134811 err=0.134811
I 2015-05-26 09:14:28 theanets.trainer:168 RmsProp 900 loss=0.161676 err=0.161676
I 2015-05-26 09:14:29 theanets.trainer:168 validation 90 loss=654.052917 err=654.052917
I 2015-05-26 09:14:34 theanets.trainer:168 RmsProp 901 loss=0.169019 err=0.169019
I 2015-05-26 09:14:39 theanets.trainer:168 RmsProp 902 loss=0.142829 err=0.142829
I 2015-05-26 09:14:45 theanets.trainer:168 RmsProp 903 loss=0.142386 err=0.142386
I 2015-05-26 09:14:50 theanets.trainer:168 RmsProp 904 loss=0.133411 err=0.133411
I 2015-05-26 09:14:55 theanets.trainer:168 RmsProp 905 loss=0.149454 err=0.149454
I 2015-05-26 09:15:01 theanets.trainer:168 RmsProp 906 loss=0.144374 err=0.144374
I 2015-05-26 09:15:06 theanets.trainer:168 RmsProp 907 loss=0.132110 err=0.132110
I 2015-05-26 09:15:11 theanets.trainer:168 RmsProp 908 loss=0.151068 err=0.151068
I 2015-05-26 09:15:16 theanets.trainer:168 RmsProp 909 loss=0.129181 err=0.129181
I 2015-05-26 09:15:22 theanets.trainer:168 RmsProp 910 loss=0.156567 err=0.156567
I 2015-05-26 09:15:22 theanets.trainer:168 validation 91 loss=653.395264 err=653.395264 *
I 2015-05-26 09:15:27 theanets.trainer:168 RmsProp 911 loss=0.142259 err=0.142259
I 2015-05-26 09:15:34 theanets.trainer:168 RmsProp 912 loss=0.136781 err=0.136781
I 2015-05-26 09:15:40 theanets.trainer:168 RmsProp 913 loss=0.149497 err=0.149497
I 2015-05-26 09:15:47 theanets.trainer:168 RmsProp 914 loss=0.143977 err=0.143977
I 2015-05-26 09:15:54 theanets.trainer:168 RmsProp 915 loss=0.136980 err=0.136980
I 2015-05-26 09:16:01 theanets.trainer:168 RmsProp 916 loss=0.140653 err=0.140653
I 2015-05-26 09:16:07 theanets.trainer:168 RmsProp 917 loss=0.134686 err=0.134686
I 2015-05-26 09:16:13 theanets.trainer:168 RmsProp 918 loss=0.131983 err=0.131983
I 2015-05-26 09:16:19 theanets.trainer:168 RmsProp 919 loss=0.156911 err=0.156911
I 2015-05-26 09:16:26 theanets.trainer:168 RmsProp 920 loss=0.130673 err=0.130673
I 2015-05-26 09:16:26 theanets.trainer:168 validation 92 loss=653.230896 err=653.230896 *
I 2015-05-26 09:16:32 theanets.trainer:168 RmsProp 921 loss=0.161955 err=0.161955
I 2015-05-26 09:16:39 theanets.trainer:168 RmsProp 922 loss=0.161931 err=0.161931
I 2015-05-26 09:16:45 theanets.trainer:168 RmsProp 923 loss=0.134701 err=0.134701
I 2015-05-26 09:16:52 theanets.trainer:168 RmsProp 924 loss=0.127520 err=0.127520
I 2015-05-26 09:16:58 theanets.trainer:168 RmsProp 925 loss=0.144998 err=0.144998
I 2015-05-26 09:17:05 theanets.trainer:168 RmsProp 926 loss=0.153695 err=0.153695
I 2015-05-26 09:17:11 theanets.trainer:168 RmsProp 927 loss=0.131087 err=0.131087
I 2015-05-26 09:17:17 theanets.trainer:168 RmsProp 928 loss=0.137379 err=0.137379
I 2015-05-26 09:17:23 theanets.trainer:168 RmsProp 929 loss=0.133738 err=0.133738
I 2015-05-26 09:17:29 theanets.trainer:168 RmsProp 930 loss=0.142312 err=0.142312
I 2015-05-26 09:17:29 theanets.trainer:168 validation 93 loss=653.619080 err=653.619080
I 2015-05-26 09:17:35 theanets.trainer:168 RmsProp 931 loss=0.134631 err=0.134631
I 2015-05-26 09:17:41 theanets.trainer:168 RmsProp 932 loss=0.128670 err=0.128670
I 2015-05-26 09:17:47 theanets.trainer:168 RmsProp 933 loss=0.148810 err=0.148810
I 2015-05-26 09:17:53 theanets.trainer:168 RmsProp 934 loss=0.139090 err=0.139090
I 2015-05-26 09:18:00 theanets.trainer:168 RmsProp 935 loss=0.131992 err=0.131992
I 2015-05-26 09:18:07 theanets.trainer:168 RmsProp 936 loss=0.135781 err=0.135781
I 2015-05-26 09:18:13 theanets.trainer:168 RmsProp 937 loss=0.136622 err=0.136622
I 2015-05-26 09:18:19 theanets.trainer:168 RmsProp 938 loss=0.136248 err=0.136248
I 2015-05-26 09:18:26 theanets.trainer:168 RmsProp 939 loss=0.137569 err=0.137569
I 2015-05-26 09:18:32 theanets.trainer:168 RmsProp 940 loss=0.124888 err=0.124888
I 2015-05-26 09:18:33 theanets.trainer:168 validation 94 loss=653.333191 err=653.333191
I 2015-05-26 09:18:39 theanets.trainer:168 RmsProp 941 loss=0.160295 err=0.160295
I 2015-05-26 09:18:46 theanets.trainer:168 RmsProp 942 loss=0.138373 err=0.138373
I 2015-05-26 09:18:53 theanets.trainer:168 RmsProp 943 loss=0.136420 err=0.136420
I 2015-05-26 09:18:59 theanets.trainer:168 RmsProp 944 loss=0.148838 err=0.148838
I 2015-05-26 09:19:05 theanets.trainer:168 RmsProp 945 loss=0.127118 err=0.127118
I 2015-05-26 09:19:12 theanets.trainer:168 RmsProp 946 loss=0.126142 err=0.126142
I 2015-05-26 09:19:18 theanets.trainer:168 RmsProp 947 loss=0.137896 err=0.137896
I 2015-05-26 09:19:24 theanets.trainer:168 RmsProp 948 loss=0.139289 err=0.139289
I 2015-05-26 09:19:30 theanets.trainer:168 RmsProp 949 loss=0.121155 err=0.121155
I 2015-05-26 09:19:37 theanets.trainer:168 RmsProp 950 loss=0.134817 err=0.134817
I 2015-05-26 09:19:37 theanets.trainer:168 validation 95 loss=653.840515 err=653.840515
I 2015-05-26 09:19:44 theanets.trainer:168 RmsProp 951 loss=0.124223 err=0.124223
I 2015-05-26 09:19:50 theanets.trainer:168 RmsProp 952 loss=0.163027 err=0.163027
I 2015-05-26 09:19:56 theanets.trainer:168 RmsProp 953 loss=0.126952 err=0.126952
I 2015-05-26 09:20:02 theanets.trainer:168 RmsProp 954 loss=0.148264 err=0.148264
I 2015-05-26 09:20:08 theanets.trainer:168 RmsProp 955 loss=0.141414 err=0.141414
I 2015-05-26 09:20:14 theanets.trainer:168 RmsProp 956 loss=0.133646 err=0.133646
I 2015-05-26 09:20:21 theanets.trainer:168 RmsProp 957 loss=0.128510 err=0.128510
I 2015-05-26 09:20:27 theanets.trainer:168 RmsProp 958 loss=0.140620 err=0.140620
I 2015-05-26 09:20:33 theanets.trainer:168 RmsProp 959 loss=0.119759 err=0.119759
I 2015-05-26 09:20:40 theanets.trainer:168 RmsProp 960 loss=0.142087 err=0.142087
I 2015-05-26 09:20:40 theanets.trainer:168 validation 96 loss=654.433899 err=654.433899
I 2015-05-26 09:20:46 theanets.trainer:168 RmsProp 961 loss=0.140911 err=0.140911
I 2015-05-26 09:20:52 theanets.trainer:168 RmsProp 962 loss=0.134458 err=0.134458
I 2015-05-26 09:20:58 theanets.trainer:168 RmsProp 963 loss=0.122475 err=0.122475
I 2015-05-26 09:21:05 theanets.trainer:168 RmsProp 964 loss=0.150634 err=0.150634
I 2015-05-26 09:21:12 theanets.trainer:168 RmsProp 965 loss=0.134943 err=0.134943
I 2015-05-26 09:21:19 theanets.trainer:168 RmsProp 966 loss=0.136463 err=0.136463
I 2015-05-26 09:21:25 theanets.trainer:168 RmsProp 967 loss=0.130842 err=0.130842
I 2015-05-26 09:21:33 theanets.trainer:168 RmsProp 968 loss=0.115576 err=0.115576
I 2015-05-26 09:21:39 theanets.trainer:168 RmsProp 969 loss=0.170673 err=0.170673
I 2015-05-26 09:21:45 theanets.trainer:168 RmsProp 970 loss=0.131890 err=0.131890
I 2015-05-26 09:21:45 theanets.trainer:168 validation 97 loss=653.834412 err=653.834412
I 2015-05-26 09:21:45 theanets.trainer:252 patience elapsed!
I 2015-05-26 09:21:45 theanets.main:237 models_deep_post_code_sep/95127-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl: saving model
I 2015-05-26 09:21:45 theanets.graph:477 models_deep_post_code_sep/95127-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl: saved model parameters

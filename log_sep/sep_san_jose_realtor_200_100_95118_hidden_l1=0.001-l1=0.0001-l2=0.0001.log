I 2015-05-26 03:35:25 theanets.layers:465 layer in: out:0 -> 1000, linear, 0 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm1_fw: in.out:1000 -> 100, logistic, 440700 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm1_bw: in.out:1000 -> 100, logistic, 440700 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm1: in.out:1000 -> 200, logistic, 881400 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm2_fw: bdlstm1.out:200 -> 50, logistic, 50350 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm2_bw: bdlstm1.out:200 -> 50, logistic, 50350 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm2: bdlstm1.out:200 -> 100, logistic, 100700 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer out: bdlstm2.out:100 -> 1, linear, 101 parameters
I 2015-05-26 03:35:25 theanets.graph:145 network has 982201 total parameters
models_deep_post_code_sep/95118-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl
I 2015-05-26 03:35:25 theanets.dataset:133 valid: 3 mini-batches from callable
I 2015-05-26 03:35:25 theanets.dataset:133 train: 30 mini-batches from callable
I 2015-05-26 03:35:25 theanets.main:87 creating trainer <class 'theanets.trainer.RmsProp'>
I 2015-05-26 03:35:25 theanets.main:89 --batch_size = 1024
I 2015-05-26 03:35:25 theanets.main:89 --gradient_clip = 1
I 2015-05-26 03:35:25 theanets.main:89 --hidden_l1 = None
I 2015-05-26 03:35:25 theanets.main:89 --learning_rate = 0.001
I 2015-05-26 03:35:25 theanets.main:89 --train_batches = 30
I 2015-05-26 03:35:25 theanets.main:89 --valid_batches = 3
I 2015-05-26 03:35:25 theanets.main:89 --weight_l1 = None
I 2015-05-26 03:35:25 theanets.main:89 --weight_l2 = None
I 2015-05-26 03:35:25 theanets.trainer:134 compiling evaluation function
I 2015-05-26 03:35:41 theanets.trainer:294 compiling RmsProp learning function
I 2015-05-26 03:38:57 theanets.trainer:168 validation 0 loss=14150.768555 err=14150.768555 *
I 2015-05-26 03:39:56 theanets.trainer:168 RmsProp 1 loss=13245.594727 err=13245.594727
I 2015-05-26 03:40:57 theanets.trainer:168 RmsProp 2 loss=13167.464844 err=13167.464844
I 2015-05-26 03:41:58 theanets.trainer:168 RmsProp 3 loss=12469.220703 err=12469.220703
I 2015-05-26 03:42:58 theanets.trainer:168 RmsProp 4 loss=10890.017578 err=10890.017578
I 2015-05-26 03:43:57 theanets.trainer:168 RmsProp 5 loss=9749.437500 err=9749.437500
I 2015-05-26 03:44:56 theanets.trainer:168 RmsProp 6 loss=9451.004883 err=9451.004883
I 2015-05-26 03:45:56 theanets.trainer:168 RmsProp 7 loss=9353.889648 err=9353.889648
I 2015-05-26 03:46:56 theanets.trainer:168 RmsProp 8 loss=8690.372070 err=8690.372070
I 2015-05-26 03:47:57 theanets.trainer:168 RmsProp 9 loss=8031.429199 err=8031.429199
I 2015-05-26 03:48:57 theanets.trainer:168 RmsProp 10 loss=7356.801270 err=7356.801270
I 2015-05-26 03:48:58 theanets.trainer:168 validation 1 loss=6494.608887 err=6494.608887 *
I 2015-05-26 03:49:59 theanets.trainer:168 RmsProp 11 loss=6850.295410 err=6850.295410
I 2015-05-26 03:50:59 theanets.trainer:168 RmsProp 12 loss=6404.109375 err=6404.109375
I 2015-05-26 03:52:00 theanets.trainer:168 RmsProp 13 loss=5849.540039 err=5849.540039
I 2015-05-26 03:53:02 theanets.trainer:168 RmsProp 14 loss=5362.114258 err=5362.114258
I 2015-05-26 03:54:03 theanets.trainer:168 RmsProp 15 loss=4874.719727 err=4874.719727
I 2015-05-26 03:55:04 theanets.trainer:168 RmsProp 16 loss=4407.060059 err=4407.060059
I 2015-05-26 03:56:06 theanets.trainer:168 RmsProp 17 loss=4092.079834 err=4092.079834
I 2015-05-26 03:57:07 theanets.trainer:168 RmsProp 18 loss=3804.700928 err=3804.700928
I 2015-05-26 03:58:08 theanets.trainer:168 RmsProp 19 loss=3546.942871 err=3546.942871
I 2015-05-26 03:59:09 theanets.trainer:168 RmsProp 20 loss=3334.289062 err=3334.289062
I 2015-05-26 03:59:10 theanets.trainer:168 validation 2 loss=3431.915771 err=3431.915771 *
I 2015-05-26 04:00:11 theanets.trainer:168 RmsProp 21 loss=3197.277100 err=3197.277100
I 2015-05-26 04:01:12 theanets.trainer:168 RmsProp 22 loss=3014.109863 err=3014.109863
I 2015-05-26 04:02:13 theanets.trainer:168 RmsProp 23 loss=2871.535400 err=2871.535400
I 2015-05-26 04:03:14 theanets.trainer:168 RmsProp 24 loss=2748.895996 err=2748.895996
I 2015-05-26 04:04:15 theanets.trainer:168 RmsProp 25 loss=2581.107910 err=2581.107910
I 2015-05-26 04:05:16 theanets.trainer:168 RmsProp 26 loss=2452.369385 err=2452.369385
I 2015-05-26 04:06:17 theanets.trainer:168 RmsProp 27 loss=2279.142822 err=2279.142822
I 2015-05-26 04:07:17 theanets.trainer:168 RmsProp 28 loss=2180.899902 err=2180.899902
I 2015-05-26 04:08:18 theanets.trainer:168 RmsProp 29 loss=2119.392090 err=2119.392090
I 2015-05-26 04:09:19 theanets.trainer:168 RmsProp 30 loss=1967.611694 err=1967.611694
I 2015-05-26 04:09:20 theanets.trainer:168 validation 3 loss=2685.572998 err=2685.572998 *
I 2015-05-26 04:10:21 theanets.trainer:168 RmsProp 31 loss=1878.361206 err=1878.361206
I 2015-05-26 04:11:21 theanets.trainer:168 RmsProp 32 loss=1781.214600 err=1781.214600
I 2015-05-26 04:12:21 theanets.trainer:168 RmsProp 33 loss=1666.022705 err=1666.022705
I 2015-05-26 04:13:20 theanets.trainer:168 RmsProp 34 loss=1577.433350 err=1577.433350
I 2015-05-26 04:14:18 theanets.trainer:168 RmsProp 35 loss=1490.472900 err=1490.472900
I 2015-05-26 04:15:14 theanets.trainer:168 RmsProp 36 loss=1422.875732 err=1422.875732
I 2015-05-26 04:16:10 theanets.trainer:168 RmsProp 37 loss=1343.850830 err=1343.850830
I 2015-05-26 04:17:07 theanets.trainer:168 RmsProp 38 loss=1297.329468 err=1297.329468
I 2015-05-26 04:18:03 theanets.trainer:168 RmsProp 39 loss=1254.078979 err=1254.078979
I 2015-05-26 04:18:59 theanets.trainer:168 RmsProp 40 loss=1210.366577 err=1210.366577
I 2015-05-26 04:19:00 theanets.trainer:168 validation 4 loss=2168.265381 err=2168.265381 *
I 2015-05-26 04:19:56 theanets.trainer:168 RmsProp 41 loss=1139.701172 err=1139.701172
I 2015-05-26 04:20:53 theanets.trainer:168 RmsProp 42 loss=1163.680542 err=1163.680542
I 2015-05-26 04:21:51 theanets.trainer:168 RmsProp 43 loss=1115.097046 err=1115.097046
I 2015-05-26 04:22:45 theanets.trainer:168 RmsProp 44 loss=1042.397095 err=1042.397095
I 2015-05-26 04:23:39 theanets.trainer:168 RmsProp 45 loss=1005.761108 err=1005.761108
I 2015-05-26 04:24:32 theanets.trainer:168 RmsProp 46 loss=982.092896 err=982.092896
I 2015-05-26 04:25:25 theanets.trainer:168 RmsProp 47 loss=938.105042 err=938.105042
I 2015-05-26 04:26:18 theanets.trainer:168 RmsProp 48 loss=897.992126 err=897.992126
I 2015-05-26 04:27:12 theanets.trainer:168 RmsProp 49 loss=853.827209 err=853.827209
I 2015-05-26 04:28:05 theanets.trainer:168 RmsProp 50 loss=841.593750 err=841.593750
I 2015-05-26 04:28:06 theanets.trainer:168 validation 5 loss=1886.478149 err=1886.478149 *
I 2015-05-26 04:28:58 theanets.trainer:168 RmsProp 51 loss=812.002075 err=812.002075
I 2015-05-26 04:29:51 theanets.trainer:168 RmsProp 52 loss=769.656433 err=769.656433
I 2015-05-26 04:30:44 theanets.trainer:168 RmsProp 53 loss=743.039124 err=743.039124
I 2015-05-26 04:31:38 theanets.trainer:168 RmsProp 54 loss=714.426208 err=714.426208
I 2015-05-26 04:32:30 theanets.trainer:168 RmsProp 55 loss=699.629333 err=699.629333
I 2015-05-26 04:33:23 theanets.trainer:168 RmsProp 56 loss=673.380127 err=673.380127
I 2015-05-26 04:34:16 theanets.trainer:168 RmsProp 57 loss=652.753967 err=652.753967
I 2015-05-26 04:35:10 theanets.trainer:168 RmsProp 58 loss=632.725159 err=632.725159
I 2015-05-26 04:36:04 theanets.trainer:168 RmsProp 59 loss=616.975830 err=616.975830
I 2015-05-26 04:36:58 theanets.trainer:168 RmsProp 60 loss=598.046448 err=598.046448
I 2015-05-26 04:36:59 theanets.trainer:168 validation 6 loss=1744.338867 err=1744.338867 *
I 2015-05-26 04:37:53 theanets.trainer:168 RmsProp 61 loss=580.589172 err=580.589172
I 2015-05-26 04:38:47 theanets.trainer:168 RmsProp 62 loss=557.745911 err=557.745911
I 2015-05-26 04:39:41 theanets.trainer:168 RmsProp 63 loss=542.743713 err=542.743713
I 2015-05-26 04:40:36 theanets.trainer:168 RmsProp 64 loss=532.539124 err=532.539124
I 2015-05-26 04:41:30 theanets.trainer:168 RmsProp 65 loss=517.012390 err=517.012390
I 2015-05-26 04:42:24 theanets.trainer:168 RmsProp 66 loss=511.577606 err=511.577606
I 2015-05-26 04:43:19 theanets.trainer:168 RmsProp 67 loss=488.937317 err=488.937317
I 2015-05-26 04:44:13 theanets.trainer:168 RmsProp 68 loss=483.471985 err=483.471985
I 2015-05-26 04:45:07 theanets.trainer:168 RmsProp 69 loss=464.867523 err=464.867523
I 2015-05-26 04:46:01 theanets.trainer:168 RmsProp 70 loss=460.923553 err=460.923553
I 2015-05-26 04:46:02 theanets.trainer:168 validation 7 loss=1648.305664 err=1648.305664 *
I 2015-05-26 04:46:56 theanets.trainer:168 RmsProp 71 loss=445.996521 err=445.996521
I 2015-05-26 04:47:49 theanets.trainer:168 RmsProp 72 loss=422.783020 err=422.783020
I 2015-05-26 04:48:43 theanets.trainer:168 RmsProp 73 loss=415.973938 err=415.973938
I 2015-05-26 04:49:37 theanets.trainer:168 RmsProp 74 loss=418.064056 err=418.064056
I 2015-05-26 04:50:32 theanets.trainer:168 RmsProp 75 loss=407.345612 err=407.345612
I 2015-05-26 04:51:25 theanets.trainer:168 RmsProp 76 loss=394.424194 err=394.424194
I 2015-05-26 04:52:20 theanets.trainer:168 RmsProp 77 loss=382.365631 err=382.365631
I 2015-05-26 04:53:15 theanets.trainer:168 RmsProp 78 loss=374.530609 err=374.530609
I 2015-05-26 04:54:08 theanets.trainer:168 RmsProp 79 loss=361.249298 err=361.249298
I 2015-05-26 04:55:02 theanets.trainer:168 RmsProp 80 loss=355.119751 err=355.119751
I 2015-05-26 04:55:03 theanets.trainer:168 validation 8 loss=1564.776733 err=1564.776733 *
I 2015-05-26 04:55:55 theanets.trainer:168 RmsProp 81 loss=338.772125 err=338.772125
I 2015-05-26 04:56:48 theanets.trainer:168 RmsProp 82 loss=330.777618 err=330.777618
I 2015-05-26 04:57:40 theanets.trainer:168 RmsProp 83 loss=325.145050 err=325.145050
I 2015-05-26 04:58:33 theanets.trainer:168 RmsProp 84 loss=311.911438 err=311.911438
I 2015-05-26 04:59:26 theanets.trainer:168 RmsProp 85 loss=307.478027 err=307.478027
I 2015-05-26 05:00:18 theanets.trainer:168 RmsProp 86 loss=299.757690 err=299.757690
I 2015-05-26 05:01:11 theanets.trainer:168 RmsProp 87 loss=295.591858 err=295.591858
I 2015-05-26 05:02:04 theanets.trainer:168 RmsProp 88 loss=285.448181 err=285.448181
I 2015-05-26 05:02:56 theanets.trainer:168 RmsProp 89 loss=280.757019 err=280.757019
I 2015-05-26 05:03:49 theanets.trainer:168 RmsProp 90 loss=265.392548 err=265.392548
I 2015-05-26 05:03:50 theanets.trainer:168 validation 9 loss=1521.873047 err=1521.873047 *
I 2015-05-26 05:04:44 theanets.trainer:168 RmsProp 91 loss=262.952026 err=262.952026
I 2015-05-26 05:05:37 theanets.trainer:168 RmsProp 92 loss=254.021530 err=254.021530
I 2015-05-26 05:06:30 theanets.trainer:168 RmsProp 93 loss=240.673126 err=240.673126
I 2015-05-26 05:07:24 theanets.trainer:168 RmsProp 94 loss=240.528519 err=240.528519
I 2015-05-26 05:08:15 theanets.trainer:168 RmsProp 95 loss=232.963516 err=232.963516
I 2015-05-26 05:09:06 theanets.trainer:168 RmsProp 96 loss=228.308914 err=228.308914
I 2015-05-26 05:09:57 theanets.trainer:168 RmsProp 97 loss=227.114883 err=227.114883
I 2015-05-26 05:10:47 theanets.trainer:168 RmsProp 98 loss=220.106628 err=220.106628
I 2015-05-26 05:11:37 theanets.trainer:168 RmsProp 99 loss=214.868408 err=214.868408
I 2015-05-26 05:12:27 theanets.trainer:168 RmsProp 100 loss=206.866272 err=206.866272
I 2015-05-26 05:12:28 theanets.trainer:168 validation 10 loss=1473.232422 err=1473.232422 *
I 2015-05-26 05:13:17 theanets.trainer:168 RmsProp 101 loss=199.357880 err=199.357880
I 2015-05-26 05:14:07 theanets.trainer:168 RmsProp 102 loss=200.875671 err=200.875671
I 2015-05-26 05:14:58 theanets.trainer:168 RmsProp 103 loss=190.214157 err=190.214157
I 2015-05-26 05:15:48 theanets.trainer:168 RmsProp 104 loss=183.948807 err=183.948807
I 2015-05-26 05:16:39 theanets.trainer:168 RmsProp 105 loss=183.957001 err=183.957001
I 2015-05-26 05:17:29 theanets.trainer:168 RmsProp 106 loss=189.735764 err=189.735764
I 2015-05-26 05:18:20 theanets.trainer:168 RmsProp 107 loss=175.172089 err=175.172089
I 2015-05-26 05:19:10 theanets.trainer:168 RmsProp 108 loss=171.765808 err=171.765808
I 2015-05-26 05:20:01 theanets.trainer:168 RmsProp 109 loss=164.745346 err=164.745346
I 2015-05-26 05:20:51 theanets.trainer:168 RmsProp 110 loss=165.381927 err=165.381927
I 2015-05-26 05:20:52 theanets.trainer:168 validation 11 loss=1446.304077 err=1446.304077 *
I 2015-05-26 05:21:43 theanets.trainer:168 RmsProp 111 loss=162.537811 err=162.537811
I 2015-05-26 05:22:33 theanets.trainer:168 RmsProp 112 loss=154.475647 err=154.475647
I 2015-05-26 05:23:24 theanets.trainer:168 RmsProp 113 loss=149.172546 err=149.172546
I 2015-05-26 05:24:15 theanets.trainer:168 RmsProp 114 loss=149.253799 err=149.253799
I 2015-05-26 05:25:05 theanets.trainer:168 RmsProp 115 loss=143.692627 err=143.692627
I 2015-05-26 05:25:56 theanets.trainer:168 RmsProp 116 loss=139.331726 err=139.331726
I 2015-05-26 05:26:46 theanets.trainer:168 RmsProp 117 loss=135.365952 err=135.365952
I 2015-05-26 05:27:37 theanets.trainer:168 RmsProp 118 loss=135.812561 err=135.812561
I 2015-05-26 05:28:28 theanets.trainer:168 RmsProp 119 loss=139.187683 err=139.187683
I 2015-05-26 05:29:18 theanets.trainer:168 RmsProp 120 loss=128.428528 err=128.428528
I 2015-05-26 05:29:19 theanets.trainer:168 validation 12 loss=1414.134399 err=1414.134399 *
I 2015-05-26 05:30:10 theanets.trainer:168 RmsProp 121 loss=126.326309 err=126.326309
I 2015-05-26 05:31:01 theanets.trainer:168 RmsProp 122 loss=121.046463 err=121.046463
I 2015-05-26 05:31:52 theanets.trainer:168 RmsProp 123 loss=120.068398 err=120.068398
I 2015-05-26 05:32:42 theanets.trainer:168 RmsProp 124 loss=120.214615 err=120.214615
I 2015-05-26 05:33:33 theanets.trainer:168 RmsProp 125 loss=114.888664 err=114.888664
I 2015-05-26 05:34:24 theanets.trainer:168 RmsProp 126 loss=111.712395 err=111.712395
I 2015-05-26 05:35:15 theanets.trainer:168 RmsProp 127 loss=114.102661 err=114.102661
I 2015-05-26 05:36:06 theanets.trainer:168 RmsProp 128 loss=107.857285 err=107.857285
I 2015-05-26 05:36:56 theanets.trainer:168 RmsProp 129 loss=106.346733 err=106.346733
I 2015-05-26 05:37:45 theanets.trainer:168 RmsProp 130 loss=101.757729 err=101.757729
I 2015-05-26 05:37:47 theanets.trainer:168 validation 13 loss=1412.126587 err=1412.126587 *
I 2015-05-26 05:38:35 theanets.trainer:168 RmsProp 131 loss=99.839935 err=99.839935
I 2015-05-26 05:39:23 theanets.trainer:168 RmsProp 132 loss=101.231674 err=101.231674
I 2015-05-26 05:40:11 theanets.trainer:168 RmsProp 133 loss=95.131340 err=95.131340
I 2015-05-26 05:41:00 theanets.trainer:168 RmsProp 134 loss=92.616333 err=92.616333
I 2015-05-26 05:41:49 theanets.trainer:168 RmsProp 135 loss=92.366524 err=92.366524
I 2015-05-26 05:42:38 theanets.trainer:168 RmsProp 136 loss=88.207054 err=88.207054
I 2015-05-26 05:43:27 theanets.trainer:168 RmsProp 137 loss=86.725777 err=86.725777
I 2015-05-26 05:44:16 theanets.trainer:168 RmsProp 138 loss=84.506638 err=84.506638
I 2015-05-26 05:45:06 theanets.trainer:168 RmsProp 139 loss=83.716515 err=83.716515
I 2015-05-26 05:45:55 theanets.trainer:168 RmsProp 140 loss=81.225067 err=81.225067
I 2015-05-26 05:45:56 theanets.trainer:168 validation 14 loss=1369.314087 err=1369.314087 *
I 2015-05-26 05:46:46 theanets.trainer:168 RmsProp 141 loss=80.122574 err=80.122574
I 2015-05-26 05:47:35 theanets.trainer:168 RmsProp 142 loss=77.965668 err=77.965668
I 2015-05-26 05:48:25 theanets.trainer:168 RmsProp 143 loss=77.301926 err=77.301926
I 2015-05-26 05:49:14 theanets.trainer:168 RmsProp 144 loss=76.834488 err=76.834488
I 2015-05-26 05:50:04 theanets.trainer:168 RmsProp 145 loss=75.024910 err=75.024910
I 2015-05-26 05:50:54 theanets.trainer:168 RmsProp 146 loss=72.055992 err=72.055992
I 2015-05-26 05:51:43 theanets.trainer:168 RmsProp 147 loss=71.108078 err=71.108078
I 2015-05-26 05:52:32 theanets.trainer:168 RmsProp 148 loss=68.746925 err=68.746925
I 2015-05-26 05:53:21 theanets.trainer:168 RmsProp 149 loss=66.689499 err=66.689499
I 2015-05-26 05:54:11 theanets.trainer:168 RmsProp 150 loss=67.162926 err=67.162926
I 2015-05-26 05:54:12 theanets.trainer:168 validation 15 loss=1335.907837 err=1335.907837 *
I 2015-05-26 05:55:01 theanets.trainer:168 RmsProp 151 loss=66.373566 err=66.373566
I 2015-05-26 05:55:50 theanets.trainer:168 RmsProp 152 loss=64.311798 err=64.311798
I 2015-05-26 05:56:40 theanets.trainer:168 RmsProp 153 loss=62.451756 err=62.451756
I 2015-05-26 05:57:30 theanets.trainer:168 RmsProp 154 loss=61.685680 err=61.685680
I 2015-05-26 05:58:19 theanets.trainer:168 RmsProp 155 loss=60.926716 err=60.926716
I 2015-05-26 05:59:08 theanets.trainer:168 RmsProp 156 loss=61.506569 err=61.506569
I 2015-05-26 05:59:57 theanets.trainer:168 RmsProp 157 loss=59.288513 err=59.288513
I 2015-05-26 06:00:47 theanets.trainer:168 RmsProp 158 loss=58.339027 err=58.339027
I 2015-05-26 06:01:36 theanets.trainer:168 RmsProp 159 loss=55.869034 err=55.869034
I 2015-05-26 06:02:26 theanets.trainer:168 RmsProp 160 loss=55.851059 err=55.851059
I 2015-05-26 06:02:27 theanets.trainer:168 validation 16 loss=1307.043335 err=1307.043335 *
I 2015-05-26 06:03:16 theanets.trainer:168 RmsProp 161 loss=54.376396 err=54.376396
I 2015-05-26 06:04:06 theanets.trainer:168 RmsProp 162 loss=53.860729 err=53.860729
I 2015-05-26 06:04:55 theanets.trainer:168 RmsProp 163 loss=52.638599 err=52.638599
I 2015-05-26 06:05:45 theanets.trainer:168 RmsProp 164 loss=50.894341 err=50.894341
I 2015-05-26 06:06:34 theanets.trainer:168 RmsProp 165 loss=50.373508 err=50.373508
I 2015-05-26 06:07:21 theanets.trainer:168 RmsProp 166 loss=49.444706 err=49.444706
I 2015-05-26 06:08:09 theanets.trainer:168 RmsProp 167 loss=48.261921 err=48.261921
I 2015-05-26 06:08:58 theanets.trainer:168 RmsProp 168 loss=48.889641 err=48.889641
I 2015-05-26 06:09:48 theanets.trainer:168 RmsProp 169 loss=47.105087 err=47.105087
I 2015-05-26 06:10:37 theanets.trainer:168 RmsProp 170 loss=46.123356 err=46.123356
I 2015-05-26 06:10:38 theanets.trainer:168 validation 17 loss=1287.263062 err=1287.263062 *
I 2015-05-26 06:11:27 theanets.trainer:168 RmsProp 171 loss=45.415829 err=45.415829
I 2015-05-26 06:12:16 theanets.trainer:168 RmsProp 172 loss=43.995457 err=43.995457
I 2015-05-26 06:13:06 theanets.trainer:168 RmsProp 173 loss=43.185444 err=43.185444
I 2015-05-26 06:13:56 theanets.trainer:168 RmsProp 174 loss=42.204601 err=42.204601
I 2015-05-26 06:14:45 theanets.trainer:168 RmsProp 175 loss=41.727749 err=41.727749
I 2015-05-26 06:15:34 theanets.trainer:168 RmsProp 176 loss=41.151237 err=41.151237
I 2015-05-26 06:16:24 theanets.trainer:168 RmsProp 177 loss=41.128254 err=41.128254
I 2015-05-26 06:17:14 theanets.trainer:168 RmsProp 178 loss=40.384087 err=40.384087
I 2015-05-26 06:18:04 theanets.trainer:168 RmsProp 179 loss=39.279251 err=39.279251
I 2015-05-26 06:18:54 theanets.trainer:168 RmsProp 180 loss=38.245518 err=38.245518
I 2015-05-26 06:18:56 theanets.trainer:168 validation 18 loss=1275.414917 err=1275.414917 *
I 2015-05-26 06:19:44 theanets.trainer:168 RmsProp 181 loss=36.963940 err=36.963940
I 2015-05-26 06:20:31 theanets.trainer:168 RmsProp 182 loss=40.655525 err=40.655525
I 2015-05-26 06:21:19 theanets.trainer:168 RmsProp 183 loss=37.141705 err=37.141705
I 2015-05-26 06:22:08 theanets.trainer:168 RmsProp 184 loss=36.149555 err=36.149555
I 2015-05-26 06:22:58 theanets.trainer:168 RmsProp 185 loss=35.325619 err=35.325619
I 2015-05-26 06:23:46 theanets.trainer:168 RmsProp 186 loss=34.208729 err=34.208729
I 2015-05-26 06:24:35 theanets.trainer:168 RmsProp 187 loss=34.285564 err=34.285564
I 2015-05-26 06:25:24 theanets.trainer:168 RmsProp 188 loss=35.534035 err=35.534035
I 2015-05-26 06:26:14 theanets.trainer:168 RmsProp 189 loss=33.524429 err=33.524429
I 2015-05-26 06:27:03 theanets.trainer:168 RmsProp 190 loss=32.803665 err=32.803665
I 2015-05-26 06:27:04 theanets.trainer:168 validation 19 loss=1242.861572 err=1242.861572 *
I 2015-05-26 06:27:53 theanets.trainer:168 RmsProp 191 loss=32.520359 err=32.520359
I 2015-05-26 06:28:43 theanets.trainer:168 RmsProp 192 loss=32.020588 err=32.020588
I 2015-05-26 06:29:32 theanets.trainer:168 RmsProp 193 loss=31.147652 err=31.147652
I 2015-05-26 06:30:22 theanets.trainer:168 RmsProp 194 loss=30.534782 err=30.534782
I 2015-05-26 06:31:11 theanets.trainer:168 RmsProp 195 loss=29.355007 err=29.355007
I 2015-05-26 06:32:01 theanets.trainer:168 RmsProp 196 loss=29.106014 err=29.106014
I 2015-05-26 06:32:51 theanets.trainer:168 RmsProp 197 loss=28.930035 err=28.930035
I 2015-05-26 06:33:41 theanets.trainer:168 RmsProp 198 loss=27.947712 err=27.947712
I 2015-05-26 06:34:29 theanets.trainer:168 RmsProp 199 loss=28.958916 err=28.958916
I 2015-05-26 06:35:14 theanets.trainer:168 RmsProp 200 loss=27.548670 err=27.548670
I 2015-05-26 06:35:15 theanets.trainer:168 validation 20 loss=1206.917358 err=1206.917358 *
I 2015-05-26 06:36:01 theanets.trainer:168 RmsProp 201 loss=26.483728 err=26.483728
I 2015-05-26 06:36:47 theanets.trainer:168 RmsProp 202 loss=26.506981 err=26.506981
I 2015-05-26 06:37:33 theanets.trainer:168 RmsProp 203 loss=26.131281 err=26.131281
I 2015-05-26 06:38:20 theanets.trainer:168 RmsProp 204 loss=26.150990 err=26.150990
I 2015-05-26 06:39:07 theanets.trainer:168 RmsProp 205 loss=25.345705 err=25.345705
I 2015-05-26 06:39:54 theanets.trainer:168 RmsProp 206 loss=24.946089 err=24.946089
I 2015-05-26 06:40:39 theanets.trainer:168 RmsProp 207 loss=24.116064 err=24.116064
I 2015-05-26 06:41:25 theanets.trainer:168 RmsProp 208 loss=25.523590 err=25.523590
I 2015-05-26 06:42:10 theanets.trainer:168 RmsProp 209 loss=23.651232 err=23.651232
I 2015-05-26 06:42:54 theanets.trainer:168 RmsProp 210 loss=26.644344 err=26.644344
I 2015-05-26 06:42:55 theanets.trainer:168 validation 21 loss=1234.299194 err=1234.299194
I 2015-05-26 06:43:40 theanets.trainer:168 RmsProp 211 loss=17.933638 err=17.933638
I 2015-05-26 06:44:25 theanets.trainer:168 RmsProp 212 loss=25.151087 err=25.151087
I 2015-05-26 06:45:10 theanets.trainer:168 RmsProp 213 loss=22.941420 err=22.941420
I 2015-05-26 06:45:55 theanets.trainer:168 RmsProp 214 loss=21.762320 err=21.762320
I 2015-05-26 06:46:41 theanets.trainer:168 RmsProp 215 loss=20.772738 err=20.772738
I 2015-05-26 06:47:26 theanets.trainer:168 RmsProp 216 loss=18.622911 err=18.622911
I 2015-05-26 06:48:10 theanets.trainer:168 RmsProp 217 loss=18.480078 err=18.480078
I 2015-05-26 06:48:55 theanets.trainer:168 RmsProp 218 loss=18.781017 err=18.781017
I 2015-05-26 06:49:40 theanets.trainer:168 RmsProp 219 loss=19.962641 err=19.962641
I 2015-05-26 06:50:25 theanets.trainer:168 RmsProp 220 loss=21.658911 err=21.658911
I 2015-05-26 06:50:26 theanets.trainer:168 validation 22 loss=1173.633667 err=1173.633667 *
I 2015-05-26 06:51:11 theanets.trainer:168 RmsProp 221 loss=19.291328 err=19.291328
I 2015-05-26 06:51:56 theanets.trainer:168 RmsProp 222 loss=18.721022 err=18.721022
I 2015-05-26 06:52:40 theanets.trainer:168 RmsProp 223 loss=18.378633 err=18.378633
I 2015-05-26 06:53:25 theanets.trainer:168 RmsProp 224 loss=20.402493 err=20.402493
I 2015-05-26 06:54:10 theanets.trainer:168 RmsProp 225 loss=18.157419 err=18.157419
I 2015-05-26 06:54:55 theanets.trainer:168 RmsProp 226 loss=17.430689 err=17.430689
I 2015-05-26 06:55:40 theanets.trainer:168 RmsProp 227 loss=23.064226 err=23.064226
I 2015-05-26 06:56:26 theanets.trainer:168 RmsProp 228 loss=19.205187 err=19.205187
I 2015-05-26 06:57:11 theanets.trainer:168 RmsProp 229 loss=18.245520 err=18.245520
I 2015-05-26 06:57:54 theanets.trainer:168 RmsProp 230 loss=19.755831 err=19.755831
I 2015-05-26 06:57:55 theanets.trainer:168 validation 23 loss=1252.973999 err=1252.973999
I 2015-05-26 06:58:36 theanets.trainer:168 RmsProp 231 loss=15.661045 err=15.661045
I 2015-05-26 06:59:17 theanets.trainer:168 RmsProp 232 loss=13.881123 err=13.881123
I 2015-05-26 06:59:58 theanets.trainer:168 RmsProp 233 loss=14.446893 err=14.446893
I 2015-05-26 07:00:38 theanets.trainer:168 RmsProp 234 loss=14.532828 err=14.532828
I 2015-05-26 07:01:19 theanets.trainer:168 RmsProp 235 loss=13.678558 err=13.678558
I 2015-05-26 07:02:00 theanets.trainer:168 RmsProp 236 loss=14.035867 err=14.035867
I 2015-05-26 07:02:41 theanets.trainer:168 RmsProp 237 loss=14.470395 err=14.470395
I 2015-05-26 07:03:23 theanets.trainer:168 RmsProp 238 loss=12.914797 err=12.914797
I 2015-05-26 07:04:04 theanets.trainer:168 RmsProp 239 loss=13.188218 err=13.188218
I 2015-05-26 07:04:45 theanets.trainer:168 RmsProp 240 loss=13.152311 err=13.152311
I 2015-05-26 07:04:46 theanets.trainer:168 validation 24 loss=1244.653198 err=1244.653198
I 2015-05-26 07:05:25 theanets.trainer:168 RmsProp 241 loss=13.052675 err=13.052675
I 2015-05-26 07:06:04 theanets.trainer:168 RmsProp 242 loss=12.421382 err=12.421382
I 2015-05-26 07:06:43 theanets.trainer:168 RmsProp 243 loss=12.278452 err=12.278452
I 2015-05-26 07:07:25 theanets.trainer:168 RmsProp 244 loss=13.269917 err=13.269917
I 2015-05-26 07:08:07 theanets.trainer:168 RmsProp 245 loss=13.797825 err=13.797825
I 2015-05-26 07:08:48 theanets.trainer:168 RmsProp 246 loss=16.786560 err=16.786560
I 2015-05-26 07:09:29 theanets.trainer:168 RmsProp 247 loss=14.806728 err=14.806728
I 2015-05-26 07:10:10 theanets.trainer:168 RmsProp 248 loss=11.854024 err=11.854024
I 2015-05-26 07:10:51 theanets.trainer:168 RmsProp 249 loss=11.187308 err=11.187308
I 2015-05-26 07:11:32 theanets.trainer:168 RmsProp 250 loss=11.648424 err=11.648424
I 2015-05-26 07:11:33 theanets.trainer:168 validation 25 loss=1188.183472 err=1188.183472
I 2015-05-26 07:12:12 theanets.trainer:168 RmsProp 251 loss=12.167055 err=12.167055
I 2015-05-26 07:12:51 theanets.trainer:168 RmsProp 252 loss=12.012326 err=12.012326
I 2015-05-26 07:13:30 theanets.trainer:168 RmsProp 253 loss=11.811833 err=11.811833
I 2015-05-26 07:14:10 theanets.trainer:168 RmsProp 254 loss=11.118517 err=11.118517
I 2015-05-26 07:14:51 theanets.trainer:168 RmsProp 255 loss=11.189495 err=11.189495
I 2015-05-26 07:15:32 theanets.trainer:168 RmsProp 256 loss=10.089227 err=10.089227
I 2015-05-26 07:16:13 theanets.trainer:168 RmsProp 257 loss=10.208792 err=10.208792
I 2015-05-26 07:16:54 theanets.trainer:168 RmsProp 258 loss=10.420691 err=10.420691
I 2015-05-26 07:17:34 theanets.trainer:168 RmsProp 259 loss=10.603942 err=10.603942
I 2015-05-26 07:18:14 theanets.trainer:168 RmsProp 260 loss=9.551849 err=9.551849
I 2015-05-26 07:18:15 theanets.trainer:168 validation 26 loss=1199.024414 err=1199.024414
I 2015-05-26 07:18:56 theanets.trainer:168 RmsProp 261 loss=9.601449 err=9.601449
I 2015-05-26 07:19:37 theanets.trainer:168 RmsProp 262 loss=9.775270 err=9.775270
I 2015-05-26 07:20:18 theanets.trainer:168 RmsProp 263 loss=9.857800 err=9.857800
I 2015-05-26 07:20:59 theanets.trainer:168 RmsProp 264 loss=9.832703 err=9.832703
I 2015-05-26 07:21:40 theanets.trainer:168 RmsProp 265 loss=8.992584 err=8.992584
I 2015-05-26 07:22:21 theanets.trainer:168 RmsProp 266 loss=9.317663 err=9.317663
I 2015-05-26 07:23:01 theanets.trainer:168 RmsProp 267 loss=8.886765 err=8.886765
I 2015-05-26 07:23:43 theanets.trainer:168 RmsProp 268 loss=8.235657 err=8.235657
I 2015-05-26 07:24:25 theanets.trainer:168 RmsProp 269 loss=9.228408 err=9.228408
I 2015-05-26 07:25:05 theanets.trainer:168 RmsProp 270 loss=8.682693 err=8.682693
I 2015-05-26 07:25:06 theanets.trainer:168 validation 27 loss=1184.542114 err=1184.542114
I 2015-05-26 07:25:06 theanets.trainer:252 patience elapsed!
I 2015-05-26 07:25:06 theanets.dataset:133 valid: 2 mini-batches from callable
I 2015-05-26 07:25:06 theanets.dataset:133 train: 10 mini-batches from callable
I 2015-05-26 07:25:06 theanets.main:87 creating trainer <class 'theanets.trainer.RmsProp'>
I 2015-05-26 07:25:06 theanets.main:89 --algorithms = rmsprop
I 2015-05-26 07:25:06 theanets.main:89 --batch_size = 1024
I 2015-05-26 07:25:06 theanets.main:89 --gradient_clip = 100000.0
I 2015-05-26 07:25:06 theanets.main:89 --hidden_l1 = None
I 2015-05-26 07:25:06 theanets.main:89 --learning_rate = 0.0001
I 2015-05-26 07:25:06 theanets.main:89 --train_batches = 10
I 2015-05-26 07:25:06 theanets.main:89 --valid_batches = 2
I 2015-05-26 07:25:06 theanets.main:89 --weight_l1 = None
I 2015-05-26 07:25:06 theanets.main:89 --weight_l2 = None
I 2015-05-26 07:25:06 theanets.trainer:134 compiling evaluation function
I 2015-05-26 07:25:15 theanets.trainer:294 compiling RmsProp learning function
I 2015-05-26 07:26:51 theanets.trainer:168 validation 0 loss=1538.418335 err=1538.418335 *
I 2015-05-26 07:27:04 theanets.trainer:168 RmsProp 1 loss=11.921253 err=11.921253
I 2015-05-26 07:27:17 theanets.trainer:168 RmsProp 2 loss=8.130159 err=8.130159
I 2015-05-26 07:27:29 theanets.trainer:168 RmsProp 3 loss=6.056401 err=6.056401
I 2015-05-26 07:27:42 theanets.trainer:168 RmsProp 4 loss=5.054807 err=5.054807
I 2015-05-26 07:27:54 theanets.trainer:168 RmsProp 5 loss=4.435428 err=4.435428
I 2015-05-26 07:28:06 theanets.trainer:168 RmsProp 6 loss=3.969123 err=3.969123
I 2015-05-26 07:28:19 theanets.trainer:168 RmsProp 7 loss=3.598974 err=3.598974
I 2015-05-26 07:28:31 theanets.trainer:168 RmsProp 8 loss=3.249037 err=3.249037
I 2015-05-26 07:28:44 theanets.trainer:168 RmsProp 9 loss=3.036503 err=3.036503
I 2015-05-26 07:28:56 theanets.trainer:168 RmsProp 10 loss=2.764799 err=2.764799
I 2015-05-26 07:28:57 theanets.trainer:168 validation 1 loss=1500.425049 err=1500.425049 *
I 2015-05-26 07:29:09 theanets.trainer:168 RmsProp 11 loss=2.604934 err=2.604934
I 2015-05-26 07:29:22 theanets.trainer:168 RmsProp 12 loss=2.515323 err=2.515323
I 2015-05-26 07:29:34 theanets.trainer:168 RmsProp 13 loss=2.314151 err=2.314151
I 2015-05-26 07:29:47 theanets.trainer:168 RmsProp 14 loss=2.225117 err=2.225117
I 2015-05-26 07:30:00 theanets.trainer:168 RmsProp 15 loss=2.162528 err=2.162528
I 2015-05-26 07:30:13 theanets.trainer:168 RmsProp 16 loss=1.969376 err=1.969376
I 2015-05-26 07:30:25 theanets.trainer:168 RmsProp 17 loss=1.976936 err=1.976936
I 2015-05-26 07:30:38 theanets.trainer:168 RmsProp 18 loss=1.925294 err=1.925294
I 2015-05-26 07:30:50 theanets.trainer:168 RmsProp 19 loss=1.845187 err=1.845187
I 2015-05-26 07:31:03 theanets.trainer:168 RmsProp 20 loss=1.787570 err=1.787570
I 2015-05-26 07:31:03 theanets.trainer:168 validation 2 loss=1488.663452 err=1488.663452 *
I 2015-05-26 07:31:16 theanets.trainer:168 RmsProp 21 loss=1.680477 err=1.680477
I 2015-05-26 07:31:28 theanets.trainer:168 RmsProp 22 loss=1.707386 err=1.707386
I 2015-05-26 07:31:40 theanets.trainer:168 RmsProp 23 loss=1.607249 err=1.607249
I 2015-05-26 07:31:52 theanets.trainer:168 RmsProp 24 loss=1.536714 err=1.536714
I 2015-05-26 07:32:04 theanets.trainer:168 RmsProp 25 loss=1.497770 err=1.497770
I 2015-05-26 07:32:16 theanets.trainer:168 RmsProp 26 loss=1.446379 err=1.446379
I 2015-05-26 07:32:28 theanets.trainer:168 RmsProp 27 loss=1.459133 err=1.459133
I 2015-05-26 07:32:40 theanets.trainer:168 RmsProp 28 loss=1.386765 err=1.386765
I 2015-05-26 07:32:52 theanets.trainer:168 RmsProp 29 loss=1.364035 err=1.364035
I 2015-05-26 07:33:03 theanets.trainer:168 RmsProp 30 loss=1.326402 err=1.326402
I 2015-05-26 07:33:04 theanets.trainer:168 validation 3 loss=1482.029419 err=1482.029419 *
I 2015-05-26 07:33:16 theanets.trainer:168 RmsProp 31 loss=1.310062 err=1.310062
I 2015-05-26 07:33:28 theanets.trainer:168 RmsProp 32 loss=1.235975 err=1.235975
I 2015-05-26 07:33:41 theanets.trainer:168 RmsProp 33 loss=1.263903 err=1.263903
I 2015-05-26 07:33:53 theanets.trainer:168 RmsProp 34 loss=1.218403 err=1.218403
I 2015-05-26 07:34:06 theanets.trainer:168 RmsProp 35 loss=1.198797 err=1.198797
I 2015-05-26 07:34:18 theanets.trainer:168 RmsProp 36 loss=1.202648 err=1.202648
I 2015-05-26 07:34:30 theanets.trainer:168 RmsProp 37 loss=1.142227 err=1.142227
I 2015-05-26 07:34:43 theanets.trainer:168 RmsProp 38 loss=1.123879 err=1.123879
I 2015-05-26 07:34:55 theanets.trainer:168 RmsProp 39 loss=1.098041 err=1.098041
I 2015-05-26 07:35:08 theanets.trainer:168 RmsProp 40 loss=1.075373 err=1.075373
I 2015-05-26 07:35:08 theanets.trainer:168 validation 4 loss=1474.268921 err=1474.268921 *
I 2015-05-26 07:35:21 theanets.trainer:168 RmsProp 41 loss=1.083334 err=1.083334
I 2015-05-26 07:35:33 theanets.trainer:168 RmsProp 42 loss=1.061465 err=1.061465
I 2015-05-26 07:35:45 theanets.trainer:168 RmsProp 43 loss=1.017394 err=1.017394
I 2015-05-26 07:35:58 theanets.trainer:168 RmsProp 44 loss=1.029688 err=1.029688
I 2015-05-26 07:36:10 theanets.trainer:168 RmsProp 45 loss=0.993888 err=0.993888
I 2015-05-26 07:36:23 theanets.trainer:168 RmsProp 46 loss=1.040413 err=1.040413
I 2015-05-26 07:36:35 theanets.trainer:168 RmsProp 47 loss=0.981632 err=0.981632
I 2015-05-26 07:36:48 theanets.trainer:168 RmsProp 48 loss=0.930111 err=0.930111
I 2015-05-26 07:37:00 theanets.trainer:168 RmsProp 49 loss=0.946973 err=0.946973
I 2015-05-26 07:37:13 theanets.trainer:168 RmsProp 50 loss=0.930995 err=0.930995
I 2015-05-26 07:37:13 theanets.trainer:168 validation 5 loss=1469.732666 err=1469.732666 *
I 2015-05-26 07:37:26 theanets.trainer:168 RmsProp 51 loss=0.926927 err=0.926927
I 2015-05-26 07:37:38 theanets.trainer:168 RmsProp 52 loss=0.947953 err=0.947953
I 2015-05-26 07:37:51 theanets.trainer:168 RmsProp 53 loss=0.893490 err=0.893490
I 2015-05-26 07:38:04 theanets.trainer:168 RmsProp 54 loss=0.853110 err=0.853110
I 2015-05-26 07:38:16 theanets.trainer:168 RmsProp 55 loss=0.879976 err=0.879976
I 2015-05-26 07:38:29 theanets.trainer:168 RmsProp 56 loss=0.849023 err=0.849023
I 2015-05-26 07:38:41 theanets.trainer:168 RmsProp 57 loss=0.882523 err=0.882523
I 2015-05-26 07:38:53 theanets.trainer:168 RmsProp 58 loss=0.819094 err=0.819094
I 2015-05-26 07:39:06 theanets.trainer:168 RmsProp 59 loss=0.814614 err=0.814614
I 2015-05-26 07:39:18 theanets.trainer:168 RmsProp 60 loss=0.828551 err=0.828551
I 2015-05-26 07:39:19 theanets.trainer:168 validation 6 loss=1463.624756 err=1463.624756 *
I 2015-05-26 07:39:32 theanets.trainer:168 RmsProp 61 loss=0.809678 err=0.809678
I 2015-05-26 07:39:45 theanets.trainer:168 RmsProp 62 loss=0.781874 err=0.781874
I 2015-05-26 07:39:57 theanets.trainer:168 RmsProp 63 loss=0.816456 err=0.816456
I 2015-05-26 07:40:11 theanets.trainer:168 RmsProp 64 loss=0.758835 err=0.758835
I 2015-05-26 07:40:24 theanets.trainer:168 RmsProp 65 loss=0.756322 err=0.756322
I 2015-05-26 07:40:37 theanets.trainer:168 RmsProp 66 loss=0.772784 err=0.772784
I 2015-05-26 07:40:49 theanets.trainer:168 RmsProp 67 loss=0.746920 err=0.746920
I 2015-05-26 07:41:02 theanets.trainer:168 RmsProp 68 loss=0.714152 err=0.714152
I 2015-05-26 07:41:15 theanets.trainer:168 RmsProp 69 loss=0.806277 err=0.806277
I 2015-05-26 07:41:28 theanets.trainer:168 RmsProp 70 loss=0.708475 err=0.708475
I 2015-05-26 07:41:29 theanets.trainer:168 validation 7 loss=1459.533813 err=1459.533813 *
I 2015-05-26 07:41:41 theanets.trainer:168 RmsProp 71 loss=0.700177 err=0.700177
I 2015-05-26 07:41:54 theanets.trainer:168 RmsProp 72 loss=0.725142 err=0.725142
I 2015-05-26 07:42:07 theanets.trainer:168 RmsProp 73 loss=0.696402 err=0.696402
I 2015-05-26 07:42:20 theanets.trainer:168 RmsProp 74 loss=0.701563 err=0.701563
I 2015-05-26 07:42:33 theanets.trainer:168 RmsProp 75 loss=0.691807 err=0.691807
I 2015-05-26 07:42:45 theanets.trainer:168 RmsProp 76 loss=0.685300 err=0.685300
I 2015-05-26 07:42:57 theanets.trainer:168 RmsProp 77 loss=0.686821 err=0.686821
I 2015-05-26 07:43:10 theanets.trainer:168 RmsProp 78 loss=0.669994 err=0.669994
I 2015-05-26 07:43:22 theanets.trainer:168 RmsProp 79 loss=0.672346 err=0.672346
I 2015-05-26 07:43:35 theanets.trainer:168 RmsProp 80 loss=0.666613 err=0.666613
I 2015-05-26 07:43:35 theanets.trainer:168 validation 8 loss=1461.288696 err=1461.288696
I 2015-05-26 07:43:47 theanets.trainer:168 RmsProp 81 loss=0.651484 err=0.651484
I 2015-05-26 07:44:00 theanets.trainer:168 RmsProp 82 loss=0.648436 err=0.648436
I 2015-05-26 07:44:12 theanets.trainer:168 RmsProp 83 loss=0.654265 err=0.654265
I 2015-05-26 07:44:24 theanets.trainer:168 RmsProp 84 loss=0.637987 err=0.637987
I 2015-05-26 07:44:37 theanets.trainer:168 RmsProp 85 loss=0.631042 err=0.631042
I 2015-05-26 07:44:49 theanets.trainer:168 RmsProp 86 loss=0.628461 err=0.628461
I 2015-05-26 07:45:01 theanets.trainer:168 RmsProp 87 loss=0.616489 err=0.616489
I 2015-05-26 07:45:13 theanets.trainer:168 RmsProp 88 loss=0.622994 err=0.622994
I 2015-05-26 07:45:25 theanets.trainer:168 RmsProp 89 loss=0.650242 err=0.650242
I 2015-05-26 07:45:38 theanets.trainer:168 RmsProp 90 loss=0.608725 err=0.608725
I 2015-05-26 07:45:38 theanets.trainer:168 validation 9 loss=1455.869507 err=1455.869507 *
I 2015-05-26 07:45:50 theanets.trainer:168 RmsProp 91 loss=0.571495 err=0.571495
I 2015-05-26 07:46:02 theanets.trainer:168 RmsProp 92 loss=0.616347 err=0.616347
I 2015-05-26 07:46:14 theanets.trainer:168 RmsProp 93 loss=0.615362 err=0.615362
I 2015-05-26 07:46:26 theanets.trainer:168 RmsProp 94 loss=0.570791 err=0.570791
I 2015-05-26 07:46:38 theanets.trainer:168 RmsProp 95 loss=0.586820 err=0.586820
I 2015-05-26 07:46:50 theanets.trainer:168 RmsProp 96 loss=0.581009 err=0.581009
I 2015-05-26 07:47:03 theanets.trainer:168 RmsProp 97 loss=0.575743 err=0.575743
I 2015-05-26 07:47:15 theanets.trainer:168 RmsProp 98 loss=0.568517 err=0.568517
I 2015-05-26 07:47:27 theanets.trainer:168 RmsProp 99 loss=0.580604 err=0.580604
I 2015-05-26 07:47:40 theanets.trainer:168 RmsProp 100 loss=0.539077 err=0.539077
I 2015-05-26 07:47:40 theanets.trainer:168 validation 10 loss=1454.908081 err=1454.908081 *
I 2015-05-26 07:47:52 theanets.trainer:168 RmsProp 101 loss=0.554225 err=0.554225
I 2015-05-26 07:48:05 theanets.trainer:168 RmsProp 102 loss=0.543922 err=0.543922
I 2015-05-26 07:48:17 theanets.trainer:168 RmsProp 103 loss=0.562760 err=0.562760
I 2015-05-26 07:48:29 theanets.trainer:168 RmsProp 104 loss=0.537312 err=0.537312
I 2015-05-26 07:48:41 theanets.trainer:168 RmsProp 105 loss=0.555262 err=0.555262
I 2015-05-26 07:48:53 theanets.trainer:168 RmsProp 106 loss=0.543379 err=0.543379
I 2015-05-26 07:49:05 theanets.trainer:168 RmsProp 107 loss=0.503739 err=0.503739
I 2015-05-26 07:49:17 theanets.trainer:168 RmsProp 108 loss=0.554482 err=0.554482
I 2015-05-26 07:49:29 theanets.trainer:168 RmsProp 109 loss=0.554025 err=0.554025
I 2015-05-26 07:49:41 theanets.trainer:168 RmsProp 110 loss=0.518544 err=0.518544
I 2015-05-26 07:49:42 theanets.trainer:168 validation 11 loss=1450.073364 err=1450.073364 *
I 2015-05-26 07:49:54 theanets.trainer:168 RmsProp 111 loss=0.500872 err=0.500872
I 2015-05-26 07:50:06 theanets.trainer:168 RmsProp 112 loss=0.534407 err=0.534407
I 2015-05-26 07:50:17 theanets.trainer:168 RmsProp 113 loss=0.514091 err=0.514091
I 2015-05-26 07:50:28 theanets.trainer:168 RmsProp 114 loss=0.485402 err=0.485402
I 2015-05-26 07:50:39 theanets.trainer:168 RmsProp 115 loss=0.498411 err=0.498411
I 2015-05-26 07:50:50 theanets.trainer:168 RmsProp 116 loss=0.529769 err=0.529769
I 2015-05-26 07:51:02 theanets.trainer:168 RmsProp 117 loss=0.480702 err=0.480702
I 2015-05-26 07:51:13 theanets.trainer:168 RmsProp 118 loss=0.483501 err=0.483501
I 2015-05-26 07:51:24 theanets.trainer:168 RmsProp 119 loss=0.470139 err=0.470139
I 2015-05-26 07:51:35 theanets.trainer:168 RmsProp 120 loss=0.501277 err=0.501277
I 2015-05-26 07:51:36 theanets.trainer:168 validation 12 loss=1445.185303 err=1445.185303 *
I 2015-05-26 07:51:47 theanets.trainer:168 RmsProp 121 loss=0.471001 err=0.471001
I 2015-05-26 07:51:58 theanets.trainer:168 RmsProp 122 loss=0.476060 err=0.476060
I 2015-05-26 07:52:11 theanets.trainer:168 RmsProp 123 loss=0.500722 err=0.500722
I 2015-05-26 07:52:23 theanets.trainer:168 RmsProp 124 loss=0.463873 err=0.463873
I 2015-05-26 07:52:35 theanets.trainer:168 RmsProp 125 loss=0.462540 err=0.462540
I 2015-05-26 07:52:47 theanets.trainer:168 RmsProp 126 loss=0.477286 err=0.477286
I 2015-05-26 07:52:59 theanets.trainer:168 RmsProp 127 loss=0.449546 err=0.449546
I 2015-05-26 07:53:11 theanets.trainer:168 RmsProp 128 loss=0.487610 err=0.487610
I 2015-05-26 07:53:23 theanets.trainer:168 RmsProp 129 loss=0.460300 err=0.460300
I 2015-05-26 07:53:36 theanets.trainer:168 RmsProp 130 loss=0.442618 err=0.442618
I 2015-05-26 07:53:36 theanets.trainer:168 validation 13 loss=1447.148560 err=1447.148560
I 2015-05-26 07:53:49 theanets.trainer:168 RmsProp 131 loss=0.465960 err=0.465960
I 2015-05-26 07:54:01 theanets.trainer:168 RmsProp 132 loss=0.440786 err=0.440786
I 2015-05-26 07:54:13 theanets.trainer:168 RmsProp 133 loss=0.458396 err=0.458396
I 2015-05-26 07:54:26 theanets.trainer:168 RmsProp 134 loss=0.466809 err=0.466809
I 2015-05-26 07:54:38 theanets.trainer:168 RmsProp 135 loss=0.431834 err=0.431834
I 2015-05-26 07:54:50 theanets.trainer:168 RmsProp 136 loss=0.450191 err=0.450191
I 2015-05-26 07:55:02 theanets.trainer:168 RmsProp 137 loss=0.447691 err=0.447691
I 2015-05-26 07:55:15 theanets.trainer:168 RmsProp 138 loss=0.418330 err=0.418330
I 2015-05-26 07:55:27 theanets.trainer:168 RmsProp 139 loss=0.429661 err=0.429661
I 2015-05-26 07:55:39 theanets.trainer:168 RmsProp 140 loss=0.437901 err=0.437901
I 2015-05-26 07:55:40 theanets.trainer:168 validation 14 loss=1447.477417 err=1447.477417
I 2015-05-26 07:55:52 theanets.trainer:168 RmsProp 141 loss=0.420434 err=0.420434
I 2015-05-26 07:56:04 theanets.trainer:168 RmsProp 142 loss=0.416650 err=0.416650
I 2015-05-26 07:56:16 theanets.trainer:168 RmsProp 143 loss=0.424047 err=0.424047
I 2015-05-26 07:56:29 theanets.trainer:168 RmsProp 144 loss=0.420744 err=0.420744
I 2015-05-26 07:56:41 theanets.trainer:168 RmsProp 145 loss=0.412695 err=0.412695
I 2015-05-26 07:56:53 theanets.trainer:168 RmsProp 146 loss=0.421858 err=0.421858
I 2015-05-26 07:57:05 theanets.trainer:168 RmsProp 147 loss=0.403375 err=0.403375
I 2015-05-26 07:57:18 theanets.trainer:168 RmsProp 148 loss=0.409820 err=0.409820
I 2015-05-26 07:57:30 theanets.trainer:168 RmsProp 149 loss=0.436080 err=0.436080
I 2015-05-26 07:57:43 theanets.trainer:168 RmsProp 150 loss=0.417867 err=0.417867
I 2015-05-26 07:57:43 theanets.trainer:168 validation 15 loss=1441.093628 err=1441.093628 *
I 2015-05-26 07:57:55 theanets.trainer:168 RmsProp 151 loss=0.409209 err=0.409209
I 2015-05-26 07:58:07 theanets.trainer:168 RmsProp 152 loss=0.411531 err=0.411531
I 2015-05-26 07:58:19 theanets.trainer:168 RmsProp 153 loss=0.404181 err=0.404181
I 2015-05-26 07:58:30 theanets.trainer:168 RmsProp 154 loss=0.393780 err=0.393780
I 2015-05-26 07:58:42 theanets.trainer:168 RmsProp 155 loss=0.395051 err=0.395051
I 2015-05-26 07:58:53 theanets.trainer:168 RmsProp 156 loss=0.397488 err=0.397488
I 2015-05-26 07:59:05 theanets.trainer:168 RmsProp 157 loss=0.414695 err=0.414695
I 2015-05-26 07:59:16 theanets.trainer:168 RmsProp 158 loss=0.388564 err=0.388564
I 2015-05-26 07:59:28 theanets.trainer:168 RmsProp 159 loss=0.383558 err=0.383558
I 2015-05-26 07:59:40 theanets.trainer:168 RmsProp 160 loss=0.383414 err=0.383414
I 2015-05-26 07:59:40 theanets.trainer:168 validation 16 loss=1440.359619 err=1440.359619 *
I 2015-05-26 07:59:52 theanets.trainer:168 RmsProp 161 loss=0.382216 err=0.382216
I 2015-05-26 08:00:04 theanets.trainer:168 RmsProp 162 loss=0.373697 err=0.373697
I 2015-05-26 08:00:16 theanets.trainer:168 RmsProp 163 loss=0.429540 err=0.429540
I 2015-05-26 08:00:29 theanets.trainer:168 RmsProp 164 loss=0.380021 err=0.380021
I 2015-05-26 08:00:41 theanets.trainer:168 RmsProp 165 loss=0.359869 err=0.359869
I 2015-05-26 08:00:53 theanets.trainer:168 RmsProp 166 loss=0.392818 err=0.392818
I 2015-05-26 08:01:06 theanets.trainer:168 RmsProp 167 loss=0.369363 err=0.369363
I 2015-05-26 08:01:18 theanets.trainer:168 RmsProp 168 loss=0.361295 err=0.361295
I 2015-05-26 08:01:30 theanets.trainer:168 RmsProp 169 loss=0.377625 err=0.377625
I 2015-05-26 08:01:42 theanets.trainer:168 RmsProp 170 loss=0.405929 err=0.405929
I 2015-05-26 08:01:43 theanets.trainer:168 validation 17 loss=1437.141846 err=1437.141846 *
I 2015-05-26 08:01:54 theanets.trainer:168 RmsProp 171 loss=0.368810 err=0.368810
I 2015-05-26 08:02:05 theanets.trainer:168 RmsProp 172 loss=0.345940 err=0.345940
I 2015-05-26 08:02:15 theanets.trainer:168 RmsProp 173 loss=0.387310 err=0.387310
I 2015-05-26 08:02:25 theanets.trainer:168 RmsProp 174 loss=0.350797 err=0.350797
I 2015-05-26 08:02:36 theanets.trainer:168 RmsProp 175 loss=0.359487 err=0.359487
I 2015-05-26 08:02:46 theanets.trainer:168 RmsProp 176 loss=0.353653 err=0.353653
I 2015-05-26 08:02:57 theanets.trainer:168 RmsProp 177 loss=0.352517 err=0.352517
I 2015-05-26 08:03:07 theanets.trainer:168 RmsProp 178 loss=0.361005 err=0.361005
I 2015-05-26 08:03:18 theanets.trainer:168 RmsProp 179 loss=0.367047 err=0.367047
I 2015-05-26 08:03:28 theanets.trainer:168 RmsProp 180 loss=0.343034 err=0.343034
I 2015-05-26 08:03:29 theanets.trainer:168 validation 18 loss=1435.879028 err=1435.879028 *
I 2015-05-26 08:03:39 theanets.trainer:168 RmsProp 181 loss=0.375104 err=0.375104
I 2015-05-26 08:03:50 theanets.trainer:168 RmsProp 182 loss=0.357128 err=0.357128
I 2015-05-26 08:04:01 theanets.trainer:168 RmsProp 183 loss=0.342212 err=0.342212
I 2015-05-26 08:04:12 theanets.trainer:168 RmsProp 184 loss=0.357752 err=0.357752
I 2015-05-26 08:04:23 theanets.trainer:168 RmsProp 185 loss=0.336310 err=0.336310
I 2015-05-26 08:04:34 theanets.trainer:168 RmsProp 186 loss=0.334600 err=0.334600
I 2015-05-26 08:04:44 theanets.trainer:168 RmsProp 187 loss=0.352032 err=0.352032
I 2015-05-26 08:04:56 theanets.trainer:168 RmsProp 188 loss=0.333377 err=0.333377
I 2015-05-26 08:05:07 theanets.trainer:168 RmsProp 189 loss=0.341553 err=0.341553
I 2015-05-26 08:05:18 theanets.trainer:168 RmsProp 190 loss=0.341707 err=0.341707
I 2015-05-26 08:05:18 theanets.trainer:168 validation 19 loss=1432.673340 err=1432.673340 *
I 2015-05-26 08:05:29 theanets.trainer:168 RmsProp 191 loss=0.334513 err=0.334513
I 2015-05-26 08:05:40 theanets.trainer:168 RmsProp 192 loss=0.330186 err=0.330186
I 2015-05-26 08:05:51 theanets.trainer:168 RmsProp 193 loss=0.332662 err=0.332662
I 2015-05-26 08:06:01 theanets.trainer:168 RmsProp 194 loss=0.318201 err=0.318201
I 2015-05-26 08:06:12 theanets.trainer:168 RmsProp 195 loss=0.326433 err=0.326433
I 2015-05-26 08:06:23 theanets.trainer:168 RmsProp 196 loss=0.348382 err=0.348382
I 2015-05-26 08:06:35 theanets.trainer:168 RmsProp 197 loss=0.337009 err=0.337009
I 2015-05-26 08:06:46 theanets.trainer:168 RmsProp 198 loss=0.315428 err=0.315428
I 2015-05-26 08:06:57 theanets.trainer:168 RmsProp 199 loss=0.341676 err=0.341676
I 2015-05-26 08:07:08 theanets.trainer:168 RmsProp 200 loss=0.310923 err=0.310923
I 2015-05-26 08:07:09 theanets.trainer:168 validation 20 loss=1430.491577 err=1430.491577 *
I 2015-05-26 08:07:20 theanets.trainer:168 RmsProp 201 loss=0.318598 err=0.318598
I 2015-05-26 08:07:30 theanets.trainer:168 RmsProp 202 loss=0.332533 err=0.332533
I 2015-05-26 08:07:41 theanets.trainer:168 RmsProp 203 loss=0.312228 err=0.312228
I 2015-05-26 08:07:51 theanets.trainer:168 RmsProp 204 loss=0.331035 err=0.331035
I 2015-05-26 08:08:01 theanets.trainer:168 RmsProp 205 loss=0.318528 err=0.318528
I 2015-05-26 08:08:12 theanets.trainer:168 RmsProp 206 loss=0.324446 err=0.324446
I 2015-05-26 08:08:22 theanets.trainer:168 RmsProp 207 loss=0.312004 err=0.312004
I 2015-05-26 08:08:32 theanets.trainer:168 RmsProp 208 loss=0.316565 err=0.316565
I 2015-05-26 08:08:43 theanets.trainer:168 RmsProp 209 loss=0.323704 err=0.323704
I 2015-05-26 08:08:53 theanets.trainer:168 RmsProp 210 loss=0.298585 err=0.298585
I 2015-05-26 08:08:53 theanets.trainer:168 validation 21 loss=1432.922241 err=1432.922241
I 2015-05-26 08:09:04 theanets.trainer:168 RmsProp 211 loss=0.321648 err=0.321648
I 2015-05-26 08:09:14 theanets.trainer:168 RmsProp 212 loss=0.309693 err=0.309693
I 2015-05-26 08:09:25 theanets.trainer:168 RmsProp 213 loss=0.312731 err=0.312731
I 2015-05-26 08:09:37 theanets.trainer:168 RmsProp 214 loss=0.325854 err=0.325854
I 2015-05-26 08:09:48 theanets.trainer:168 RmsProp 215 loss=0.299344 err=0.299344
I 2015-05-26 08:09:59 theanets.trainer:168 RmsProp 216 loss=0.305032 err=0.305032
I 2015-05-26 08:10:10 theanets.trainer:168 RmsProp 217 loss=0.302080 err=0.302080
I 2015-05-26 08:10:21 theanets.trainer:168 RmsProp 218 loss=0.300592 err=0.300592
I 2015-05-26 08:10:33 theanets.trainer:168 RmsProp 219 loss=0.300366 err=0.300366
I 2015-05-26 08:10:44 theanets.trainer:168 RmsProp 220 loss=0.301853 err=0.301853
I 2015-05-26 08:10:44 theanets.trainer:168 validation 22 loss=1432.517944 err=1432.517944
I 2015-05-26 08:10:55 theanets.trainer:168 RmsProp 221 loss=0.298282 err=0.298282
I 2015-05-26 08:11:06 theanets.trainer:168 RmsProp 222 loss=0.295227 err=0.295227
I 2015-05-26 08:11:17 theanets.trainer:168 RmsProp 223 loss=0.291465 err=0.291465
I 2015-05-26 08:11:28 theanets.trainer:168 RmsProp 224 loss=0.292925 err=0.292925
I 2015-05-26 08:11:40 theanets.trainer:168 RmsProp 225 loss=0.312390 err=0.312390
I 2015-05-26 08:11:50 theanets.trainer:168 RmsProp 226 loss=0.289268 err=0.289268
I 2015-05-26 08:12:01 theanets.trainer:168 RmsProp 227 loss=0.316854 err=0.316854
I 2015-05-26 08:12:12 theanets.trainer:168 RmsProp 228 loss=0.297019 err=0.297019
I 2015-05-26 08:12:23 theanets.trainer:168 RmsProp 229 loss=0.275448 err=0.275448
I 2015-05-26 08:12:35 theanets.trainer:168 RmsProp 230 loss=0.298854 err=0.298854
I 2015-05-26 08:12:35 theanets.trainer:168 validation 23 loss=1429.252441 err=1429.252441 *
I 2015-05-26 08:12:46 theanets.trainer:168 RmsProp 231 loss=0.287534 err=0.287534
I 2015-05-26 08:12:57 theanets.trainer:168 RmsProp 232 loss=0.290304 err=0.290304
I 2015-05-26 08:13:09 theanets.trainer:168 RmsProp 233 loss=0.289915 err=0.289915
I 2015-05-26 08:13:20 theanets.trainer:168 RmsProp 234 loss=0.291353 err=0.291353
I 2015-05-26 08:13:32 theanets.trainer:168 RmsProp 235 loss=0.285079 err=0.285079
I 2015-05-26 08:13:43 theanets.trainer:168 RmsProp 236 loss=0.288601 err=0.288601
I 2015-05-26 08:13:54 theanets.trainer:168 RmsProp 237 loss=0.292743 err=0.292743
I 2015-05-26 08:14:05 theanets.trainer:168 RmsProp 238 loss=0.280154 err=0.280154
I 2015-05-26 08:14:15 theanets.trainer:168 RmsProp 239 loss=0.282956 err=0.282956
I 2015-05-26 08:14:27 theanets.trainer:168 RmsProp 240 loss=0.275730 err=0.275730
I 2015-05-26 08:14:27 theanets.trainer:168 validation 24 loss=1430.899658 err=1430.899658
I 2015-05-26 08:14:38 theanets.trainer:168 RmsProp 241 loss=0.278233 err=0.278233
I 2015-05-26 08:14:49 theanets.trainer:168 RmsProp 242 loss=0.272492 err=0.272492
I 2015-05-26 08:15:00 theanets.trainer:168 RmsProp 243 loss=0.269840 err=0.269840
I 2015-05-26 08:15:11 theanets.trainer:168 RmsProp 244 loss=0.270952 err=0.270952
I 2015-05-26 08:15:23 theanets.trainer:168 RmsProp 245 loss=0.284980 err=0.284980
I 2015-05-26 08:15:34 theanets.trainer:168 RmsProp 246 loss=0.268955 err=0.268955
I 2015-05-26 08:15:45 theanets.trainer:168 RmsProp 247 loss=0.277351 err=0.277351
I 2015-05-26 08:15:57 theanets.trainer:168 RmsProp 248 loss=0.266247 err=0.266247
I 2015-05-26 08:16:08 theanets.trainer:168 RmsProp 249 loss=0.307373 err=0.307373
I 2015-05-26 08:16:19 theanets.trainer:168 RmsProp 250 loss=0.266289 err=0.266289
I 2015-05-26 08:16:20 theanets.trainer:168 validation 25 loss=1426.745117 err=1426.745117 *
I 2015-05-26 08:16:31 theanets.trainer:168 RmsProp 251 loss=0.272779 err=0.272779
I 2015-05-26 08:16:42 theanets.trainer:168 RmsProp 252 loss=0.259076 err=0.259076
I 2015-05-26 08:16:53 theanets.trainer:168 RmsProp 253 loss=0.269610 err=0.269610
I 2015-05-26 08:17:04 theanets.trainer:168 RmsProp 254 loss=0.263368 err=0.263368
I 2015-05-26 08:17:15 theanets.trainer:168 RmsProp 255 loss=0.278909 err=0.278909
I 2015-05-26 08:17:26 theanets.trainer:168 RmsProp 256 loss=0.266583 err=0.266583
I 2015-05-26 08:17:37 theanets.trainer:168 RmsProp 257 loss=0.265439 err=0.265439
I 2015-05-26 08:17:48 theanets.trainer:168 RmsProp 258 loss=0.275596 err=0.275596
I 2015-05-26 08:17:58 theanets.trainer:168 RmsProp 259 loss=0.254416 err=0.254416
I 2015-05-26 08:18:10 theanets.trainer:168 RmsProp 260 loss=0.264506 err=0.264506
I 2015-05-26 08:18:10 theanets.trainer:168 validation 26 loss=1427.483643 err=1427.483643
I 2015-05-26 08:18:21 theanets.trainer:168 RmsProp 261 loss=0.255545 err=0.255545
I 2015-05-26 08:18:32 theanets.trainer:168 RmsProp 262 loss=0.256934 err=0.256934
I 2015-05-26 08:18:43 theanets.trainer:168 RmsProp 263 loss=0.270363 err=0.270363
I 2015-05-26 08:18:54 theanets.trainer:168 RmsProp 264 loss=0.243858 err=0.243858
I 2015-05-26 08:19:05 theanets.trainer:168 RmsProp 265 loss=0.251048 err=0.251048
I 2015-05-26 08:19:16 theanets.trainer:168 RmsProp 266 loss=0.255553 err=0.255553
I 2015-05-26 08:19:28 theanets.trainer:168 RmsProp 267 loss=0.267242 err=0.267242
I 2015-05-26 08:19:39 theanets.trainer:168 RmsProp 268 loss=0.249270 err=0.249270
I 2015-05-26 08:19:51 theanets.trainer:168 RmsProp 269 loss=0.250194 err=0.250194
I 2015-05-26 08:20:02 theanets.trainer:168 RmsProp 270 loss=0.272426 err=0.272426
I 2015-05-26 08:20:03 theanets.trainer:168 validation 27 loss=1427.490112 err=1427.490112
I 2015-05-26 08:20:14 theanets.trainer:168 RmsProp 271 loss=0.254722 err=0.254722
I 2015-05-26 08:20:26 theanets.trainer:168 RmsProp 272 loss=0.245532 err=0.245532
I 2015-05-26 08:20:37 theanets.trainer:168 RmsProp 273 loss=0.249995 err=0.249995
I 2015-05-26 08:20:49 theanets.trainer:168 RmsProp 274 loss=0.258342 err=0.258342
I 2015-05-26 08:21:00 theanets.trainer:168 RmsProp 275 loss=0.240124 err=0.240124
I 2015-05-26 08:21:11 theanets.trainer:168 RmsProp 276 loss=0.259081 err=0.259081
I 2015-05-26 08:21:23 theanets.trainer:168 RmsProp 277 loss=0.287097 err=0.287097
I 2015-05-26 08:21:34 theanets.trainer:168 RmsProp 278 loss=0.251034 err=0.251034
I 2015-05-26 08:21:46 theanets.trainer:168 RmsProp 279 loss=0.241156 err=0.241156
I 2015-05-26 08:21:57 theanets.trainer:168 RmsProp 280 loss=0.229461 err=0.229461
I 2015-05-26 08:21:57 theanets.trainer:168 validation 28 loss=1425.945679 err=1425.945679 *
I 2015-05-26 08:22:09 theanets.trainer:168 RmsProp 281 loss=0.245795 err=0.245795
I 2015-05-26 08:22:20 theanets.trainer:168 RmsProp 282 loss=0.256487 err=0.256487
I 2015-05-26 08:22:31 theanets.trainer:168 RmsProp 283 loss=0.247947 err=0.247947
I 2015-05-26 08:22:43 theanets.trainer:168 RmsProp 284 loss=0.244810 err=0.244810
I 2015-05-26 08:22:54 theanets.trainer:168 RmsProp 285 loss=0.245722 err=0.245722
I 2015-05-26 08:23:06 theanets.trainer:168 RmsProp 286 loss=0.242683 err=0.242683
I 2015-05-26 08:23:17 theanets.trainer:168 RmsProp 287 loss=0.240195 err=0.240195
I 2015-05-26 08:23:29 theanets.trainer:168 RmsProp 288 loss=0.249501 err=0.249501
I 2015-05-26 08:23:40 theanets.trainer:168 RmsProp 289 loss=0.227020 err=0.227020
I 2015-05-26 08:23:51 theanets.trainer:168 RmsProp 290 loss=0.253504 err=0.253504
I 2015-05-26 08:23:52 theanets.trainer:168 validation 29 loss=1422.099243 err=1422.099243 *
I 2015-05-26 08:24:03 theanets.trainer:168 RmsProp 291 loss=0.239246 err=0.239246
I 2015-05-26 08:24:14 theanets.trainer:168 RmsProp 292 loss=0.237615 err=0.237615
I 2015-05-26 08:24:26 theanets.trainer:168 RmsProp 293 loss=0.245859 err=0.245859
I 2015-05-26 08:24:37 theanets.trainer:168 RmsProp 294 loss=0.234210 err=0.234210
I 2015-05-26 08:24:49 theanets.trainer:168 RmsProp 295 loss=0.241147 err=0.241147
I 2015-05-26 08:25:00 theanets.trainer:168 RmsProp 296 loss=0.235625 err=0.235625
I 2015-05-26 08:25:11 theanets.trainer:168 RmsProp 297 loss=0.239326 err=0.239326
I 2015-05-26 08:25:22 theanets.trainer:168 RmsProp 298 loss=0.230859 err=0.230859
I 2015-05-26 08:25:34 theanets.trainer:168 RmsProp 299 loss=0.227428 err=0.227428
I 2015-05-26 08:25:46 theanets.trainer:168 RmsProp 300 loss=0.260805 err=0.260805
I 2015-05-26 08:25:46 theanets.trainer:168 validation 30 loss=1417.562012 err=1417.562012 *
I 2015-05-26 08:25:58 theanets.trainer:168 RmsProp 301 loss=0.234536 err=0.234536
I 2015-05-26 08:26:09 theanets.trainer:168 RmsProp 302 loss=0.224302 err=0.224302
I 2015-05-26 08:26:21 theanets.trainer:168 RmsProp 303 loss=0.247945 err=0.247945
I 2015-05-26 08:26:32 theanets.trainer:168 RmsProp 304 loss=0.218724 err=0.218724
I 2015-05-26 08:26:44 theanets.trainer:168 RmsProp 305 loss=0.230352 err=0.230352
I 2015-05-26 08:26:55 theanets.trainer:168 RmsProp 306 loss=0.229677 err=0.229677
I 2015-05-26 08:27:07 theanets.trainer:168 RmsProp 307 loss=0.235571 err=0.235571
I 2015-05-26 08:27:19 theanets.trainer:168 RmsProp 308 loss=0.235011 err=0.235011
I 2015-05-26 08:27:30 theanets.trainer:168 RmsProp 309 loss=0.224355 err=0.224355
I 2015-05-26 08:27:42 theanets.trainer:168 RmsProp 310 loss=0.219013 err=0.219013
I 2015-05-26 08:27:43 theanets.trainer:168 validation 31 loss=1423.942017 err=1423.942017
I 2015-05-26 08:27:54 theanets.trainer:168 RmsProp 311 loss=0.244839 err=0.244839
I 2015-05-26 08:28:05 theanets.trainer:168 RmsProp 312 loss=0.214267 err=0.214267
I 2015-05-26 08:28:17 theanets.trainer:168 RmsProp 313 loss=0.250424 err=0.250424
I 2015-05-26 08:28:28 theanets.trainer:168 RmsProp 314 loss=0.219287 err=0.219287
I 2015-05-26 08:28:39 theanets.trainer:168 RmsProp 315 loss=0.208196 err=0.208196
I 2015-05-26 08:28:50 theanets.trainer:168 RmsProp 316 loss=0.242246 err=0.242246
I 2015-05-26 08:29:02 theanets.trainer:168 RmsProp 317 loss=0.221945 err=0.221945
I 2015-05-26 08:29:13 theanets.trainer:168 RmsProp 318 loss=0.223756 err=0.223756
I 2015-05-26 08:29:25 theanets.trainer:168 RmsProp 319 loss=0.214612 err=0.214612
I 2015-05-26 08:29:36 theanets.trainer:168 RmsProp 320 loss=0.257623 err=0.257623
I 2015-05-26 08:29:37 theanets.trainer:168 validation 32 loss=1422.070557 err=1422.070557
I 2015-05-26 08:29:48 theanets.trainer:168 RmsProp 321 loss=0.210982 err=0.210982
I 2015-05-26 08:29:59 theanets.trainer:168 RmsProp 322 loss=0.223335 err=0.223335
I 2015-05-26 08:30:11 theanets.trainer:168 RmsProp 323 loss=0.243122 err=0.243122
I 2015-05-26 08:30:23 theanets.trainer:168 RmsProp 324 loss=0.218453 err=0.218453
I 2015-05-26 08:30:34 theanets.trainer:168 RmsProp 325 loss=0.210996 err=0.210996
I 2015-05-26 08:30:46 theanets.trainer:168 RmsProp 326 loss=0.219599 err=0.219599
I 2015-05-26 08:30:58 theanets.trainer:168 RmsProp 327 loss=0.228702 err=0.228702
I 2015-05-26 08:31:09 theanets.trainer:168 RmsProp 328 loss=0.206310 err=0.206310
I 2015-05-26 08:31:21 theanets.trainer:168 RmsProp 329 loss=0.218947 err=0.218947
I 2015-05-26 08:31:31 theanets.trainer:168 RmsProp 330 loss=0.214089 err=0.214089
I 2015-05-26 08:31:31 theanets.trainer:168 validation 33 loss=1420.882446 err=1420.882446
I 2015-05-26 08:31:42 theanets.trainer:168 RmsProp 331 loss=0.210669 err=0.210669
I 2015-05-26 08:31:52 theanets.trainer:168 RmsProp 332 loss=0.216879 err=0.216879
I 2015-05-26 08:32:02 theanets.trainer:168 RmsProp 333 loss=0.213832 err=0.213832
I 2015-05-26 08:32:12 theanets.trainer:168 RmsProp 334 loss=0.216774 err=0.216774
I 2015-05-26 08:32:23 theanets.trainer:168 RmsProp 335 loss=0.210766 err=0.210766
I 2015-05-26 08:32:34 theanets.trainer:168 RmsProp 336 loss=0.207965 err=0.207965
I 2015-05-26 08:32:44 theanets.trainer:168 RmsProp 337 loss=0.232710 err=0.232710
I 2015-05-26 08:32:55 theanets.trainer:168 RmsProp 338 loss=0.202931 err=0.202931
I 2015-05-26 08:33:05 theanets.trainer:168 RmsProp 339 loss=0.216247 err=0.216247
I 2015-05-26 08:33:16 theanets.trainer:168 RmsProp 340 loss=0.201262 err=0.201262
I 2015-05-26 08:33:17 theanets.trainer:168 validation 34 loss=1417.713867 err=1417.713867
I 2015-05-26 08:33:27 theanets.trainer:168 RmsProp 341 loss=0.227384 err=0.227384
I 2015-05-26 08:33:37 theanets.trainer:168 RmsProp 342 loss=0.213108 err=0.213108
I 2015-05-26 08:33:47 theanets.trainer:168 RmsProp 343 loss=0.201102 err=0.201102
I 2015-05-26 08:33:57 theanets.trainer:168 RmsProp 344 loss=0.243166 err=0.243166
I 2015-05-26 08:34:08 theanets.trainer:168 RmsProp 345 loss=0.229620 err=0.229620
I 2015-05-26 08:34:18 theanets.trainer:168 RmsProp 346 loss=0.196007 err=0.196007
I 2015-05-26 08:34:29 theanets.trainer:168 RmsProp 347 loss=0.220675 err=0.220675
I 2015-05-26 08:34:39 theanets.trainer:168 RmsProp 348 loss=0.210670 err=0.210670
I 2015-05-26 08:34:49 theanets.trainer:168 RmsProp 349 loss=0.194682 err=0.194682
I 2015-05-26 08:35:00 theanets.trainer:168 RmsProp 350 loss=0.219273 err=0.219273
I 2015-05-26 08:35:00 theanets.trainer:168 validation 35 loss=1416.371948 err=1416.371948 *
I 2015-05-26 08:35:11 theanets.trainer:168 RmsProp 351 loss=0.193917 err=0.193917
I 2015-05-26 08:35:22 theanets.trainer:168 RmsProp 352 loss=0.227163 err=0.227163
I 2015-05-26 08:35:32 theanets.trainer:168 RmsProp 353 loss=0.200677 err=0.200677
I 2015-05-26 08:35:42 theanets.trainer:168 RmsProp 354 loss=0.203223 err=0.203223
I 2015-05-26 08:35:52 theanets.trainer:168 RmsProp 355 loss=0.202665 err=0.202665
I 2015-05-26 08:36:02 theanets.trainer:168 RmsProp 356 loss=0.193581 err=0.193581
I 2015-05-26 08:36:13 theanets.trainer:168 RmsProp 357 loss=0.193207 err=0.193207
I 2015-05-26 08:36:23 theanets.trainer:168 RmsProp 358 loss=0.207785 err=0.207785
I 2015-05-26 08:36:34 theanets.trainer:168 RmsProp 359 loss=0.206707 err=0.206707
I 2015-05-26 08:36:44 theanets.trainer:168 RmsProp 360 loss=0.212411 err=0.212411
I 2015-05-26 08:36:45 theanets.trainer:168 validation 36 loss=1417.100952 err=1417.100952
I 2015-05-26 08:36:55 theanets.trainer:168 RmsProp 361 loss=0.189588 err=0.189588
I 2015-05-26 08:37:06 theanets.trainer:168 RmsProp 362 loss=0.190288 err=0.190288
I 2015-05-26 08:37:16 theanets.trainer:168 RmsProp 363 loss=0.229721 err=0.229721
I 2015-05-26 08:37:27 theanets.trainer:168 RmsProp 364 loss=0.190425 err=0.190425
I 2015-05-26 08:37:37 theanets.trainer:168 RmsProp 365 loss=0.203715 err=0.203715
I 2015-05-26 08:37:48 theanets.trainer:168 RmsProp 366 loss=0.194106 err=0.194106
I 2015-05-26 08:37:58 theanets.trainer:168 RmsProp 367 loss=0.200624 err=0.200624
I 2015-05-26 08:38:09 theanets.trainer:168 RmsProp 368 loss=0.188034 err=0.188034
I 2015-05-26 08:38:19 theanets.trainer:168 RmsProp 369 loss=0.204732 err=0.204732
I 2015-05-26 08:38:30 theanets.trainer:168 RmsProp 370 loss=0.199181 err=0.199181
I 2015-05-26 08:38:30 theanets.trainer:168 validation 37 loss=1418.775635 err=1418.775635
I 2015-05-26 08:38:41 theanets.trainer:168 RmsProp 371 loss=0.193663 err=0.193663
I 2015-05-26 08:38:51 theanets.trainer:168 RmsProp 372 loss=0.198373 err=0.198373
I 2015-05-26 08:39:01 theanets.trainer:168 RmsProp 373 loss=0.196587 err=0.196587
I 2015-05-26 08:39:12 theanets.trainer:168 RmsProp 374 loss=0.201626 err=0.201626
I 2015-05-26 08:39:22 theanets.trainer:168 RmsProp 375 loss=0.184604 err=0.184604
I 2015-05-26 08:39:32 theanets.trainer:168 RmsProp 376 loss=0.202534 err=0.202534
I 2015-05-26 08:39:43 theanets.trainer:168 RmsProp 377 loss=0.189777 err=0.189777
I 2015-05-26 08:39:54 theanets.trainer:168 RmsProp 378 loss=0.203341 err=0.203341
I 2015-05-26 08:40:05 theanets.trainer:168 RmsProp 379 loss=0.193405 err=0.193405
I 2015-05-26 08:40:15 theanets.trainer:168 RmsProp 380 loss=0.198099 err=0.198099
I 2015-05-26 08:40:16 theanets.trainer:168 validation 38 loss=1413.564819 err=1413.564819 *
I 2015-05-26 08:40:26 theanets.trainer:168 RmsProp 381 loss=0.187404 err=0.187404
I 2015-05-26 08:40:37 theanets.trainer:168 RmsProp 382 loss=0.206692 err=0.206692
I 2015-05-26 08:40:47 theanets.trainer:168 RmsProp 383 loss=0.187164 err=0.187164
I 2015-05-26 08:40:57 theanets.trainer:168 RmsProp 384 loss=0.181159 err=0.181159
I 2015-05-26 08:41:08 theanets.trainer:168 RmsProp 385 loss=0.197263 err=0.197263
I 2015-05-26 08:41:18 theanets.trainer:168 RmsProp 386 loss=0.192386 err=0.192386
I 2015-05-26 08:41:29 theanets.trainer:168 RmsProp 387 loss=0.177891 err=0.177891
I 2015-05-26 08:41:39 theanets.trainer:168 RmsProp 388 loss=0.188805 err=0.188805
I 2015-05-26 08:41:50 theanets.trainer:168 RmsProp 389 loss=0.195087 err=0.195087
I 2015-05-26 08:42:00 theanets.trainer:168 RmsProp 390 loss=0.182333 err=0.182333
I 2015-05-26 08:42:01 theanets.trainer:168 validation 39 loss=1414.119385 err=1414.119385
I 2015-05-26 08:42:11 theanets.trainer:168 RmsProp 391 loss=0.203379 err=0.203379
I 2015-05-26 08:42:21 theanets.trainer:168 RmsProp 392 loss=0.180248 err=0.180248
I 2015-05-26 08:42:32 theanets.trainer:168 RmsProp 393 loss=0.175145 err=0.175145
I 2015-05-26 08:42:42 theanets.trainer:168 RmsProp 394 loss=0.214408 err=0.214408
I 2015-05-26 08:42:52 theanets.trainer:168 RmsProp 395 loss=0.182661 err=0.182661
I 2015-05-26 08:43:02 theanets.trainer:168 RmsProp 396 loss=0.186559 err=0.186559
I 2015-05-26 08:43:13 theanets.trainer:168 RmsProp 397 loss=0.185518 err=0.185518
I 2015-05-26 08:43:23 theanets.trainer:168 RmsProp 398 loss=0.186930 err=0.186930
I 2015-05-26 08:43:34 theanets.trainer:168 RmsProp 399 loss=0.175950 err=0.175950
I 2015-05-26 08:43:44 theanets.trainer:168 RmsProp 400 loss=0.186850 err=0.186850
I 2015-05-26 08:43:45 theanets.trainer:168 validation 40 loss=1416.139038 err=1416.139038
I 2015-05-26 08:43:55 theanets.trainer:168 RmsProp 401 loss=0.184863 err=0.184863
I 2015-05-26 08:44:05 theanets.trainer:168 RmsProp 402 loss=0.193222 err=0.193222
I 2015-05-26 08:44:16 theanets.trainer:168 RmsProp 403 loss=0.184234 err=0.184234
I 2015-05-26 08:44:26 theanets.trainer:168 RmsProp 404 loss=0.175782 err=0.175782
I 2015-05-26 08:44:37 theanets.trainer:168 RmsProp 405 loss=0.216845 err=0.216845
I 2015-05-26 08:44:47 theanets.trainer:168 RmsProp 406 loss=0.182109 err=0.182109
I 2015-05-26 08:44:57 theanets.trainer:168 RmsProp 407 loss=0.175168 err=0.175168
I 2015-05-26 08:45:08 theanets.trainer:168 RmsProp 408 loss=0.182054 err=0.182054
I 2015-05-26 08:45:18 theanets.trainer:168 RmsProp 409 loss=0.189973 err=0.189973
I 2015-05-26 08:45:29 theanets.trainer:168 RmsProp 410 loss=0.176283 err=0.176283
I 2015-05-26 08:45:29 theanets.trainer:168 validation 41 loss=1412.330200 err=1412.330200 *
I 2015-05-26 08:45:39 theanets.trainer:168 RmsProp 411 loss=0.170120 err=0.170120
I 2015-05-26 08:45:50 theanets.trainer:168 RmsProp 412 loss=0.195991 err=0.195991
I 2015-05-26 08:46:00 theanets.trainer:168 RmsProp 413 loss=0.186406 err=0.186406
I 2015-05-26 08:46:10 theanets.trainer:168 RmsProp 414 loss=0.181989 err=0.181989
I 2015-05-26 08:46:20 theanets.trainer:168 RmsProp 415 loss=0.174685 err=0.174685
I 2015-05-26 08:46:30 theanets.trainer:168 RmsProp 416 loss=0.180465 err=0.180465
I 2015-05-26 08:46:40 theanets.trainer:168 RmsProp 417 loss=0.176633 err=0.176633
I 2015-05-26 08:46:50 theanets.trainer:168 RmsProp 418 loss=0.176822 err=0.176822
I 2015-05-26 08:47:00 theanets.trainer:168 RmsProp 419 loss=0.187385 err=0.187385
I 2015-05-26 08:47:10 theanets.trainer:168 RmsProp 420 loss=0.173371 err=0.173371
I 2015-05-26 08:47:11 theanets.trainer:168 validation 42 loss=1415.237305 err=1415.237305
I 2015-05-26 08:47:20 theanets.trainer:168 RmsProp 421 loss=0.168019 err=0.168019
I 2015-05-26 08:47:30 theanets.trainer:168 RmsProp 422 loss=0.212345 err=0.212345
I 2015-05-26 08:47:40 theanets.trainer:168 RmsProp 423 loss=0.201437 err=0.201437
I 2015-05-26 08:47:50 theanets.trainer:168 RmsProp 424 loss=0.172345 err=0.172345
I 2015-05-26 08:48:00 theanets.trainer:168 RmsProp 425 loss=0.161888 err=0.161888
I 2015-05-26 08:48:10 theanets.trainer:168 RmsProp 426 loss=0.173087 err=0.173087
I 2015-05-26 08:48:21 theanets.trainer:168 RmsProp 427 loss=0.180845 err=0.180845
I 2015-05-26 08:48:31 theanets.trainer:168 RmsProp 428 loss=0.179583 err=0.179583
I 2015-05-26 08:48:41 theanets.trainer:168 RmsProp 429 loss=0.175506 err=0.175506
I 2015-05-26 08:48:52 theanets.trainer:168 RmsProp 430 loss=0.168276 err=0.168276
I 2015-05-26 08:48:52 theanets.trainer:168 validation 43 loss=1412.979858 err=1412.979858
I 2015-05-26 08:49:02 theanets.trainer:168 RmsProp 431 loss=0.167749 err=0.167749
I 2015-05-26 08:49:11 theanets.trainer:168 RmsProp 432 loss=0.186908 err=0.186908
I 2015-05-26 08:49:21 theanets.trainer:168 RmsProp 433 loss=0.169505 err=0.169505
I 2015-05-26 08:49:31 theanets.trainer:168 RmsProp 434 loss=0.181991 err=0.181991
I 2015-05-26 08:49:41 theanets.trainer:168 RmsProp 435 loss=0.167793 err=0.167793
I 2015-05-26 08:49:51 theanets.trainer:168 RmsProp 436 loss=0.160286 err=0.160286
I 2015-05-26 08:50:01 theanets.trainer:168 RmsProp 437 loss=0.197777 err=0.197777
I 2015-05-26 08:50:11 theanets.trainer:168 RmsProp 438 loss=0.159872 err=0.159872
I 2015-05-26 08:50:20 theanets.trainer:168 RmsProp 439 loss=0.173294 err=0.173294
I 2015-05-26 08:50:30 theanets.trainer:168 RmsProp 440 loss=0.173264 err=0.173264
I 2015-05-26 08:50:31 theanets.trainer:168 validation 44 loss=1410.276001 err=1410.276001 *
I 2015-05-26 08:50:41 theanets.trainer:168 RmsProp 441 loss=0.160850 err=0.160850
I 2015-05-26 08:50:50 theanets.trainer:168 RmsProp 442 loss=0.178252 err=0.178252
I 2015-05-26 08:50:58 theanets.trainer:168 RmsProp 443 loss=0.162538 err=0.162538
I 2015-05-26 08:51:07 theanets.trainer:168 RmsProp 444 loss=0.165395 err=0.165395
I 2015-05-26 08:51:15 theanets.trainer:168 RmsProp 445 loss=0.182438 err=0.182438
I 2015-05-26 08:51:24 theanets.trainer:168 RmsProp 446 loss=0.170549 err=0.170549
I 2015-05-26 08:51:33 theanets.trainer:168 RmsProp 447 loss=0.173070 err=0.173070
I 2015-05-26 08:51:42 theanets.trainer:168 RmsProp 448 loss=0.158063 err=0.158063
I 2015-05-26 08:51:50 theanets.trainer:168 RmsProp 449 loss=0.184790 err=0.184790
I 2015-05-26 08:51:59 theanets.trainer:168 RmsProp 450 loss=0.164152 err=0.164152
I 2015-05-26 08:51:59 theanets.trainer:168 validation 45 loss=1410.342041 err=1410.342041
I 2015-05-26 08:52:08 theanets.trainer:168 RmsProp 451 loss=0.162795 err=0.162795
I 2015-05-26 08:52:16 theanets.trainer:168 RmsProp 452 loss=0.167123 err=0.167123
I 2015-05-26 08:52:25 theanets.trainer:168 RmsProp 453 loss=0.166134 err=0.166134
I 2015-05-26 08:52:33 theanets.trainer:168 RmsProp 454 loss=0.174930 err=0.174930
I 2015-05-26 08:52:42 theanets.trainer:168 RmsProp 455 loss=0.167511 err=0.167511
I 2015-05-26 08:52:50 theanets.trainer:168 RmsProp 456 loss=0.170819 err=0.170819
I 2015-05-26 08:52:59 theanets.trainer:168 RmsProp 457 loss=0.166997 err=0.166997
I 2015-05-26 08:53:07 theanets.trainer:168 RmsProp 458 loss=0.169212 err=0.169212
I 2015-05-26 08:53:15 theanets.trainer:168 RmsProp 459 loss=0.163971 err=0.163971
I 2015-05-26 08:53:24 theanets.trainer:168 RmsProp 460 loss=0.164365 err=0.164365
I 2015-05-26 08:53:24 theanets.trainer:168 validation 46 loss=1408.706909 err=1408.706909 *
I 2015-05-26 08:53:33 theanets.trainer:168 RmsProp 461 loss=0.165041 err=0.165041
I 2015-05-26 08:53:41 theanets.trainer:168 RmsProp 462 loss=0.163130 err=0.163130
I 2015-05-26 08:53:50 theanets.trainer:168 RmsProp 463 loss=0.158140 err=0.158140
I 2015-05-26 08:53:58 theanets.trainer:168 RmsProp 464 loss=0.189726 err=0.189726
I 2015-05-26 08:54:06 theanets.trainer:168 RmsProp 465 loss=0.162340 err=0.162340
I 2015-05-26 08:54:14 theanets.trainer:168 RmsProp 466 loss=0.158511 err=0.158511
I 2015-05-26 08:54:22 theanets.trainer:168 RmsProp 467 loss=0.166916 err=0.166916
I 2015-05-26 08:54:30 theanets.trainer:168 RmsProp 468 loss=0.157804 err=0.157804
I 2015-05-26 08:54:37 theanets.trainer:168 RmsProp 469 loss=0.165860 err=0.165860
I 2015-05-26 08:54:44 theanets.trainer:168 RmsProp 470 loss=0.163666 err=0.163666
I 2015-05-26 08:54:44 theanets.trainer:168 validation 47 loss=1411.313599 err=1411.313599
I 2015-05-26 08:54:51 theanets.trainer:168 RmsProp 471 loss=0.166677 err=0.166677
I 2015-05-26 08:54:59 theanets.trainer:168 RmsProp 472 loss=0.168675 err=0.168675
I 2015-05-26 08:55:07 theanets.trainer:168 RmsProp 473 loss=0.157419 err=0.157419
I 2015-05-26 08:55:14 theanets.trainer:168 RmsProp 474 loss=0.150835 err=0.150835
I 2015-05-26 08:55:22 theanets.trainer:168 RmsProp 475 loss=0.177408 err=0.177408
I 2015-05-26 08:55:31 theanets.trainer:168 RmsProp 476 loss=0.157007 err=0.157007
I 2015-05-26 08:55:38 theanets.trainer:168 RmsProp 477 loss=0.146895 err=0.146895
I 2015-05-26 08:55:45 theanets.trainer:168 RmsProp 478 loss=0.165969 err=0.165969
I 2015-05-26 08:55:53 theanets.trainer:168 RmsProp 479 loss=0.164465 err=0.164465
I 2015-05-26 08:56:00 theanets.trainer:168 RmsProp 480 loss=0.158505 err=0.158505
I 2015-05-26 08:56:01 theanets.trainer:168 validation 48 loss=1408.912354 err=1408.912354
I 2015-05-26 08:56:08 theanets.trainer:168 RmsProp 481 loss=0.156952 err=0.156952
I 2015-05-26 08:56:15 theanets.trainer:168 RmsProp 482 loss=0.168330 err=0.168330
I 2015-05-26 08:56:23 theanets.trainer:168 RmsProp 483 loss=0.157758 err=0.157758
I 2015-05-26 08:56:29 theanets.trainer:168 RmsProp 484 loss=0.154894 err=0.154894
I 2015-05-26 08:56:37 theanets.trainer:168 RmsProp 485 loss=0.163112 err=0.163112
I 2015-05-26 08:56:45 theanets.trainer:168 RmsProp 486 loss=0.153826 err=0.153826
I 2015-05-26 08:56:52 theanets.trainer:168 RmsProp 487 loss=0.173941 err=0.173941
I 2015-05-26 08:57:00 theanets.trainer:168 RmsProp 488 loss=0.147647 err=0.147647
I 2015-05-26 08:57:08 theanets.trainer:168 RmsProp 489 loss=0.171356 err=0.171356
I 2015-05-26 08:57:17 theanets.trainer:168 RmsProp 490 loss=0.203401 err=0.203401
I 2015-05-26 08:57:17 theanets.trainer:168 validation 49 loss=1405.421143 err=1405.421143 *
I 2015-05-26 08:57:25 theanets.trainer:168 RmsProp 491 loss=0.154584 err=0.154584
I 2015-05-26 08:57:32 theanets.trainer:168 RmsProp 492 loss=0.148985 err=0.148985
I 2015-05-26 08:57:40 theanets.trainer:168 RmsProp 493 loss=0.155367 err=0.155367
I 2015-05-26 08:57:47 theanets.trainer:168 RmsProp 494 loss=0.155436 err=0.155436
I 2015-05-26 08:57:54 theanets.trainer:168 RmsProp 495 loss=0.151101 err=0.151101
I 2015-05-26 08:58:02 theanets.trainer:168 RmsProp 496 loss=0.156169 err=0.156169
I 2015-05-26 08:58:09 theanets.trainer:168 RmsProp 497 loss=0.148142 err=0.148142
I 2015-05-26 08:58:17 theanets.trainer:168 RmsProp 498 loss=0.146209 err=0.146209
I 2015-05-26 08:58:25 theanets.trainer:168 RmsProp 499 loss=0.168181 err=0.168181
I 2015-05-26 08:58:32 theanets.trainer:168 RmsProp 500 loss=0.149037 err=0.149037
I 2015-05-26 08:58:32 theanets.trainer:168 validation 50 loss=1408.251465 err=1408.251465
I 2015-05-26 08:58:40 theanets.trainer:168 RmsProp 501 loss=0.167533 err=0.167533
I 2015-05-26 08:58:48 theanets.trainer:168 RmsProp 502 loss=0.152681 err=0.152681
I 2015-05-26 08:58:56 theanets.trainer:168 RmsProp 503 loss=0.161405 err=0.161405
I 2015-05-26 08:59:03 theanets.trainer:168 RmsProp 504 loss=0.153317 err=0.153317
I 2015-05-26 08:59:10 theanets.trainer:168 RmsProp 505 loss=0.161505 err=0.161505
I 2015-05-26 08:59:18 theanets.trainer:168 RmsProp 506 loss=0.157758 err=0.157758
I 2015-05-26 08:59:25 theanets.trainer:168 RmsProp 507 loss=0.143475 err=0.143475
I 2015-05-26 08:59:33 theanets.trainer:168 RmsProp 508 loss=0.153991 err=0.153991
I 2015-05-26 08:59:41 theanets.trainer:168 RmsProp 509 loss=0.157991 err=0.157991
I 2015-05-26 08:59:48 theanets.trainer:168 RmsProp 510 loss=0.151025 err=0.151025
I 2015-05-26 08:59:48 theanets.trainer:168 validation 51 loss=1408.563477 err=1408.563477
I 2015-05-26 08:59:55 theanets.trainer:168 RmsProp 511 loss=0.155982 err=0.155982
I 2015-05-26 09:00:03 theanets.trainer:168 RmsProp 512 loss=0.148376 err=0.148376
I 2015-05-26 09:00:09 theanets.trainer:168 RmsProp 513 loss=0.157131 err=0.157131
I 2015-05-26 09:00:17 theanets.trainer:168 RmsProp 514 loss=0.175584 err=0.175584
I 2015-05-26 09:00:25 theanets.trainer:168 RmsProp 515 loss=0.155764 err=0.155764
I 2015-05-26 09:00:32 theanets.trainer:168 RmsProp 516 loss=0.133918 err=0.133918
I 2015-05-26 09:00:39 theanets.trainer:168 RmsProp 517 loss=0.151873 err=0.151873
I 2015-05-26 09:00:47 theanets.trainer:168 RmsProp 518 loss=0.149675 err=0.149675
I 2015-05-26 09:00:55 theanets.trainer:168 RmsProp 519 loss=0.149254 err=0.149254
I 2015-05-26 09:01:04 theanets.trainer:168 RmsProp 520 loss=0.144502 err=0.144502
I 2015-05-26 09:01:04 theanets.trainer:168 validation 52 loss=1408.984741 err=1408.984741
I 2015-05-26 09:01:12 theanets.trainer:168 RmsProp 521 loss=0.137881 err=0.137881
I 2015-05-26 09:01:19 theanets.trainer:168 RmsProp 522 loss=0.167636 err=0.167636
I 2015-05-26 09:01:27 theanets.trainer:168 RmsProp 523 loss=0.143061 err=0.143061
I 2015-05-26 09:01:35 theanets.trainer:168 RmsProp 524 loss=0.153975 err=0.153975
I 2015-05-26 09:01:42 theanets.trainer:168 RmsProp 525 loss=0.141805 err=0.141805
I 2015-05-26 09:01:50 theanets.trainer:168 RmsProp 526 loss=0.159895 err=0.159895
I 2015-05-26 09:01:57 theanets.trainer:168 RmsProp 527 loss=0.145945 err=0.145945
I 2015-05-26 09:02:05 theanets.trainer:168 RmsProp 528 loss=0.146531 err=0.146531
I 2015-05-26 09:02:12 theanets.trainer:168 RmsProp 529 loss=0.139013 err=0.139013
I 2015-05-26 09:02:20 theanets.trainer:168 RmsProp 530 loss=0.153393 err=0.153393
I 2015-05-26 09:02:20 theanets.trainer:168 validation 53 loss=1407.473999 err=1407.473999
I 2015-05-26 09:02:28 theanets.trainer:168 RmsProp 531 loss=0.143470 err=0.143470
I 2015-05-26 09:02:36 theanets.trainer:168 RmsProp 532 loss=0.148024 err=0.148024
I 2015-05-26 09:02:44 theanets.trainer:168 RmsProp 533 loss=0.144386 err=0.144386
I 2015-05-26 09:02:52 theanets.trainer:168 RmsProp 534 loss=0.145547 err=0.145547
I 2015-05-26 09:03:00 theanets.trainer:168 RmsProp 535 loss=0.147502 err=0.147502
I 2015-05-26 09:03:08 theanets.trainer:168 RmsProp 536 loss=0.155901 err=0.155901
I 2015-05-26 09:03:15 theanets.trainer:168 RmsProp 537 loss=0.134788 err=0.134788
I 2015-05-26 09:03:22 theanets.trainer:168 RmsProp 538 loss=0.146704 err=0.146704
I 2015-05-26 09:03:30 theanets.trainer:168 RmsProp 539 loss=0.140788 err=0.140788
I 2015-05-26 09:03:38 theanets.trainer:168 RmsProp 540 loss=0.144799 err=0.144799
I 2015-05-26 09:03:38 theanets.trainer:168 validation 54 loss=1405.132202 err=1405.132202 *
I 2015-05-26 09:03:46 theanets.trainer:168 RmsProp 541 loss=0.140276 err=0.140276
I 2015-05-26 09:03:53 theanets.trainer:168 RmsProp 542 loss=0.156375 err=0.156375
I 2015-05-26 09:04:00 theanets.trainer:168 RmsProp 543 loss=0.139289 err=0.139289
I 2015-05-26 09:04:08 theanets.trainer:168 RmsProp 544 loss=0.137332 err=0.137332
I 2015-05-26 09:04:15 theanets.trainer:168 RmsProp 545 loss=0.133162 err=0.133162
I 2015-05-26 09:04:23 theanets.trainer:168 RmsProp 546 loss=0.170702 err=0.170702
I 2015-05-26 09:04:30 theanets.trainer:168 RmsProp 547 loss=0.139870 err=0.139870
I 2015-05-26 09:04:38 theanets.trainer:168 RmsProp 548 loss=0.136882 err=0.136882
I 2015-05-26 09:04:46 theanets.trainer:168 RmsProp 549 loss=0.151915 err=0.151915
I 2015-05-26 09:04:54 theanets.trainer:168 RmsProp 550 loss=0.131960 err=0.131960
I 2015-05-26 09:04:54 theanets.trainer:168 validation 55 loss=1401.658691 err=1401.658691 *
I 2015-05-26 09:05:02 theanets.trainer:168 RmsProp 551 loss=0.185186 err=0.185186
I 2015-05-26 09:05:10 theanets.trainer:168 RmsProp 552 loss=0.142203 err=0.142203
I 2015-05-26 09:05:18 theanets.trainer:168 RmsProp 553 loss=0.122604 err=0.122604
I 2015-05-26 09:05:26 theanets.trainer:168 RmsProp 554 loss=0.171156 err=0.171156
I 2015-05-26 09:05:34 theanets.trainer:168 RmsProp 555 loss=0.141388 err=0.141388
I 2015-05-26 09:05:41 theanets.trainer:168 RmsProp 556 loss=0.128282 err=0.128282
I 2015-05-26 09:05:48 theanets.trainer:168 RmsProp 557 loss=0.164051 err=0.164051
I 2015-05-26 09:05:56 theanets.trainer:168 RmsProp 558 loss=0.131858 err=0.131858
I 2015-05-26 09:06:04 theanets.trainer:168 RmsProp 559 loss=0.140752 err=0.140752
I 2015-05-26 09:06:10 theanets.trainer:168 RmsProp 560 loss=0.148012 err=0.148012
I 2015-05-26 09:06:11 theanets.trainer:168 validation 56 loss=1405.900757 err=1405.900757
I 2015-05-26 09:06:18 theanets.trainer:168 RmsProp 561 loss=0.147498 err=0.147498
I 2015-05-26 09:06:25 theanets.trainer:168 RmsProp 562 loss=0.131305 err=0.131305
I 2015-05-26 09:06:32 theanets.trainer:168 RmsProp 563 loss=0.159254 err=0.159254
I 2015-05-26 09:06:39 theanets.trainer:168 RmsProp 564 loss=0.149011 err=0.149011
I 2015-05-26 09:06:47 theanets.trainer:168 RmsProp 565 loss=0.138997 err=0.138997
I 2015-05-26 09:06:55 theanets.trainer:168 RmsProp 566 loss=0.137318 err=0.137318
I 2015-05-26 09:07:02 theanets.trainer:168 RmsProp 567 loss=0.141465 err=0.141465
I 2015-05-26 09:07:09 theanets.trainer:168 RmsProp 568 loss=0.138837 err=0.138837
I 2015-05-26 09:07:16 theanets.trainer:168 RmsProp 569 loss=0.144522 err=0.144522
I 2015-05-26 09:07:23 theanets.trainer:168 RmsProp 570 loss=0.127400 err=0.127400
I 2015-05-26 09:07:24 theanets.trainer:168 validation 57 loss=1402.997192 err=1402.997192
I 2015-05-26 09:07:31 theanets.trainer:168 RmsProp 571 loss=0.142360 err=0.142360
I 2015-05-26 09:07:38 theanets.trainer:168 RmsProp 572 loss=0.127266 err=0.127266
I 2015-05-26 09:07:46 theanets.trainer:168 RmsProp 573 loss=0.154657 err=0.154657
I 2015-05-26 09:07:53 theanets.trainer:168 RmsProp 574 loss=0.128368 err=0.128368
I 2015-05-26 09:08:00 theanets.trainer:168 RmsProp 575 loss=0.157796 err=0.157796
I 2015-05-26 09:08:08 theanets.trainer:168 RmsProp 576 loss=0.141709 err=0.141709
I 2015-05-26 09:08:16 theanets.trainer:168 RmsProp 577 loss=0.123728 err=0.123728
I 2015-05-26 09:08:23 theanets.trainer:168 RmsProp 578 loss=0.146474 err=0.146474
I 2015-05-26 09:08:31 theanets.trainer:168 RmsProp 579 loss=0.133404 err=0.133404
I 2015-05-26 09:08:39 theanets.trainer:168 RmsProp 580 loss=0.139746 err=0.139746
I 2015-05-26 09:08:39 theanets.trainer:168 validation 58 loss=1407.109131 err=1407.109131
I 2015-05-26 09:08:47 theanets.trainer:168 RmsProp 581 loss=0.141123 err=0.141123
I 2015-05-26 09:08:55 theanets.trainer:168 RmsProp 582 loss=0.133725 err=0.133725
I 2015-05-26 09:09:03 theanets.trainer:168 RmsProp 583 loss=0.138293 err=0.138293
I 2015-05-26 09:09:10 theanets.trainer:168 RmsProp 584 loss=0.137151 err=0.137151
I 2015-05-26 09:09:18 theanets.trainer:168 RmsProp 585 loss=0.133172 err=0.133172
I 2015-05-26 09:09:25 theanets.trainer:168 RmsProp 586 loss=0.140068 err=0.140068
I 2015-05-26 09:09:32 theanets.trainer:168 RmsProp 587 loss=0.124503 err=0.124503
I 2015-05-26 09:09:40 theanets.trainer:168 RmsProp 588 loss=0.155123 err=0.155123
I 2015-05-26 09:09:47 theanets.trainer:168 RmsProp 589 loss=0.126537 err=0.126537
I 2015-05-26 09:09:55 theanets.trainer:168 RmsProp 590 loss=0.147777 err=0.147777
I 2015-05-26 09:09:55 theanets.trainer:168 validation 59 loss=1403.289673 err=1403.289673
I 2015-05-26 09:10:02 theanets.trainer:168 RmsProp 591 loss=0.134721 err=0.134721
I 2015-05-26 09:10:10 theanets.trainer:168 RmsProp 592 loss=0.127018 err=0.127018
I 2015-05-26 09:10:17 theanets.trainer:168 RmsProp 593 loss=0.129553 err=0.129553
I 2015-05-26 09:10:24 theanets.trainer:168 RmsProp 594 loss=0.128096 err=0.128096
I 2015-05-26 09:10:31 theanets.trainer:168 RmsProp 595 loss=0.161512 err=0.161512
I 2015-05-26 09:10:38 theanets.trainer:168 RmsProp 596 loss=0.133756 err=0.133756
I 2015-05-26 09:10:46 theanets.trainer:168 RmsProp 597 loss=0.125039 err=0.125039
I 2015-05-26 09:10:53 theanets.trainer:168 RmsProp 598 loss=0.143395 err=0.143395
I 2015-05-26 09:11:01 theanets.trainer:168 RmsProp 599 loss=0.126300 err=0.126300
I 2015-05-26 09:11:08 theanets.trainer:168 RmsProp 600 loss=0.132414 err=0.132414
I 2015-05-26 09:11:09 theanets.trainer:168 validation 60 loss=1406.873291 err=1406.873291
I 2015-05-26 09:11:09 theanets.trainer:252 patience elapsed!
I 2015-05-26 09:11:09 theanets.main:237 models_deep_post_code_sep/95118-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl: saving model
I 2015-05-26 09:11:09 theanets.graph:477 models_deep_post_code_sep/95118-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl: saved model parameters

I 2015-05-27 15:54:43 theanets.layers:465 layer in: out:0 -> 1000, linear, 0 parameters
I 2015-05-27 15:54:43 theanets.layers:465 layer bdlstm1_fw: in.out:1000 -> 100, logistic, 440700 parameters
I 2015-05-27 15:54:43 theanets.layers:465 layer bdlstm1_bw: in.out:1000 -> 100, logistic, 440700 parameters
I 2015-05-27 15:54:43 theanets.layers:465 layer bdlstm1: in.out:1000 -> 200, logistic, 881400 parameters
I 2015-05-27 15:54:43 theanets.layers:465 layer bdlstm2_fw: bdlstm1.out:200 -> 50, logistic, 50350 parameters
I 2015-05-27 15:54:43 theanets.layers:465 layer bdlstm2_bw: bdlstm1.out:200 -> 50, logistic, 50350 parameters
I 2015-05-27 15:54:43 theanets.layers:465 layer bdlstm2: bdlstm1.out:200 -> 100, logistic, 100700 parameters
I 2015-05-27 15:54:43 theanets.layers:465 layer out: bdlstm2.out:100 -> 1, linear, 101 parameters
I 2015-05-27 15:54:43 theanets.graph:145 network has 982201 total parameters
models_deep_post_code_sep/95132-models-sep_san_jose_realtor_200_100.conf-1024-None-0.1-0.01.pkl
I 2015-05-27 15:54:43 theanets.dataset:133 valid: 3 mini-batches from callable
I 2015-05-27 15:54:43 theanets.dataset:133 train: 30 mini-batches from callable
I 2015-05-27 15:54:43 theanets.main:87 creating trainer <class 'theanets.trainer.RmsProp'>
I 2015-05-27 15:54:43 theanets.main:89 --batch_size = 1024
I 2015-05-27 15:54:43 theanets.main:89 --gradient_clip = 1
I 2015-05-27 15:54:43 theanets.main:89 --hidden_l1 = None
I 2015-05-27 15:54:43 theanets.main:89 --learning_rate = 0.001
I 2015-05-27 15:54:43 theanets.main:89 --train_batches = 30
I 2015-05-27 15:54:43 theanets.main:89 --valid_batches = 3
I 2015-05-27 15:54:43 theanets.main:89 --weight_l1 = 0.1
I 2015-05-27 15:54:43 theanets.main:89 --weight_l2 = 0.01
I 2015-05-27 15:54:43 theanets.trainer:134 compiling evaluation function
I 2015-05-27 15:54:54 theanets.trainer:294 compiling RmsProp learning function
I 2015-05-27 15:57:35 theanets.trainer:168 validation 0 loss=16572.923828 err=14151.737305 *
I 2015-05-27 15:58:08 theanets.trainer:168 RmsProp 1 loss=14247.883789 err=13204.981445
I 2015-05-27 15:58:43 theanets.trainer:168 RmsProp 2 loss=13304.465820 err=13037.339844
I 2015-05-27 15:59:19 theanets.trainer:168 RmsProp 3 loss=13355.498047 err=13204.159180
I 2015-05-27 15:59:55 theanets.trainer:168 RmsProp 4 loss=13282.600586 err=13142.385742
I 2015-05-27 16:00:32 theanets.trainer:168 RmsProp 5 loss=13323.302734 err=13184.634766
I 2015-05-27 16:01:10 theanets.trainer:168 RmsProp 6 loss=13385.271484 err=13246.486328
I 2015-05-27 16:01:46 theanets.trainer:168 RmsProp 7 loss=13330.737305 err=13191.389648
I 2015-05-27 16:02:23 theanets.trainer:168 RmsProp 8 loss=13349.504883 err=13211.329102
I 2015-05-27 16:03:00 theanets.trainer:168 RmsProp 9 loss=13357.961914 err=13218.936523
I 2015-05-27 16:03:36 theanets.trainer:168 RmsProp 10 loss=13279.866211 err=13140.512695
I 2015-05-27 16:03:37 theanets.trainer:168 validation 1 loss=14293.699219 err=14161.049805 *
I 2015-05-27 16:04:13 theanets.trainer:168 RmsProp 11 loss=13333.112305 err=13194.821289
I 2015-05-27 16:04:51 theanets.trainer:168 RmsProp 12 loss=13307.784180 err=13168.886719
I 2015-05-27 16:05:28 theanets.trainer:168 RmsProp 13 loss=13277.390625 err=13138.150391
I 2015-05-27 16:06:04 theanets.trainer:168 RmsProp 14 loss=13291.191406 err=13153.356445
I 2015-05-27 16:06:41 theanets.trainer:168 RmsProp 15 loss=13281.873047 err=13143.319336
I 2015-05-27 16:07:16 theanets.trainer:168 RmsProp 16 loss=13242.000977 err=13102.329102
I 2015-05-27 16:07:52 theanets.trainer:168 RmsProp 17 loss=13371.037109 err=13232.057617
I 2015-05-27 16:08:28 theanets.trainer:168 RmsProp 18 loss=13362.843750 err=13224.384766
I 2015-05-27 16:09:04 theanets.trainer:168 RmsProp 19 loss=13372.407227 err=13232.519531
I 2015-05-27 16:09:40 theanets.trainer:168 RmsProp 20 loss=13311.982422 err=13172.475586
I 2015-05-27 16:09:41 theanets.trainer:168 validation 2 loss=14303.041992 err=14163.928711
I 2015-05-27 16:10:17 theanets.trainer:168 RmsProp 21 loss=13340.287109 err=13200.438477
I 2015-05-27 16:10:53 theanets.trainer:168 RmsProp 22 loss=13217.118164 err=13077.480469
I 2015-05-27 16:11:28 theanets.trainer:168 RmsProp 23 loss=13278.419922 err=13138.283203
I 2015-05-27 16:12:05 theanets.trainer:168 RmsProp 24 loss=13389.209961 err=13248.661133
I 2015-05-27 16:12:42 theanets.trainer:168 RmsProp 25 loss=13288.974609 err=13148.571289
I 2015-05-27 16:13:20 theanets.trainer:168 RmsProp 26 loss=13308.766602 err=13168.309570
I 2015-05-27 16:13:56 theanets.trainer:168 RmsProp 27 loss=13286.187500 err=13145.185547
I 2015-05-27 16:14:32 theanets.trainer:168 RmsProp 28 loss=13430.582031 err=13288.553711
I 2015-05-27 16:15:09 theanets.trainer:168 RmsProp 29 loss=13349.630859 err=13207.883789
I 2015-05-27 16:15:46 theanets.trainer:168 RmsProp 30 loss=13391.218750 err=13249.189453
I 2015-05-27 16:15:47 theanets.trainer:168 validation 3 loss=14303.073242 err=14161.323242
I 2015-05-27 16:16:23 theanets.trainer:168 RmsProp 31 loss=13374.199219 err=13231.769531
I 2015-05-27 16:17:00 theanets.trainer:168 RmsProp 32 loss=13400.345703 err=13258.628906
I 2015-05-27 16:17:37 theanets.trainer:168 RmsProp 33 loss=13368.167969 err=13225.751953
I 2015-05-27 16:18:14 theanets.trainer:168 RmsProp 34 loss=13463.135742 err=13319.972656
I 2015-05-27 16:18:51 theanets.trainer:168 RmsProp 35 loss=13284.979492 err=13142.046875
I 2015-05-27 16:19:28 theanets.trainer:168 RmsProp 36 loss=13246.410156 err=13103.643555
I 2015-05-27 16:20:04 theanets.trainer:168 RmsProp 37 loss=13283.125977 err=13139.955078
I 2015-05-27 16:20:41 theanets.trainer:168 RmsProp 38 loss=13304.145508 err=13160.970703
I 2015-05-27 16:21:18 theanets.trainer:168 RmsProp 39 loss=13497.499023 err=13354.313477
I 2015-05-27 16:21:55 theanets.trainer:168 RmsProp 40 loss=13323.133789 err=13179.269531
I 2015-05-27 16:21:56 theanets.trainer:168 validation 4 loss=14305.249023 err=14161.424805
I 2015-05-27 16:22:33 theanets.trainer:168 RmsProp 41 loss=13332.962891 err=13188.701172
I 2015-05-27 16:23:10 theanets.trainer:168 RmsProp 42 loss=13321.550781 err=13176.812500
I 2015-05-27 16:23:46 theanets.trainer:168 RmsProp 43 loss=13217.471680 err=13073.006836
I 2015-05-27 16:24:24 theanets.trainer:168 RmsProp 44 loss=13422.598633 err=13277.912109
I 2015-05-27 16:25:01 theanets.trainer:168 RmsProp 45 loss=13490.957031 err=13346.500000
I 2015-05-27 16:25:39 theanets.trainer:168 RmsProp 46 loss=13374.379883 err=13230.236328
I 2015-05-27 16:26:16 theanets.trainer:168 RmsProp 47 loss=13308.011719 err=13163.848633
I 2015-05-27 16:26:54 theanets.trainer:168 RmsProp 48 loss=13376.678711 err=13231.469727
I 2015-05-27 16:27:33 theanets.trainer:168 RmsProp 49 loss=13333.968750 err=13189.232422
I 2015-05-27 16:28:12 theanets.trainer:168 RmsProp 50 loss=13417.141602 err=13272.305664
I 2015-05-27 16:28:13 theanets.trainer:168 validation 5 loss=14313.407227 err=14167.450195
I 2015-05-27 16:28:52 theanets.trainer:168 RmsProp 51 loss=13436.000000 err=13289.982422
I 2015-05-27 16:29:31 theanets.trainer:168 RmsProp 52 loss=13336.097656 err=13190.619141
I 2015-05-27 16:30:09 theanets.trainer:168 RmsProp 53 loss=13149.002930 err=13004.646484
I 2015-05-27 16:30:49 theanets.trainer:168 RmsProp 54 loss=13247.984375 err=13103.188477
I 2015-05-27 16:31:28 theanets.trainer:168 RmsProp 55 loss=13389.139648 err=13242.624023
I 2015-05-27 16:32:06 theanets.trainer:168 RmsProp 56 loss=13287.249023 err=13141.790039
I 2015-05-27 16:32:43 theanets.trainer:168 RmsProp 57 loss=13407.343750 err=13262.811523
I 2015-05-27 16:33:21 theanets.trainer:168 RmsProp 58 loss=13346.308594 err=13201.256836
I 2015-05-27 16:33:59 theanets.trainer:168 RmsProp 59 loss=13164.431641 err=13019.897461
I 2015-05-27 16:34:36 theanets.trainer:168 RmsProp 60 loss=13358.378906 err=13213.174805
I 2015-05-27 16:34:37 theanets.trainer:168 validation 6 loss=14300.057617 err=14159.659180
I 2015-05-27 16:34:37 theanets.trainer:252 patience elapsed!
I 2015-05-27 16:34:37 theanets.dataset:133 valid: 2 mini-batches from callable
I 2015-05-27 16:34:37 theanets.dataset:133 train: 10 mini-batches from callable
I 2015-05-27 16:34:37 theanets.main:87 creating trainer <class 'theanets.trainer.RmsProp'>
I 2015-05-27 16:34:37 theanets.main:89 --algorithms = rmsprop
I 2015-05-27 16:34:37 theanets.main:89 --batch_size = 1024
I 2015-05-27 16:34:37 theanets.main:89 --gradient_clip = 100000.0
I 2015-05-27 16:34:37 theanets.main:89 --hidden_l1 = None
I 2015-05-27 16:34:37 theanets.main:89 --learning_rate = 0.0001
I 2015-05-27 16:34:37 theanets.main:89 --train_batches = 10
I 2015-05-27 16:34:37 theanets.main:89 --valid_batches = 2
I 2015-05-27 16:34:37 theanets.main:89 --weight_l1 = 0.1
I 2015-05-27 16:34:37 theanets.main:89 --weight_l2 = 0.01
I 2015-05-27 16:34:37 theanets.trainer:134 compiling evaluation function
I 2015-05-27 16:34:47 theanets.trainer:294 compiling RmsProp learning function
I 2015-05-27 16:36:41 theanets.trainer:168 validation 0 loss=6252.852051 err=6120.202637 *
I 2015-05-27 16:36:54 theanets.trainer:168 RmsProp 1 loss=6355.391602 err=6267.309082
I 2015-05-27 16:37:07 theanets.trainer:168 RmsProp 2 loss=6326.731934 err=6262.754883
I 2015-05-27 16:37:20 theanets.trainer:168 RmsProp 3 loss=6290.049316 err=6248.484375
I 2015-05-27 16:37:33 theanets.trainer:168 RmsProp 4 loss=6297.434570 err=6269.428711
I 2015-05-27 16:37:45 theanets.trainer:168 RmsProp 5 loss=6271.340820 err=6250.966309
I 2015-05-27 16:37:57 theanets.trainer:168 RmsProp 6 loss=6289.096191 err=6271.606934
I 2015-05-27 16:38:09 theanets.trainer:168 RmsProp 7 loss=6280.753906 err=6264.533691
I 2015-05-27 16:38:19 theanets.trainer:168 RmsProp 8 loss=6273.486328 err=6258.435059
I 2015-05-27 16:38:29 theanets.trainer:168 RmsProp 9 loss=6258.565918 err=6242.937500
I 2015-05-27 16:38:40 theanets.trainer:168 RmsProp 10 loss=6273.594727 err=6258.774902
I 2015-05-27 16:38:41 theanets.trainer:168 validation 1 loss=6118.878418 err=6102.646484 *
I 2015-05-27 16:38:52 theanets.trainer:168 RmsProp 11 loss=6266.620605 err=6251.563477
I 2015-05-27 16:39:04 theanets.trainer:168 RmsProp 12 loss=6272.529785 err=6257.506836
I 2015-05-27 16:39:15 theanets.trainer:168 RmsProp 13 loss=6264.574707 err=6249.625488
I 2015-05-27 16:39:27 theanets.trainer:168 RmsProp 14 loss=6278.761230 err=6263.766602
I 2015-05-27 16:39:38 theanets.trainer:168 RmsProp 15 loss=6287.345703 err=6272.590820
I 2015-05-27 16:39:50 theanets.trainer:168 RmsProp 16 loss=6300.164062 err=6285.083008
I 2015-05-27 16:40:02 theanets.trainer:168 RmsProp 17 loss=6285.492676 err=6270.811523
I 2015-05-27 16:40:13 theanets.trainer:168 RmsProp 18 loss=6267.587891 err=6252.282715
I 2015-05-27 16:40:25 theanets.trainer:168 RmsProp 19 loss=6250.427246 err=6235.577148
I 2015-05-27 16:40:37 theanets.trainer:168 RmsProp 20 loss=6262.345703 err=6247.585938
I 2015-05-27 16:40:38 theanets.trainer:168 validation 2 loss=6119.527832 err=6103.294922
I 2015-05-27 16:40:49 theanets.trainer:168 RmsProp 21 loss=6279.916992 err=6264.592285
I 2015-05-27 16:41:01 theanets.trainer:168 RmsProp 22 loss=6263.942383 err=6249.271484
I 2015-05-27 16:41:12 theanets.trainer:168 RmsProp 23 loss=6264.302734 err=6249.112305
I 2015-05-27 16:41:24 theanets.trainer:168 RmsProp 24 loss=6287.895508 err=6273.181641
I 2015-05-27 16:41:35 theanets.trainer:168 RmsProp 25 loss=6281.662109 err=6266.343750
I 2015-05-27 16:41:47 theanets.trainer:168 RmsProp 26 loss=6286.141113 err=6271.400879
I 2015-05-27 16:41:58 theanets.trainer:168 RmsProp 27 loss=6281.388184 err=6266.133789
I 2015-05-27 16:42:10 theanets.trainer:168 RmsProp 28 loss=6274.737305 err=6259.560059
I 2015-05-27 16:42:22 theanets.trainer:168 RmsProp 29 loss=6273.159180 err=6258.182617
I 2015-05-27 16:42:34 theanets.trainer:168 RmsProp 30 loss=6276.312012 err=6260.977539
I 2015-05-27 16:42:34 theanets.trainer:168 validation 3 loss=6117.757812 err=6103.251465 *
I 2015-05-27 16:42:46 theanets.trainer:168 RmsProp 31 loss=6263.905273 err=6249.098633
I 2015-05-27 16:42:57 theanets.trainer:168 RmsProp 32 loss=6288.682617 err=6273.467773
I 2015-05-27 16:43:09 theanets.trainer:168 RmsProp 33 loss=6282.382324 err=6267.342285
I 2015-05-27 16:43:21 theanets.trainer:168 RmsProp 34 loss=6256.544434 err=6241.141602
I 2015-05-27 16:43:32 theanets.trainer:168 RmsProp 35 loss=6266.893555 err=6251.755371
I 2015-05-27 16:43:44 theanets.trainer:168 RmsProp 36 loss=6298.125977 err=6283.004395
I 2015-05-27 16:43:56 theanets.trainer:168 RmsProp 37 loss=6278.708008 err=6263.249023
I 2015-05-27 16:44:08 theanets.trainer:168 RmsProp 38 loss=6279.903809 err=6265.060547
I 2015-05-27 16:44:20 theanets.trainer:168 RmsProp 39 loss=6276.626465 err=6260.989258
I 2015-05-27 16:44:31 theanets.trainer:168 RmsProp 40 loss=6271.577637 err=6256.671875
I 2015-05-27 16:44:32 theanets.trainer:168 validation 4 loss=6118.212402 err=6103.252930
I 2015-05-27 16:44:44 theanets.trainer:168 RmsProp 41 loss=6259.225586 err=6243.821777
I 2015-05-27 16:44:55 theanets.trainer:168 RmsProp 42 loss=6282.475098 err=6267.187012
I 2015-05-27 16:45:07 theanets.trainer:168 RmsProp 43 loss=6271.409668 err=6256.162109
I 2015-05-27 16:45:19 theanets.trainer:168 RmsProp 44 loss=6271.268555 err=6255.923828
I 2015-05-27 16:45:31 theanets.trainer:168 RmsProp 45 loss=6305.296387 err=6290.139160
I 2015-05-27 16:45:43 theanets.trainer:168 RmsProp 46 loss=6265.075684 err=6249.202148
I 2015-05-27 16:45:54 theanets.trainer:168 RmsProp 47 loss=6263.585449 err=6248.489258
I 2015-05-27 16:46:06 theanets.trainer:168 RmsProp 48 loss=6302.867676 err=6287.123047
I 2015-05-27 16:46:18 theanets.trainer:168 RmsProp 49 loss=6264.719727 err=6249.351074
I 2015-05-27 16:46:30 theanets.trainer:168 RmsProp 50 loss=6295.613770 err=6280.356934
I 2015-05-27 16:46:31 theanets.trainer:168 validation 5 loss=6119.655273 err=6102.874023
I 2015-05-27 16:46:42 theanets.trainer:168 RmsProp 51 loss=6284.527832 err=6268.872070
I 2015-05-27 16:46:54 theanets.trainer:168 RmsProp 52 loss=6273.682129 err=6258.442871
I 2015-05-27 16:47:06 theanets.trainer:168 RmsProp 53 loss=6291.857422 err=6276.204590
I 2015-05-27 16:47:18 theanets.trainer:168 RmsProp 54 loss=6270.816406 err=6255.483398
I 2015-05-27 16:47:30 theanets.trainer:168 RmsProp 55 loss=6268.832031 err=6253.152832
I 2015-05-27 16:47:42 theanets.trainer:168 RmsProp 56 loss=6287.428223 err=6272.137695
I 2015-05-27 16:47:54 theanets.trainer:168 RmsProp 57 loss=6273.622070 err=6258.160156
I 2015-05-27 16:48:06 theanets.trainer:168 RmsProp 58 loss=6278.826660 err=6263.165527
I 2015-05-27 16:48:17 theanets.trainer:168 RmsProp 59 loss=6266.296875 err=6251.139160
I 2015-05-27 16:48:29 theanets.trainer:168 RmsProp 60 loss=6275.335449 err=6259.582520
I 2015-05-27 16:48:30 theanets.trainer:168 validation 6 loss=6118.879883 err=6103.029297
I 2015-05-27 16:48:41 theanets.trainer:168 RmsProp 61 loss=6277.840820 err=6262.472168
I 2015-05-27 16:48:53 theanets.trainer:168 RmsProp 62 loss=6269.047852 err=6253.508301
I 2015-05-27 16:49:04 theanets.trainer:168 RmsProp 63 loss=6276.567383 err=6260.711914
I 2015-05-27 16:49:16 theanets.trainer:168 RmsProp 64 loss=6264.789062 err=6249.134277
I 2015-05-27 16:49:28 theanets.trainer:168 RmsProp 65 loss=6266.526855 err=6250.619141
I 2015-05-27 16:49:39 theanets.trainer:168 RmsProp 66 loss=6271.323730 err=6255.916992
I 2015-05-27 16:49:51 theanets.trainer:168 RmsProp 67 loss=6285.667480 err=6269.801270
I 2015-05-27 16:50:03 theanets.trainer:168 RmsProp 68 loss=6270.452148 err=6255.031738
I 2015-05-27 16:50:15 theanets.trainer:168 RmsProp 69 loss=6265.549316 err=6249.662109
I 2015-05-27 16:50:27 theanets.trainer:168 RmsProp 70 loss=6283.258789 err=6267.333984
I 2015-05-27 16:50:27 theanets.trainer:168 validation 7 loss=6117.542480 err=6103.324219 *
I 2015-05-27 16:50:39 theanets.trainer:168 RmsProp 71 loss=6263.455566 err=6247.919922
I 2015-05-27 16:50:51 theanets.trainer:168 RmsProp 72 loss=6270.244629 err=6254.133789
I 2015-05-27 16:51:03 theanets.trainer:168 RmsProp 73 loss=6289.453613 err=6273.838867
I 2015-05-27 16:51:14 theanets.trainer:168 RmsProp 74 loss=6269.162109 err=6253.254883
I 2015-05-27 16:51:26 theanets.trainer:168 RmsProp 75 loss=6277.843750 err=6262.119141
I 2015-05-27 16:51:38 theanets.trainer:168 RmsProp 76 loss=6288.763672 err=6272.920898
I 2015-05-27 16:51:50 theanets.trainer:168 RmsProp 77 loss=6277.165039 err=6261.276367
I 2015-05-27 16:52:02 theanets.trainer:168 RmsProp 78 loss=6288.829102 err=6273.155273
I 2015-05-27 16:52:14 theanets.trainer:168 RmsProp 79 loss=6280.758789 err=6264.704102
I 2015-05-27 16:52:26 theanets.trainer:168 RmsProp 80 loss=6276.655273 err=6261.221680
I 2015-05-27 16:52:26 theanets.trainer:168 validation 8 loss=6119.784180 err=6102.957031
I 2015-05-27 16:52:38 theanets.trainer:168 RmsProp 81 loss=6297.331055 err=6281.156250
I 2015-05-27 16:52:46 theanets.trainer:168 RmsProp 82 loss=6257.139160 err=6241.395508
I 2015-05-27 16:52:54 theanets.trainer:168 RmsProp 83 loss=6297.879883 err=6281.998047
I 2015-05-27 16:53:02 theanets.trainer:168 RmsProp 84 loss=6271.919922 err=6255.994141
I 2015-05-27 16:53:10 theanets.trainer:168 RmsProp 85 loss=6282.308594 err=6266.573242
I 2015-05-27 16:53:18 theanets.trainer:168 RmsProp 86 loss=6270.340820 err=6254.438477
I 2015-05-27 16:53:27 theanets.trainer:168 RmsProp 87 loss=6267.805664 err=6252.227539
I 2015-05-27 16:53:35 theanets.trainer:168 RmsProp 88 loss=6279.375977 err=6263.336426
I 2015-05-27 16:53:43 theanets.trainer:168 RmsProp 89 loss=6267.372070 err=6251.780273
I 2015-05-27 16:53:52 theanets.trainer:168 RmsProp 90 loss=6298.312012 err=6282.457031
I 2015-05-27 16:53:52 theanets.trainer:168 validation 9 loss=6120.546387 err=6104.073730
I 2015-05-27 16:54:00 theanets.trainer:168 RmsProp 91 loss=6274.796875 err=6258.981445
I 2015-05-27 16:54:09 theanets.trainer:168 RmsProp 92 loss=6274.425781 err=6258.964844
I 2015-05-27 16:54:18 theanets.trainer:168 RmsProp 93 loss=6266.100586 err=6250.094727
I 2015-05-27 16:54:26 theanets.trainer:168 RmsProp 94 loss=6271.045898 err=6255.517090
I 2015-05-27 16:54:34 theanets.trainer:168 RmsProp 95 loss=6270.947754 err=6255.044922
I 2015-05-27 16:54:42 theanets.trainer:168 RmsProp 96 loss=6264.206055 err=6248.616211
I 2015-05-27 16:54:50 theanets.trainer:168 RmsProp 97 loss=6268.031250 err=6252.162598
I 2015-05-27 16:54:58 theanets.trainer:168 RmsProp 98 loss=6279.387207 err=6263.693359
I 2015-05-27 16:55:06 theanets.trainer:168 RmsProp 99 loss=6272.449219 err=6256.757812
I 2015-05-27 16:55:13 theanets.trainer:168 RmsProp 100 loss=6295.646973 err=6279.715820
I 2015-05-27 16:55:14 theanets.trainer:168 validation 10 loss=6118.780273 err=6103.187500
I 2015-05-27 16:55:21 theanets.trainer:168 RmsProp 101 loss=6282.945312 err=6267.464844
I 2015-05-27 16:55:29 theanets.trainer:168 RmsProp 102 loss=6251.999023 err=6236.001953
I 2015-05-27 16:55:36 theanets.trainer:168 RmsProp 103 loss=6280.229980 err=6264.641602
I 2015-05-27 16:55:44 theanets.trainer:168 RmsProp 104 loss=6271.297852 err=6255.576172
I 2015-05-27 16:55:51 theanets.trainer:168 RmsProp 105 loss=6271.770996 err=6255.921387
I 2015-05-27 16:55:58 theanets.trainer:168 RmsProp 106 loss=6274.511230 err=6258.719727
I 2015-05-27 16:56:06 theanets.trainer:168 RmsProp 107 loss=6257.831543 err=6241.704590
I 2015-05-27 16:56:14 theanets.trainer:168 RmsProp 108 loss=6272.706055 err=6257.062500
I 2015-05-27 16:56:21 theanets.trainer:168 RmsProp 109 loss=6263.671387 err=6247.589844
I 2015-05-27 16:56:28 theanets.trainer:168 RmsProp 110 loss=6259.915039 err=6244.254883
I 2015-05-27 16:56:29 theanets.trainer:168 validation 11 loss=6120.214844 err=6104.037109
I 2015-05-27 16:56:35 theanets.trainer:168 RmsProp 111 loss=6279.608887 err=6263.302246
I 2015-05-27 16:56:42 theanets.trainer:168 RmsProp 112 loss=6262.575684 err=6246.470215
I 2015-05-27 16:56:48 theanets.trainer:168 RmsProp 113 loss=6268.707520 err=6252.979004
I 2015-05-27 16:56:55 theanets.trainer:168 RmsProp 114 loss=6291.588379 err=6275.427246
I 2015-05-27 16:57:01 theanets.trainer:168 RmsProp 115 loss=6291.809570 err=6276.182617
I 2015-05-27 16:57:08 theanets.trainer:168 RmsProp 116 loss=6263.981445 err=6247.979492
I 2015-05-27 16:57:15 theanets.trainer:168 RmsProp 117 loss=6267.969727 err=6252.178711
I 2015-05-27 16:57:21 theanets.trainer:168 RmsProp 118 loss=6301.768555 err=6285.760742
I 2015-05-27 16:57:28 theanets.trainer:168 RmsProp 119 loss=6278.875977 err=6262.911133
I 2015-05-27 16:57:35 theanets.trainer:168 RmsProp 120 loss=6264.461914 err=6248.575684
I 2015-05-27 16:57:35 theanets.trainer:168 validation 12 loss=6120.295898 err=6103.491699
I 2015-05-27 16:57:35 theanets.trainer:252 patience elapsed!
I 2015-05-27 16:57:36 theanets.main:237 models_deep_post_code_sep/95132-models-sep_san_jose_realtor_200_100.conf-1024-None-0.1-0.01.pkl: saving model
I 2015-05-27 16:57:36 theanets.graph:477 models_deep_post_code_sep/95132-models-sep_san_jose_realtor_200_100.conf-1024-None-0.1-0.01.pkl: saved model parameters

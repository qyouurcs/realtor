I 2015-05-26 03:35:25 theanets.layers:465 layer in: out:0 -> 1000, linear, 0 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm1_fw: in.out:1000 -> 100, logistic, 440700 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm1_bw: in.out:1000 -> 100, logistic, 440700 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm1: in.out:1000 -> 200, logistic, 881400 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm2_fw: bdlstm1.out:200 -> 50, logistic, 50350 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm2_bw: bdlstm1.out:200 -> 50, logistic, 50350 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer bdlstm2: bdlstm1.out:200 -> 100, logistic, 100700 parameters
I 2015-05-26 03:35:25 theanets.layers:465 layer out: bdlstm2.out:100 -> 1, linear, 101 parameters
I 2015-05-26 03:35:25 theanets.graph:145 network has 982201 total parameters
models_deep_post_code_sep/95125-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl
I 2015-05-26 03:35:25 theanets.dataset:133 valid: 3 mini-batches from callable
I 2015-05-26 03:35:25 theanets.dataset:133 train: 30 mini-batches from callable
I 2015-05-26 03:35:25 theanets.main:87 creating trainer <class 'theanets.trainer.RmsProp'>
I 2015-05-26 03:35:25 theanets.main:89 --batch_size = 1024
I 2015-05-26 03:35:25 theanets.main:89 --gradient_clip = 1
I 2015-05-26 03:35:25 theanets.main:89 --hidden_l1 = None
I 2015-05-26 03:35:25 theanets.main:89 --learning_rate = 0.001
I 2015-05-26 03:35:25 theanets.main:89 --train_batches = 30
I 2015-05-26 03:35:25 theanets.main:89 --valid_batches = 3
I 2015-05-26 03:35:25 theanets.main:89 --weight_l1 = None
I 2015-05-26 03:35:25 theanets.main:89 --weight_l2 = None
I 2015-05-26 03:35:25 theanets.trainer:134 compiling evaluation function
I 2015-05-26 03:35:41 theanets.trainer:294 compiling RmsProp learning function
I 2015-05-26 03:38:56 theanets.trainer:168 validation 0 loss=14150.319336 err=14150.319336 *
I 2015-05-26 03:39:56 theanets.trainer:168 RmsProp 1 loss=13272.701172 err=13272.701172
I 2015-05-26 03:40:56 theanets.trainer:168 RmsProp 2 loss=13219.714844 err=13219.714844
I 2015-05-26 03:41:57 theanets.trainer:168 RmsProp 3 loss=13195.979492 err=13195.979492
I 2015-05-26 03:42:57 theanets.trainer:168 RmsProp 4 loss=12552.172852 err=12552.172852
I 2015-05-26 03:43:56 theanets.trainer:168 RmsProp 5 loss=11127.067383 err=11127.067383
I 2015-05-26 03:44:55 theanets.trainer:168 RmsProp 6 loss=10179.701172 err=10179.701172
I 2015-05-26 03:45:55 theanets.trainer:168 RmsProp 7 loss=9643.581055 err=9643.581055
I 2015-05-26 03:46:56 theanets.trainer:168 RmsProp 8 loss=9068.041016 err=9068.041016
I 2015-05-26 03:47:56 theanets.trainer:168 RmsProp 9 loss=8206.999023 err=8206.999023
I 2015-05-26 03:48:56 theanets.trainer:168 RmsProp 10 loss=7435.772949 err=7435.772949
I 2015-05-26 03:48:58 theanets.trainer:168 validation 1 loss=7609.294434 err=7609.294434 *
I 2015-05-26 03:49:58 theanets.trainer:168 RmsProp 11 loss=6842.879395 err=6842.879395
I 2015-05-26 03:50:59 theanets.trainer:168 RmsProp 12 loss=6238.331055 err=6238.331055
I 2015-05-26 03:52:00 theanets.trainer:168 RmsProp 13 loss=5778.923828 err=5778.923828
I 2015-05-26 03:53:01 theanets.trainer:168 RmsProp 14 loss=5384.222168 err=5384.222168
I 2015-05-26 03:54:02 theanets.trainer:168 RmsProp 15 loss=4967.701660 err=4967.701660
I 2015-05-26 03:55:03 theanets.trainer:168 RmsProp 16 loss=4667.715332 err=4667.715332
I 2015-05-26 03:56:05 theanets.trainer:168 RmsProp 17 loss=4337.935059 err=4337.935059
I 2015-05-26 03:57:06 theanets.trainer:168 RmsProp 18 loss=4050.775146 err=4050.775146
I 2015-05-26 03:58:07 theanets.trainer:168 RmsProp 19 loss=3736.315186 err=3736.315186
I 2015-05-26 03:59:08 theanets.trainer:168 RmsProp 20 loss=3483.362061 err=3483.362061
I 2015-05-26 03:59:09 theanets.trainer:168 validation 2 loss=3683.391357 err=3683.391357 *
I 2015-05-26 04:00:10 theanets.trainer:168 RmsProp 21 loss=3309.269287 err=3309.269287
I 2015-05-26 04:01:11 theanets.trainer:168 RmsProp 22 loss=3026.840576 err=3026.840576
I 2015-05-26 04:02:12 theanets.trainer:168 RmsProp 23 loss=2841.100342 err=2841.100342
I 2015-05-26 04:03:13 theanets.trainer:168 RmsProp 24 loss=2756.171387 err=2756.171387
I 2015-05-26 04:04:14 theanets.trainer:168 RmsProp 25 loss=2563.045166 err=2563.045166
I 2015-05-26 04:05:15 theanets.trainer:168 RmsProp 26 loss=2487.258057 err=2487.258057
I 2015-05-26 04:06:16 theanets.trainer:168 RmsProp 27 loss=2332.860840 err=2332.860840
I 2015-05-26 04:07:16 theanets.trainer:168 RmsProp 28 loss=2210.959961 err=2210.959961
I 2015-05-26 04:08:17 theanets.trainer:168 RmsProp 29 loss=2114.481201 err=2114.481201
I 2015-05-26 04:09:18 theanets.trainer:168 RmsProp 30 loss=2016.759399 err=2016.759399
I 2015-05-26 04:09:19 theanets.trainer:168 validation 3 loss=2985.994873 err=2985.994873 *
I 2015-05-26 04:10:20 theanets.trainer:168 RmsProp 31 loss=1936.566284 err=1936.566284
I 2015-05-26 04:11:20 theanets.trainer:168 RmsProp 32 loss=1874.621216 err=1874.621216
I 2015-05-26 04:12:20 theanets.trainer:168 RmsProp 33 loss=1783.075806 err=1783.075806
I 2015-05-26 04:13:19 theanets.trainer:168 RmsProp 34 loss=1694.612549 err=1694.612549
I 2015-05-26 04:14:17 theanets.trainer:168 RmsProp 35 loss=1618.851440 err=1618.851440
I 2015-05-26 04:15:13 theanets.trainer:168 RmsProp 36 loss=1569.222046 err=1569.222046
I 2015-05-26 04:16:09 theanets.trainer:168 RmsProp 37 loss=1478.606567 err=1478.606567
I 2015-05-26 04:17:06 theanets.trainer:168 RmsProp 38 loss=1434.237793 err=1434.237793
I 2015-05-26 04:18:02 theanets.trainer:168 RmsProp 39 loss=1395.175293 err=1395.175293
I 2015-05-26 04:18:58 theanets.trainer:168 RmsProp 40 loss=1354.302246 err=1354.302246
I 2015-05-26 04:19:00 theanets.trainer:168 validation 4 loss=2491.596436 err=2491.596436 *
I 2015-05-26 04:19:56 theanets.trainer:168 RmsProp 41 loss=1328.917114 err=1328.917114
I 2015-05-26 04:20:53 theanets.trainer:168 RmsProp 42 loss=1248.918335 err=1248.918335
I 2015-05-26 04:21:50 theanets.trainer:168 RmsProp 43 loss=1212.203613 err=1212.203613
I 2015-05-26 04:22:45 theanets.trainer:168 RmsProp 44 loss=1231.187134 err=1231.187134
I 2015-05-26 04:23:38 theanets.trainer:168 RmsProp 45 loss=1177.986206 err=1177.986206
I 2015-05-26 04:24:31 theanets.trainer:168 RmsProp 46 loss=1131.800537 err=1131.800537
I 2015-05-26 04:25:25 theanets.trainer:168 RmsProp 47 loss=1118.894409 err=1118.894409
I 2015-05-26 04:26:18 theanets.trainer:168 RmsProp 48 loss=1053.078979 err=1053.078979
I 2015-05-26 04:27:12 theanets.trainer:168 RmsProp 49 loss=987.697632 err=987.697632
I 2015-05-26 04:28:05 theanets.trainer:168 RmsProp 50 loss=968.239380 err=968.239380
I 2015-05-26 04:28:06 theanets.trainer:168 validation 5 loss=2109.163574 err=2109.163574 *
I 2015-05-26 04:28:58 theanets.trainer:168 RmsProp 51 loss=967.830811 err=967.830811
I 2015-05-26 04:29:51 theanets.trainer:168 RmsProp 52 loss=924.067444 err=924.067444
I 2015-05-26 04:30:44 theanets.trainer:168 RmsProp 53 loss=865.554749 err=865.554749
I 2015-05-26 04:31:38 theanets.trainer:168 RmsProp 54 loss=847.933350 err=847.933350
I 2015-05-26 04:32:30 theanets.trainer:168 RmsProp 55 loss=816.469788 err=816.469788
I 2015-05-26 04:33:23 theanets.trainer:168 RmsProp 56 loss=779.288757 err=779.288757
I 2015-05-26 04:34:16 theanets.trainer:168 RmsProp 57 loss=782.682739 err=782.682739
I 2015-05-26 04:35:10 theanets.trainer:168 RmsProp 58 loss=757.017273 err=757.017273
I 2015-05-26 04:36:04 theanets.trainer:168 RmsProp 59 loss=756.552429 err=756.552429
I 2015-05-26 04:36:58 theanets.trainer:168 RmsProp 60 loss=732.251160 err=732.251160
I 2015-05-26 04:36:59 theanets.trainer:168 validation 6 loss=2005.345093 err=2005.345093 *
I 2015-05-26 04:37:53 theanets.trainer:168 RmsProp 61 loss=699.246399 err=699.246399
I 2015-05-26 04:38:47 theanets.trainer:168 RmsProp 62 loss=685.731873 err=685.731873
I 2015-05-26 04:39:41 theanets.trainer:168 RmsProp 63 loss=676.592590 err=676.592590
I 2015-05-26 04:40:36 theanets.trainer:168 RmsProp 64 loss=645.025208 err=645.025208
I 2015-05-26 04:41:30 theanets.trainer:168 RmsProp 65 loss=630.968628 err=630.968628
I 2015-05-26 04:42:24 theanets.trainer:168 RmsProp 66 loss=606.391785 err=606.391785
I 2015-05-26 04:43:19 theanets.trainer:168 RmsProp 67 loss=592.539612 err=592.539612
I 2015-05-26 04:44:13 theanets.trainer:168 RmsProp 68 loss=567.325867 err=567.325867
I 2015-05-26 04:45:07 theanets.trainer:168 RmsProp 69 loss=556.749634 err=556.749634
I 2015-05-26 04:46:01 theanets.trainer:168 RmsProp 70 loss=563.766296 err=563.766296
I 2015-05-26 04:46:02 theanets.trainer:168 validation 7 loss=1916.974243 err=1916.974243 *
I 2015-05-26 04:46:56 theanets.trainer:168 RmsProp 71 loss=545.390930 err=545.390930
I 2015-05-26 04:47:49 theanets.trainer:168 RmsProp 72 loss=519.007019 err=519.007019
I 2015-05-26 04:48:43 theanets.trainer:168 RmsProp 73 loss=504.358551 err=504.358551
I 2015-05-26 04:49:37 theanets.trainer:168 RmsProp 74 loss=497.611023 err=497.611023
I 2015-05-26 04:50:31 theanets.trainer:168 RmsProp 75 loss=469.566437 err=469.566437
I 2015-05-26 04:51:25 theanets.trainer:168 RmsProp 76 loss=468.997894 err=468.997894
I 2015-05-26 04:52:19 theanets.trainer:168 RmsProp 77 loss=446.753357 err=446.753357
I 2015-05-26 04:53:14 theanets.trainer:168 RmsProp 78 loss=427.014069 err=427.014069
I 2015-05-26 04:54:08 theanets.trainer:168 RmsProp 79 loss=421.955780 err=421.955780
I 2015-05-26 04:55:01 theanets.trainer:168 RmsProp 80 loss=411.610413 err=411.610413
I 2015-05-26 04:55:02 theanets.trainer:168 validation 8 loss=1843.371948 err=1843.371948 *
I 2015-05-26 04:55:55 theanets.trainer:168 RmsProp 81 loss=413.979797 err=413.979797
I 2015-05-26 04:56:47 theanets.trainer:168 RmsProp 82 loss=407.701050 err=407.701050
I 2015-05-26 04:57:40 theanets.trainer:168 RmsProp 83 loss=385.112732 err=385.112732
I 2015-05-26 04:58:32 theanets.trainer:168 RmsProp 84 loss=373.931152 err=373.931152
I 2015-05-26 04:59:25 theanets.trainer:168 RmsProp 85 loss=363.514343 err=363.514343
I 2015-05-26 05:00:18 theanets.trainer:168 RmsProp 86 loss=358.386597 err=358.386597
I 2015-05-26 05:01:11 theanets.trainer:168 RmsProp 87 loss=343.958618 err=343.958618
I 2015-05-26 05:02:03 theanets.trainer:168 RmsProp 88 loss=337.543945 err=337.543945
I 2015-05-26 05:02:56 theanets.trainer:168 RmsProp 89 loss=337.872894 err=337.872894
I 2015-05-26 05:03:49 theanets.trainer:168 RmsProp 90 loss=334.150482 err=334.150482
I 2015-05-26 05:03:50 theanets.trainer:168 validation 9 loss=1872.822632 err=1872.822632
I 2015-05-26 05:04:43 theanets.trainer:168 RmsProp 91 loss=312.773590 err=312.773590
I 2015-05-26 05:05:36 theanets.trainer:168 RmsProp 92 loss=326.671326 err=326.671326
I 2015-05-26 05:06:30 theanets.trainer:168 RmsProp 93 loss=310.982483 err=310.982483
I 2015-05-26 05:07:23 theanets.trainer:168 RmsProp 94 loss=293.525299 err=293.525299
I 2015-05-26 05:08:14 theanets.trainer:168 RmsProp 95 loss=289.753113 err=289.753113
I 2015-05-26 05:09:05 theanets.trainer:168 RmsProp 96 loss=283.842072 err=283.842072
I 2015-05-26 05:09:56 theanets.trainer:168 RmsProp 97 loss=274.882629 err=274.882629
I 2015-05-26 05:10:47 theanets.trainer:168 RmsProp 98 loss=262.329834 err=262.329834
I 2015-05-26 05:11:36 theanets.trainer:168 RmsProp 99 loss=255.565338 err=255.565338
I 2015-05-26 05:12:26 theanets.trainer:168 RmsProp 100 loss=256.063019 err=256.063019
I 2015-05-26 05:12:27 theanets.trainer:168 validation 10 loss=1871.607788 err=1871.607788
I 2015-05-26 05:13:16 theanets.trainer:168 RmsProp 101 loss=253.900177 err=253.900177
I 2015-05-26 05:14:07 theanets.trainer:168 RmsProp 102 loss=244.612320 err=244.612320
I 2015-05-26 05:14:58 theanets.trainer:168 RmsProp 103 loss=238.994141 err=238.994141
I 2015-05-26 05:15:48 theanets.trainer:168 RmsProp 104 loss=232.592285 err=232.592285
I 2015-05-26 05:16:39 theanets.trainer:168 RmsProp 105 loss=226.351166 err=226.351166
I 2015-05-26 05:17:29 theanets.trainer:168 RmsProp 106 loss=223.643021 err=223.643021
I 2015-05-26 05:18:20 theanets.trainer:168 RmsProp 107 loss=229.278305 err=229.278305
I 2015-05-26 05:19:10 theanets.trainer:168 RmsProp 108 loss=215.132721 err=215.132721
I 2015-05-26 05:20:01 theanets.trainer:168 RmsProp 109 loss=210.619003 err=210.619003
I 2015-05-26 05:20:51 theanets.trainer:168 RmsProp 110 loss=205.965378 err=205.965378
I 2015-05-26 05:20:52 theanets.trainer:168 validation 11 loss=1706.236694 err=1706.236694 *
I 2015-05-26 05:21:43 theanets.trainer:168 RmsProp 111 loss=197.884918 err=197.884918
I 2015-05-26 05:22:34 theanets.trainer:168 RmsProp 112 loss=192.869003 err=192.869003
I 2015-05-26 05:23:25 theanets.trainer:168 RmsProp 113 loss=192.268890 err=192.268890
I 2015-05-26 05:24:16 theanets.trainer:168 RmsProp 114 loss=189.721298 err=189.721298
I 2015-05-26 05:25:06 theanets.trainer:168 RmsProp 115 loss=186.451096 err=186.451096
I 2015-05-26 05:25:57 theanets.trainer:168 RmsProp 116 loss=186.473221 err=186.473221
I 2015-05-26 05:26:47 theanets.trainer:168 RmsProp 117 loss=178.393829 err=178.393829
I 2015-05-26 05:27:37 theanets.trainer:168 RmsProp 118 loss=181.704926 err=181.704926
I 2015-05-26 05:28:28 theanets.trainer:168 RmsProp 119 loss=175.053192 err=175.053192
I 2015-05-26 05:29:19 theanets.trainer:168 RmsProp 120 loss=171.407501 err=171.407501
I 2015-05-26 05:29:20 theanets.trainer:168 validation 12 loss=1707.066040 err=1707.066040
I 2015-05-26 05:30:11 theanets.trainer:168 RmsProp 121 loss=161.189133 err=161.189133
I 2015-05-26 05:31:02 theanets.trainer:168 RmsProp 122 loss=162.762772 err=162.762772
I 2015-05-26 05:31:52 theanets.trainer:168 RmsProp 123 loss=156.400558 err=156.400558
I 2015-05-26 05:32:43 theanets.trainer:168 RmsProp 124 loss=155.559830 err=155.559830
I 2015-05-26 05:33:34 theanets.trainer:168 RmsProp 125 loss=151.792984 err=151.792984
I 2015-05-26 05:34:25 theanets.trainer:168 RmsProp 126 loss=146.206131 err=146.206131
I 2015-05-26 05:35:15 theanets.trainer:168 RmsProp 127 loss=143.731934 err=143.731934
I 2015-05-26 05:36:06 theanets.trainer:168 RmsProp 128 loss=144.258392 err=144.258392
I 2015-05-26 05:36:57 theanets.trainer:168 RmsProp 129 loss=140.382675 err=140.382675
I 2015-05-26 05:37:46 theanets.trainer:168 RmsProp 130 loss=135.471756 err=135.471756
I 2015-05-26 05:37:47 theanets.trainer:168 validation 13 loss=1677.846558 err=1677.846558 *
I 2015-05-26 05:38:36 theanets.trainer:168 RmsProp 131 loss=130.186005 err=130.186005
I 2015-05-26 05:39:24 theanets.trainer:168 RmsProp 132 loss=129.807785 err=129.807785
I 2015-05-26 05:40:12 theanets.trainer:168 RmsProp 133 loss=132.529572 err=132.529572
I 2015-05-26 05:41:01 theanets.trainer:168 RmsProp 134 loss=128.794495 err=128.794495
I 2015-05-26 05:41:49 theanets.trainer:168 RmsProp 135 loss=120.469131 err=120.469131
I 2015-05-26 05:42:38 theanets.trainer:168 RmsProp 136 loss=117.069633 err=117.069633
I 2015-05-26 05:43:28 theanets.trainer:168 RmsProp 137 loss=110.765953 err=110.765953
I 2015-05-26 05:44:17 theanets.trainer:168 RmsProp 138 loss=114.081718 err=114.081718
I 2015-05-26 05:45:06 theanets.trainer:168 RmsProp 139 loss=114.034950 err=114.034950
I 2015-05-26 05:45:56 theanets.trainer:168 RmsProp 140 loss=107.728767 err=107.728767
I 2015-05-26 05:45:57 theanets.trainer:168 validation 14 loss=1690.057495 err=1690.057495
I 2015-05-26 05:46:46 theanets.trainer:168 RmsProp 141 loss=103.280869 err=103.280869
I 2015-05-26 05:47:36 theanets.trainer:168 RmsProp 142 loss=103.538200 err=103.538200
I 2015-05-26 05:48:26 theanets.trainer:168 RmsProp 143 loss=102.199875 err=102.199875
I 2015-05-26 05:49:15 theanets.trainer:168 RmsProp 144 loss=99.505867 err=99.505867
I 2015-05-26 05:50:05 theanets.trainer:168 RmsProp 145 loss=99.555779 err=99.555779
I 2015-05-26 05:50:55 theanets.trainer:168 RmsProp 146 loss=95.726494 err=95.726494
I 2015-05-26 05:51:44 theanets.trainer:168 RmsProp 147 loss=95.587158 err=95.587158
I 2015-05-26 05:52:33 theanets.trainer:168 RmsProp 148 loss=93.557701 err=93.557701
I 2015-05-26 05:53:22 theanets.trainer:168 RmsProp 149 loss=88.882233 err=88.882233
I 2015-05-26 05:54:12 theanets.trainer:168 RmsProp 150 loss=88.337578 err=88.337578
I 2015-05-26 05:54:13 theanets.trainer:168 validation 15 loss=1631.172241 err=1631.172241 *
I 2015-05-26 05:55:02 theanets.trainer:168 RmsProp 151 loss=85.262810 err=85.262810
I 2015-05-26 05:55:52 theanets.trainer:168 RmsProp 152 loss=88.247696 err=88.247696
I 2015-05-26 05:56:41 theanets.trainer:168 RmsProp 153 loss=83.612808 err=83.612808
I 2015-05-26 05:57:31 theanets.trainer:168 RmsProp 154 loss=80.870857 err=80.870857
I 2015-05-26 05:58:20 theanets.trainer:168 RmsProp 155 loss=78.772263 err=78.772263
I 2015-05-26 05:59:09 theanets.trainer:168 RmsProp 156 loss=79.284645 err=79.284645
I 2015-05-26 05:59:58 theanets.trainer:168 RmsProp 157 loss=75.243736 err=75.243736
I 2015-05-26 06:00:48 theanets.trainer:168 RmsProp 158 loss=72.298882 err=72.298882
I 2015-05-26 06:01:37 theanets.trainer:168 RmsProp 159 loss=71.067757 err=71.067757
I 2015-05-26 06:02:27 theanets.trainer:168 RmsProp 160 loss=70.878151 err=70.878151
I 2015-05-26 06:02:28 theanets.trainer:168 validation 16 loss=1620.226929 err=1620.226929 *
I 2015-05-26 06:03:17 theanets.trainer:168 RmsProp 161 loss=68.953293 err=68.953293
I 2015-05-26 06:04:07 theanets.trainer:168 RmsProp 162 loss=66.406441 err=66.406441
I 2015-05-26 06:04:56 theanets.trainer:168 RmsProp 163 loss=67.750336 err=67.750336
I 2015-05-26 06:05:46 theanets.trainer:168 RmsProp 164 loss=67.398613 err=67.398613
I 2015-05-26 06:06:35 theanets.trainer:168 RmsProp 165 loss=63.540562 err=63.540562
I 2015-05-26 06:07:22 theanets.trainer:168 RmsProp 166 loss=62.101624 err=62.101624
I 2015-05-26 06:08:10 theanets.trainer:168 RmsProp 167 loss=60.720692 err=60.720692
I 2015-05-26 06:08:59 theanets.trainer:168 RmsProp 168 loss=58.035313 err=58.035313
I 2015-05-26 06:09:48 theanets.trainer:168 RmsProp 169 loss=56.415852 err=56.415852
I 2015-05-26 06:10:37 theanets.trainer:168 RmsProp 170 loss=61.688484 err=61.688484
I 2015-05-26 06:10:38 theanets.trainer:168 validation 17 loss=1552.897827 err=1552.897827 *
I 2015-05-26 06:11:27 theanets.trainer:168 RmsProp 171 loss=57.751724 err=57.751724
I 2015-05-26 06:12:17 theanets.trainer:168 RmsProp 172 loss=54.997353 err=54.997353
I 2015-05-26 06:13:07 theanets.trainer:168 RmsProp 173 loss=54.458118 err=54.458118
I 2015-05-26 06:13:57 theanets.trainer:168 RmsProp 174 loss=54.479725 err=54.479725
I 2015-05-26 06:14:46 theanets.trainer:168 RmsProp 175 loss=50.828457 err=50.828457
I 2015-05-26 06:15:36 theanets.trainer:168 RmsProp 176 loss=50.291477 err=50.291477
I 2015-05-26 06:16:25 theanets.trainer:168 RmsProp 177 loss=52.171280 err=52.171280
I 2015-05-26 06:17:15 theanets.trainer:168 RmsProp 178 loss=49.126865 err=49.126865
I 2015-05-26 06:18:06 theanets.trainer:168 RmsProp 179 loss=48.047390 err=48.047390
I 2015-05-26 06:18:55 theanets.trainer:168 RmsProp 180 loss=47.052921 err=47.052921
I 2015-05-26 06:18:57 theanets.trainer:168 validation 18 loss=1561.418945 err=1561.418945
I 2015-05-26 06:19:45 theanets.trainer:168 RmsProp 181 loss=45.813858 err=45.813858
I 2015-05-26 06:20:32 theanets.trainer:168 RmsProp 182 loss=44.314518 err=44.314518
I 2015-05-26 06:21:20 theanets.trainer:168 RmsProp 183 loss=44.615219 err=44.615219
I 2015-05-26 06:22:10 theanets.trainer:168 RmsProp 184 loss=42.173584 err=42.173584
I 2015-05-26 06:22:59 theanets.trainer:168 RmsProp 185 loss=46.205250 err=46.205250
I 2015-05-26 06:23:48 theanets.trainer:168 RmsProp 186 loss=43.497402 err=43.497402
I 2015-05-26 06:24:36 theanets.trainer:168 RmsProp 187 loss=41.236046 err=41.236046
I 2015-05-26 06:25:25 theanets.trainer:168 RmsProp 188 loss=40.358608 err=40.358608
I 2015-05-26 06:26:15 theanets.trainer:168 RmsProp 189 loss=40.012043 err=40.012043
I 2015-05-26 06:27:04 theanets.trainer:168 RmsProp 190 loss=37.865601 err=37.865601
I 2015-05-26 06:27:05 theanets.trainer:168 validation 19 loss=1538.019409 err=1538.019409 *
I 2015-05-26 06:27:54 theanets.trainer:168 RmsProp 191 loss=36.414295 err=36.414295
I 2015-05-26 06:28:44 theanets.trainer:168 RmsProp 192 loss=40.152203 err=40.152203
I 2015-05-26 06:29:33 theanets.trainer:168 RmsProp 193 loss=37.387714 err=37.387714
I 2015-05-26 06:30:23 theanets.trainer:168 RmsProp 194 loss=35.262436 err=35.262436
I 2015-05-26 06:31:12 theanets.trainer:168 RmsProp 195 loss=35.087090 err=35.087090
I 2015-05-26 06:32:02 theanets.trainer:168 RmsProp 196 loss=34.253826 err=34.253826
I 2015-05-26 06:32:52 theanets.trainer:168 RmsProp 197 loss=33.567036 err=33.567036
I 2015-05-26 06:33:42 theanets.trainer:168 RmsProp 198 loss=32.734444 err=32.734444
I 2015-05-26 06:34:30 theanets.trainer:168 RmsProp 199 loss=34.254143 err=34.254143
I 2015-05-26 06:35:16 theanets.trainer:168 RmsProp 200 loss=32.978130 err=32.978130
I 2015-05-26 06:35:17 theanets.trainer:168 validation 20 loss=1489.429565 err=1489.429565 *
I 2015-05-26 06:36:03 theanets.trainer:168 RmsProp 201 loss=31.584465 err=31.584465
I 2015-05-26 06:36:48 theanets.trainer:168 RmsProp 202 loss=31.410831 err=31.410831
I 2015-05-26 06:37:35 theanets.trainer:168 RmsProp 203 loss=32.463341 err=32.463341
I 2015-05-26 06:38:21 theanets.trainer:168 RmsProp 204 loss=28.838194 err=28.838194
I 2015-05-26 06:39:08 theanets.trainer:168 RmsProp 205 loss=30.462259 err=30.462259
I 2015-05-26 06:39:55 theanets.trainer:168 RmsProp 206 loss=30.954576 err=30.954576
I 2015-05-26 06:40:40 theanets.trainer:168 RmsProp 207 loss=28.761898 err=28.761898
I 2015-05-26 06:41:26 theanets.trainer:168 RmsProp 208 loss=28.029226 err=28.029226
I 2015-05-26 06:42:11 theanets.trainer:168 RmsProp 209 loss=27.083996 err=27.083996
I 2015-05-26 06:42:56 theanets.trainer:168 RmsProp 210 loss=26.544956 err=26.544956
I 2015-05-26 06:42:57 theanets.trainer:168 validation 21 loss=1444.530884 err=1444.530884 *
I 2015-05-26 06:43:41 theanets.trainer:168 RmsProp 211 loss=26.437923 err=26.437923
I 2015-05-26 06:44:26 theanets.trainer:168 RmsProp 212 loss=25.805763 err=25.805763
I 2015-05-26 06:45:11 theanets.trainer:168 RmsProp 213 loss=25.831951 err=25.831951
I 2015-05-26 06:45:56 theanets.trainer:168 RmsProp 214 loss=25.375912 err=25.375912
I 2015-05-26 06:46:42 theanets.trainer:168 RmsProp 215 loss=27.619755 err=27.619755
I 2015-05-26 06:47:27 theanets.trainer:168 RmsProp 216 loss=26.280788 err=26.280788
I 2015-05-26 06:48:12 theanets.trainer:168 RmsProp 217 loss=24.240271 err=24.240271
I 2015-05-26 06:48:56 theanets.trainer:168 RmsProp 218 loss=24.054365 err=24.054365
I 2015-05-26 06:49:41 theanets.trainer:168 RmsProp 219 loss=23.501257 err=23.501257
I 2015-05-26 06:50:26 theanets.trainer:168 RmsProp 220 loss=22.534876 err=22.534876
I 2015-05-26 06:50:27 theanets.trainer:168 validation 22 loss=1416.480591 err=1416.480591 *
I 2015-05-26 06:51:12 theanets.trainer:168 RmsProp 221 loss=22.041887 err=22.041887
I 2015-05-26 06:51:57 theanets.trainer:168 RmsProp 222 loss=20.872791 err=20.872791
I 2015-05-26 06:52:42 theanets.trainer:168 RmsProp 223 loss=20.486458 err=20.486458
I 2015-05-26 06:53:27 theanets.trainer:168 RmsProp 224 loss=21.764851 err=21.764851
I 2015-05-26 06:54:11 theanets.trainer:168 RmsProp 225 loss=26.296936 err=26.296936
I 2015-05-26 06:54:56 theanets.trainer:168 RmsProp 226 loss=23.297771 err=23.297771
I 2015-05-26 06:55:42 theanets.trainer:168 RmsProp 227 loss=21.020544 err=21.020544
I 2015-05-26 06:56:27 theanets.trainer:168 RmsProp 228 loss=20.254732 err=20.254732
I 2015-05-26 06:57:12 theanets.trainer:168 RmsProp 229 loss=19.954578 err=19.954578
I 2015-05-26 06:57:55 theanets.trainer:168 RmsProp 230 loss=19.699900 err=19.699900
I 2015-05-26 06:57:56 theanets.trainer:168 validation 23 loss=1422.845215 err=1422.845215
I 2015-05-26 06:58:37 theanets.trainer:168 RmsProp 231 loss=19.204964 err=19.204964
I 2015-05-26 06:59:18 theanets.trainer:168 RmsProp 232 loss=19.540224 err=19.540224
I 2015-05-26 06:59:58 theanets.trainer:168 RmsProp 233 loss=19.204588 err=19.204588
I 2015-05-26 07:00:39 theanets.trainer:168 RmsProp 234 loss=18.391056 err=18.391056
I 2015-05-26 07:01:20 theanets.trainer:168 RmsProp 235 loss=18.095808 err=18.095808
I 2015-05-26 07:02:01 theanets.trainer:168 RmsProp 236 loss=17.878822 err=17.878822
I 2015-05-26 07:02:42 theanets.trainer:168 RmsProp 237 loss=17.760414 err=17.760414
I 2015-05-26 07:03:24 theanets.trainer:168 RmsProp 238 loss=17.424612 err=17.424612
I 2015-05-26 07:04:05 theanets.trainer:168 RmsProp 239 loss=17.361115 err=17.361115
I 2015-05-26 07:04:46 theanets.trainer:168 RmsProp 240 loss=17.052458 err=17.052458
I 2015-05-26 07:04:47 theanets.trainer:168 validation 24 loss=1448.401733 err=1448.401733
I 2015-05-26 07:05:26 theanets.trainer:168 RmsProp 241 loss=16.192089 err=16.192089
I 2015-05-26 07:06:05 theanets.trainer:168 RmsProp 242 loss=16.106798 err=16.106798
I 2015-05-26 07:06:45 theanets.trainer:168 RmsProp 243 loss=15.969669 err=15.969669
I 2015-05-26 07:07:27 theanets.trainer:168 RmsProp 244 loss=16.632189 err=16.632189
I 2015-05-26 07:08:08 theanets.trainer:168 RmsProp 245 loss=15.983459 err=15.983459
I 2015-05-26 07:08:49 theanets.trainer:168 RmsProp 246 loss=15.567640 err=15.567640
I 2015-05-26 07:09:30 theanets.trainer:168 RmsProp 247 loss=14.860350 err=14.860350
I 2015-05-26 07:10:11 theanets.trainer:168 RmsProp 248 loss=14.373670 err=14.373670
I 2015-05-26 07:10:52 theanets.trainer:168 RmsProp 249 loss=11.363150 err=11.363150
I 2015-05-26 07:11:34 theanets.trainer:168 RmsProp 250 loss=11.789504 err=11.789504
I 2015-05-26 07:11:35 theanets.trainer:168 validation 25 loss=1399.406860 err=1399.406860 *
I 2015-05-26 07:12:14 theanets.trainer:168 RmsProp 251 loss=15.405431 err=15.405431
I 2015-05-26 07:12:52 theanets.trainer:168 RmsProp 252 loss=18.179960 err=18.179960
I 2015-05-26 07:13:31 theanets.trainer:168 RmsProp 253 loss=14.075882 err=14.075882
I 2015-05-26 07:14:11 theanets.trainer:168 RmsProp 254 loss=13.023569 err=13.023569
I 2015-05-26 07:14:52 theanets.trainer:168 RmsProp 255 loss=12.179574 err=12.179574
I 2015-05-26 07:15:33 theanets.trainer:168 RmsProp 256 loss=11.730604 err=11.730604
I 2015-05-26 07:16:14 theanets.trainer:168 RmsProp 257 loss=11.351142 err=11.351142
I 2015-05-26 07:16:54 theanets.trainer:168 RmsProp 258 loss=11.467655 err=11.467655
I 2015-05-26 07:17:35 theanets.trainer:168 RmsProp 259 loss=12.836437 err=12.836437
I 2015-05-26 07:18:16 theanets.trainer:168 RmsProp 260 loss=12.206480 err=12.206480
I 2015-05-26 07:18:16 theanets.trainer:168 validation 26 loss=1379.703491 err=1379.703491 *
I 2015-05-26 07:18:57 theanets.trainer:168 RmsProp 261 loss=11.072238 err=11.072238
I 2015-05-26 07:19:38 theanets.trainer:168 RmsProp 262 loss=10.812082 err=10.812082
I 2015-05-26 07:20:19 theanets.trainer:168 RmsProp 263 loss=11.813773 err=11.813773
I 2015-05-26 07:21:00 theanets.trainer:168 RmsProp 264 loss=10.933585 err=10.933585
I 2015-05-26 07:21:41 theanets.trainer:168 RmsProp 265 loss=11.969409 err=11.969409
I 2015-05-26 07:22:22 theanets.trainer:168 RmsProp 266 loss=10.194565 err=10.194565
I 2015-05-26 07:23:02 theanets.trainer:168 RmsProp 267 loss=9.459597 err=9.459597
I 2015-05-26 07:23:44 theanets.trainer:168 RmsProp 268 loss=8.229349 err=8.229349
I 2015-05-26 07:24:26 theanets.trainer:168 RmsProp 269 loss=7.880722 err=7.880722
I 2015-05-26 07:25:06 theanets.trainer:168 RmsProp 270 loss=7.116664 err=7.116664
I 2015-05-26 07:25:07 theanets.trainer:168 validation 27 loss=1358.611206 err=1358.611206 *
I 2015-05-26 07:25:45 theanets.trainer:168 RmsProp 271 loss=7.644477 err=7.644477
I 2015-05-26 07:26:24 theanets.trainer:168 RmsProp 272 loss=6.977289 err=6.977289
I 2015-05-26 07:27:02 theanets.trainer:168 RmsProp 273 loss=6.518021 err=6.518021
I 2015-05-26 07:27:40 theanets.trainer:168 RmsProp 274 loss=6.451150 err=6.451150
I 2015-05-26 07:28:17 theanets.trainer:168 RmsProp 275 loss=7.349886 err=7.349886
I 2015-05-26 07:28:55 theanets.trainer:168 RmsProp 276 loss=7.312091 err=7.312091
I 2015-05-26 07:29:33 theanets.trainer:168 RmsProp 277 loss=7.981524 err=7.981524
I 2015-05-26 07:30:11 theanets.trainer:168 RmsProp 278 loss=8.119259 err=8.119259
I 2015-05-26 07:30:48 theanets.trainer:168 RmsProp 279 loss=7.252744 err=7.252744
I 2015-05-26 07:31:25 theanets.trainer:168 RmsProp 280 loss=7.392756 err=7.392756
I 2015-05-26 07:31:26 theanets.trainer:168 validation 28 loss=1334.784546 err=1334.784546 *
I 2015-05-26 07:32:03 theanets.trainer:168 RmsProp 281 loss=6.560280 err=6.560280
I 2015-05-26 07:32:38 theanets.trainer:168 RmsProp 282 loss=6.030873 err=6.030873
I 2015-05-26 07:33:13 theanets.trainer:168 RmsProp 283 loss=8.174388 err=8.174388
I 2015-05-26 07:33:51 theanets.trainer:168 RmsProp 284 loss=7.031564 err=7.031564
I 2015-05-26 07:34:28 theanets.trainer:168 RmsProp 285 loss=6.688529 err=6.688529
I 2015-05-26 07:35:05 theanets.trainer:168 RmsProp 286 loss=6.430469 err=6.430469
I 2015-05-26 07:35:42 theanets.trainer:168 RmsProp 287 loss=7.575509 err=7.575509
I 2015-05-26 07:36:20 theanets.trainer:168 RmsProp 288 loss=10.332448 err=10.332448
I 2015-05-26 07:36:57 theanets.trainer:168 RmsProp 289 loss=9.428143 err=9.428143
I 2015-05-26 07:37:34 theanets.trainer:168 RmsProp 290 loss=7.975104 err=7.975104
I 2015-05-26 07:37:35 theanets.trainer:168 validation 29 loss=1341.362305 err=1341.362305
I 2015-05-26 07:38:13 theanets.trainer:168 RmsProp 291 loss=7.601442 err=7.601442
I 2015-05-26 07:38:50 theanets.trainer:168 RmsProp 292 loss=8.932584 err=8.932584
I 2015-05-26 07:39:28 theanets.trainer:168 RmsProp 293 loss=8.458421 err=8.458421
I 2015-05-26 07:40:06 theanets.trainer:168 RmsProp 294 loss=9.267422 err=9.267422
I 2015-05-26 07:40:45 theanets.trainer:168 RmsProp 295 loss=8.902023 err=8.902023
I 2015-05-26 07:41:24 theanets.trainer:168 RmsProp 296 loss=8.618429 err=8.618429
I 2015-05-26 07:42:02 theanets.trainer:168 RmsProp 297 loss=8.404125 err=8.404125
I 2015-05-26 07:42:40 theanets.trainer:168 RmsProp 298 loss=9.639567 err=9.639567
I 2015-05-26 07:43:17 theanets.trainer:168 RmsProp 299 loss=9.241277 err=9.241277
I 2015-05-26 07:43:54 theanets.trainer:168 RmsProp 300 loss=8.389720 err=8.389720
I 2015-05-26 07:43:55 theanets.trainer:168 validation 30 loss=1416.789062 err=1416.789062
I 2015-05-26 07:44:32 theanets.trainer:168 RmsProp 301 loss=8.436994 err=8.436994
I 2015-05-26 07:45:08 theanets.trainer:168 RmsProp 302 loss=8.734233 err=8.734233
I 2015-05-26 07:45:45 theanets.trainer:168 RmsProp 303 loss=7.450040 err=7.450040
I 2015-05-26 07:46:21 theanets.trainer:168 RmsProp 304 loss=8.347252 err=8.347252
I 2015-05-26 07:46:57 theanets.trainer:168 RmsProp 305 loss=7.496360 err=7.496360
I 2015-05-26 07:47:34 theanets.trainer:168 RmsProp 306 loss=7.273889 err=7.273889
I 2015-05-26 07:48:11 theanets.trainer:168 RmsProp 307 loss=7.616987 err=7.616987
I 2015-05-26 07:48:47 theanets.trainer:168 RmsProp 308 loss=5.880319 err=5.880319
I 2015-05-26 07:49:23 theanets.trainer:168 RmsProp 309 loss=5.360874 err=5.360874
I 2015-05-26 07:49:59 theanets.trainer:168 RmsProp 310 loss=5.720308 err=5.720308
I 2015-05-26 07:50:00 theanets.trainer:168 validation 31 loss=1390.053101 err=1390.053101
I 2015-05-26 07:50:34 theanets.trainer:168 RmsProp 311 loss=5.192152 err=5.192152
I 2015-05-26 07:51:08 theanets.trainer:168 RmsProp 312 loss=5.197737 err=5.197737
I 2015-05-26 07:51:41 theanets.trainer:168 RmsProp 313 loss=6.070500 err=6.070500
I 2015-05-26 07:52:16 theanets.trainer:168 RmsProp 314 loss=5.610105 err=5.610105
I 2015-05-26 07:52:53 theanets.trainer:168 RmsProp 315 loss=5.616098 err=5.616098
I 2015-05-26 07:53:30 theanets.trainer:168 RmsProp 316 loss=5.474557 err=5.474557
I 2015-05-26 07:54:07 theanets.trainer:168 RmsProp 317 loss=5.256089 err=5.256089
I 2015-05-26 07:54:43 theanets.trainer:168 RmsProp 318 loss=4.351403 err=4.351403
I 2015-05-26 07:55:20 theanets.trainer:168 RmsProp 319 loss=5.577130 err=5.577130
I 2015-05-26 07:55:57 theanets.trainer:168 RmsProp 320 loss=5.107491 err=5.107491
I 2015-05-26 07:55:58 theanets.trainer:168 validation 32 loss=1441.853882 err=1441.853882
I 2015-05-26 07:56:34 theanets.trainer:168 RmsProp 321 loss=4.498452 err=4.498452
I 2015-05-26 07:57:11 theanets.trainer:168 RmsProp 322 loss=4.610872 err=4.610872
I 2015-05-26 07:57:48 theanets.trainer:168 RmsProp 323 loss=5.278642 err=5.278642
I 2015-05-26 07:58:23 theanets.trainer:168 RmsProp 324 loss=6.582995 err=6.582995
I 2015-05-26 07:58:58 theanets.trainer:168 RmsProp 325 loss=5.633115 err=5.633115
I 2015-05-26 07:59:32 theanets.trainer:168 RmsProp 326 loss=5.111196 err=5.111196
I 2015-05-26 08:00:08 theanets.trainer:168 RmsProp 327 loss=4.693824 err=4.693824
I 2015-05-26 08:00:45 theanets.trainer:168 RmsProp 328 loss=3.917853 err=3.917853
I 2015-05-26 08:01:22 theanets.trainer:168 RmsProp 329 loss=3.833450 err=3.833450
I 2015-05-26 08:01:57 theanets.trainer:168 RmsProp 330 loss=4.302014 err=4.302014
I 2015-05-26 08:01:58 theanets.trainer:168 validation 33 loss=1420.372192 err=1420.372192
I 2015-05-26 08:01:58 theanets.trainer:252 patience elapsed!
I 2015-05-26 08:01:58 theanets.dataset:133 valid: 2 mini-batches from callable
I 2015-05-26 08:01:58 theanets.dataset:133 train: 10 mini-batches from callable
I 2015-05-26 08:01:58 theanets.main:87 creating trainer <class 'theanets.trainer.RmsProp'>
I 2015-05-26 08:01:58 theanets.main:89 --algorithms = rmsprop
I 2015-05-26 08:01:58 theanets.main:89 --batch_size = 1024
I 2015-05-26 08:01:58 theanets.main:89 --gradient_clip = 100000.0
I 2015-05-26 08:01:58 theanets.main:89 --hidden_l1 = None
I 2015-05-26 08:01:58 theanets.main:89 --learning_rate = 0.0001
I 2015-05-26 08:01:58 theanets.main:89 --train_batches = 10
I 2015-05-26 08:01:58 theanets.main:89 --valid_batches = 2
I 2015-05-26 08:01:58 theanets.main:89 --weight_l1 = None
I 2015-05-26 08:01:58 theanets.main:89 --weight_l2 = None
I 2015-05-26 08:01:58 theanets.trainer:134 compiling evaluation function
I 2015-05-26 08:02:06 theanets.trainer:294 compiling RmsProp learning function
I 2015-05-26 08:03:42 theanets.trainer:168 validation 0 loss=1810.592407 err=1810.592407 *
I 2015-05-26 08:03:53 theanets.trainer:168 RmsProp 1 loss=4.231544 err=4.231544
I 2015-05-26 08:04:04 theanets.trainer:168 RmsProp 2 loss=2.775574 err=2.775574
I 2015-05-26 08:04:15 theanets.trainer:168 RmsProp 3 loss=2.119367 err=2.119367
I 2015-05-26 08:04:26 theanets.trainer:168 RmsProp 4 loss=1.893149 err=1.893149
I 2015-05-26 08:04:37 theanets.trainer:168 RmsProp 5 loss=1.708699 err=1.708699
I 2015-05-26 08:04:48 theanets.trainer:168 RmsProp 6 loss=1.580851 err=1.580851
I 2015-05-26 08:04:59 theanets.trainer:168 RmsProp 7 loss=1.491705 err=1.491705
I 2015-05-26 08:05:11 theanets.trainer:168 RmsProp 8 loss=1.439047 err=1.439047
I 2015-05-26 08:05:22 theanets.trainer:168 RmsProp 9 loss=1.383616 err=1.383616
I 2015-05-26 08:05:32 theanets.trainer:168 RmsProp 10 loss=1.313685 err=1.313685
I 2015-05-26 08:05:33 theanets.trainer:168 validation 1 loss=1802.066040 err=1802.066040 *
I 2015-05-26 08:05:44 theanets.trainer:168 RmsProp 11 loss=1.223412 err=1.223412
I 2015-05-26 08:05:55 theanets.trainer:168 RmsProp 12 loss=1.204566 err=1.204566
I 2015-05-26 08:06:06 theanets.trainer:168 RmsProp 13 loss=1.188105 err=1.188105
I 2015-05-26 08:06:17 theanets.trainer:168 RmsProp 14 loss=1.156506 err=1.156506
I 2015-05-26 08:06:28 theanets.trainer:168 RmsProp 15 loss=1.122259 err=1.122259
I 2015-05-26 08:06:39 theanets.trainer:168 RmsProp 16 loss=1.103044 err=1.103044
I 2015-05-26 08:06:51 theanets.trainer:168 RmsProp 17 loss=1.059731 err=1.059731
I 2015-05-26 08:07:02 theanets.trainer:168 RmsProp 18 loss=1.013849 err=1.013849
I 2015-05-26 08:07:13 theanets.trainer:168 RmsProp 19 loss=1.043977 err=1.043977
I 2015-05-26 08:07:24 theanets.trainer:168 RmsProp 20 loss=0.991775 err=0.991775
I 2015-05-26 08:07:24 theanets.trainer:168 validation 2 loss=1793.810547 err=1793.810547 *
I 2015-05-26 08:07:34 theanets.trainer:168 RmsProp 21 loss=0.995798 err=0.995798
I 2015-05-26 08:07:44 theanets.trainer:168 RmsProp 22 loss=0.962092 err=0.962092
I 2015-05-26 08:07:55 theanets.trainer:168 RmsProp 23 loss=0.939345 err=0.939345
I 2015-05-26 08:08:05 theanets.trainer:168 RmsProp 24 loss=0.916067 err=0.916067
I 2015-05-26 08:08:16 theanets.trainer:168 RmsProp 25 loss=0.899491 err=0.899491
I 2015-05-26 08:08:26 theanets.trainer:168 RmsProp 26 loss=0.886776 err=0.886776
I 2015-05-26 08:08:36 theanets.trainer:168 RmsProp 27 loss=0.853042 err=0.853042
I 2015-05-26 08:08:46 theanets.trainer:168 RmsProp 28 loss=0.842705 err=0.842705
I 2015-05-26 08:08:57 theanets.trainer:168 RmsProp 29 loss=0.845257 err=0.845257
I 2015-05-26 08:09:07 theanets.trainer:168 RmsProp 30 loss=0.830616 err=0.830616
I 2015-05-26 08:09:08 theanets.trainer:168 validation 3 loss=1787.554321 err=1787.554321 *
I 2015-05-26 08:09:19 theanets.trainer:168 RmsProp 31 loss=0.826969 err=0.826969
I 2015-05-26 08:09:30 theanets.trainer:168 RmsProp 32 loss=0.831099 err=0.831099
I 2015-05-26 08:09:41 theanets.trainer:168 RmsProp 33 loss=0.810330 err=0.810330
I 2015-05-26 08:09:53 theanets.trainer:168 RmsProp 34 loss=0.791349 err=0.791349
I 2015-05-26 08:10:04 theanets.trainer:168 RmsProp 35 loss=0.751440 err=0.751440
I 2015-05-26 08:10:15 theanets.trainer:168 RmsProp 36 loss=0.779970 err=0.779970
I 2015-05-26 08:10:26 theanets.trainer:168 RmsProp 37 loss=0.771723 err=0.771723
I 2015-05-26 08:10:37 theanets.trainer:168 RmsProp 38 loss=0.724111 err=0.724111
I 2015-05-26 08:10:48 theanets.trainer:168 RmsProp 39 loss=0.742256 err=0.742256
I 2015-05-26 08:10:59 theanets.trainer:168 RmsProp 40 loss=0.738281 err=0.738281
I 2015-05-26 08:11:00 theanets.trainer:168 validation 4 loss=1783.187378 err=1783.187378 *
I 2015-05-26 08:11:11 theanets.trainer:168 RmsProp 41 loss=0.708243 err=0.708243
I 2015-05-26 08:11:22 theanets.trainer:168 RmsProp 42 loss=0.706020 err=0.706020
I 2015-05-26 08:11:33 theanets.trainer:168 RmsProp 43 loss=0.697856 err=0.697856
I 2015-05-26 08:11:44 theanets.trainer:168 RmsProp 44 loss=0.686059 err=0.686059
I 2015-05-26 08:11:55 theanets.trainer:168 RmsProp 45 loss=0.721240 err=0.721240
I 2015-05-26 08:12:06 theanets.trainer:168 RmsProp 46 loss=0.695088 err=0.695088
I 2015-05-26 08:12:17 theanets.trainer:168 RmsProp 47 loss=0.679344 err=0.679344
I 2015-05-26 08:12:29 theanets.trainer:168 RmsProp 48 loss=0.678168 err=0.678168
I 2015-05-26 08:12:40 theanets.trainer:168 RmsProp 49 loss=0.657253 err=0.657253
I 2015-05-26 08:12:51 theanets.trainer:168 RmsProp 50 loss=0.665912 err=0.665912
I 2015-05-26 08:12:51 theanets.trainer:168 validation 5 loss=1778.923096 err=1778.923096 *
I 2015-05-26 08:13:02 theanets.trainer:168 RmsProp 51 loss=0.664640 err=0.664640
I 2015-05-26 08:13:14 theanets.trainer:168 RmsProp 52 loss=0.628978 err=0.628978
I 2015-05-26 08:13:25 theanets.trainer:168 RmsProp 53 loss=0.628147 err=0.628147
I 2015-05-26 08:13:37 theanets.trainer:168 RmsProp 54 loss=0.621154 err=0.621154
I 2015-05-26 08:13:48 theanets.trainer:168 RmsProp 55 loss=0.617972 err=0.617972
I 2015-05-26 08:13:59 theanets.trainer:168 RmsProp 56 loss=0.613218 err=0.613218
I 2015-05-26 08:14:10 theanets.trainer:168 RmsProp 57 loss=0.614916 err=0.614916
I 2015-05-26 08:14:21 theanets.trainer:168 RmsProp 58 loss=0.603307 err=0.603307
I 2015-05-26 08:14:32 theanets.trainer:168 RmsProp 59 loss=0.600611 err=0.600611
I 2015-05-26 08:14:43 theanets.trainer:168 RmsProp 60 loss=0.592046 err=0.592046
I 2015-05-26 08:14:43 theanets.trainer:168 validation 6 loss=1774.561401 err=1774.561401 *
I 2015-05-26 08:14:55 theanets.trainer:168 RmsProp 61 loss=0.619142 err=0.619142
I 2015-05-26 08:15:06 theanets.trainer:168 RmsProp 62 loss=0.586886 err=0.586886
I 2015-05-26 08:15:17 theanets.trainer:168 RmsProp 63 loss=0.557747 err=0.557747
I 2015-05-26 08:15:28 theanets.trainer:168 RmsProp 64 loss=0.557967 err=0.557967
I 2015-05-26 08:15:40 theanets.trainer:168 RmsProp 65 loss=0.561416 err=0.561416
I 2015-05-26 08:15:51 theanets.trainer:168 RmsProp 66 loss=0.546092 err=0.546092
I 2015-05-26 08:16:03 theanets.trainer:168 RmsProp 67 loss=0.571429 err=0.571429
I 2015-05-26 08:16:14 theanets.trainer:168 RmsProp 68 loss=0.547136 err=0.547136
I 2015-05-26 08:16:25 theanets.trainer:168 RmsProp 69 loss=0.541383 err=0.541383
I 2015-05-26 08:16:36 theanets.trainer:168 RmsProp 70 loss=0.536993 err=0.536993
I 2015-05-26 08:16:37 theanets.trainer:168 validation 7 loss=1772.436768 err=1772.436768 *
I 2015-05-26 08:16:48 theanets.trainer:168 RmsProp 71 loss=0.547777 err=0.547777
I 2015-05-26 08:16:59 theanets.trainer:168 RmsProp 72 loss=0.522026 err=0.522026
I 2015-05-26 08:17:10 theanets.trainer:168 RmsProp 73 loss=0.520437 err=0.520437
I 2015-05-26 08:17:21 theanets.trainer:168 RmsProp 74 loss=0.532672 err=0.532672
I 2015-05-26 08:17:32 theanets.trainer:168 RmsProp 75 loss=0.514003 err=0.514003
I 2015-05-26 08:17:43 theanets.trainer:168 RmsProp 76 loss=0.511883 err=0.511883
I 2015-05-26 08:17:54 theanets.trainer:168 RmsProp 77 loss=0.529110 err=0.529110
I 2015-05-26 08:18:05 theanets.trainer:168 RmsProp 78 loss=0.508201 err=0.508201
I 2015-05-26 08:18:16 theanets.trainer:168 RmsProp 79 loss=0.491410 err=0.491410
I 2015-05-26 08:18:27 theanets.trainer:168 RmsProp 80 loss=0.504286 err=0.504286
I 2015-05-26 08:18:28 theanets.trainer:168 validation 8 loss=1767.945557 err=1767.945557 *
I 2015-05-26 08:18:38 theanets.trainer:168 RmsProp 81 loss=0.489092 err=0.489092
I 2015-05-26 08:18:49 theanets.trainer:168 RmsProp 82 loss=0.515213 err=0.515213
I 2015-05-26 08:19:01 theanets.trainer:168 RmsProp 83 loss=0.500261 err=0.500261
I 2015-05-26 08:19:12 theanets.trainer:168 RmsProp 84 loss=0.483262 err=0.483262
I 2015-05-26 08:19:23 theanets.trainer:168 RmsProp 85 loss=0.477410 err=0.477410
I 2015-05-26 08:19:35 theanets.trainer:168 RmsProp 86 loss=0.467171 err=0.467171
I 2015-05-26 08:19:46 theanets.trainer:168 RmsProp 87 loss=0.472444 err=0.472444
I 2015-05-26 08:19:58 theanets.trainer:168 RmsProp 88 loss=0.460327 err=0.460327
I 2015-05-26 08:20:09 theanets.trainer:168 RmsProp 89 loss=0.477896 err=0.477896
I 2015-05-26 08:20:20 theanets.trainer:168 RmsProp 90 loss=0.467442 err=0.467442
I 2015-05-26 08:20:21 theanets.trainer:168 validation 9 loss=1766.512573 err=1766.512573 *
I 2015-05-26 08:20:33 theanets.trainer:168 RmsProp 91 loss=0.459457 err=0.459457
I 2015-05-26 08:20:44 theanets.trainer:168 RmsProp 92 loss=0.472979 err=0.472979
I 2015-05-26 08:20:55 theanets.trainer:168 RmsProp 93 loss=0.449759 err=0.449759
I 2015-05-26 08:21:07 theanets.trainer:168 RmsProp 94 loss=0.460038 err=0.460038
I 2015-05-26 08:21:18 theanets.trainer:168 RmsProp 95 loss=0.448510 err=0.448510
I 2015-05-26 08:21:30 theanets.trainer:168 RmsProp 96 loss=0.457000 err=0.457000
I 2015-05-26 08:21:41 theanets.trainer:168 RmsProp 97 loss=0.458412 err=0.458412
I 2015-05-26 08:21:52 theanets.trainer:168 RmsProp 98 loss=0.444137 err=0.444137
I 2015-05-26 08:22:04 theanets.trainer:168 RmsProp 99 loss=0.436609 err=0.436609
I 2015-05-26 08:22:15 theanets.trainer:168 RmsProp 100 loss=0.445029 err=0.445029
I 2015-05-26 08:22:16 theanets.trainer:168 validation 10 loss=1765.890015 err=1765.890015 *
I 2015-05-26 08:22:27 theanets.trainer:168 RmsProp 101 loss=0.433092 err=0.433092
I 2015-05-26 08:22:38 theanets.trainer:168 RmsProp 102 loss=0.444808 err=0.444808
I 2015-05-26 08:22:50 theanets.trainer:168 RmsProp 103 loss=0.429293 err=0.429293
I 2015-05-26 08:23:01 theanets.trainer:168 RmsProp 104 loss=0.422254 err=0.422254
I 2015-05-26 08:23:13 theanets.trainer:168 RmsProp 105 loss=0.424159 err=0.424159
I 2015-05-26 08:23:24 theanets.trainer:168 RmsProp 106 loss=0.425664 err=0.425664
I 2015-05-26 08:23:35 theanets.trainer:168 RmsProp 107 loss=0.410796 err=0.410796
I 2015-05-26 08:23:47 theanets.trainer:168 RmsProp 108 loss=0.422546 err=0.422546
I 2015-05-26 08:23:58 theanets.trainer:168 RmsProp 109 loss=0.400339 err=0.400339
I 2015-05-26 08:24:09 theanets.trainer:168 RmsProp 110 loss=0.402911 err=0.402911
I 2015-05-26 08:24:10 theanets.trainer:168 validation 11 loss=1763.511353 err=1763.511353 *
I 2015-05-26 08:24:21 theanets.trainer:168 RmsProp 111 loss=0.421665 err=0.421665
I 2015-05-26 08:24:32 theanets.trainer:168 RmsProp 112 loss=0.406586 err=0.406586
I 2015-05-26 08:24:44 theanets.trainer:168 RmsProp 113 loss=0.399822 err=0.399822
I 2015-05-26 08:24:56 theanets.trainer:168 RmsProp 114 loss=0.407446 err=0.407446
I 2015-05-26 08:25:07 theanets.trainer:168 RmsProp 115 loss=0.401300 err=0.401300
I 2015-05-26 08:25:18 theanets.trainer:168 RmsProp 116 loss=0.395392 err=0.395392
I 2015-05-26 08:25:29 theanets.trainer:168 RmsProp 117 loss=0.395625 err=0.395625
I 2015-05-26 08:25:41 theanets.trainer:168 RmsProp 118 loss=0.406725 err=0.406725
I 2015-05-26 08:25:52 theanets.trainer:168 RmsProp 119 loss=0.399580 err=0.399580
I 2015-05-26 08:26:04 theanets.trainer:168 RmsProp 120 loss=0.394649 err=0.394649
I 2015-05-26 08:26:04 theanets.trainer:168 validation 12 loss=1761.944214 err=1761.944214 *
I 2015-05-26 08:26:16 theanets.trainer:168 RmsProp 121 loss=0.381131 err=0.381131
I 2015-05-26 08:26:28 theanets.trainer:168 RmsProp 122 loss=0.380804 err=0.380804
I 2015-05-26 08:26:39 theanets.trainer:168 RmsProp 123 loss=0.390499 err=0.390499
I 2015-05-26 08:26:51 theanets.trainer:168 RmsProp 124 loss=0.391950 err=0.391950
I 2015-05-26 08:27:02 theanets.trainer:168 RmsProp 125 loss=0.383447 err=0.383447
I 2015-05-26 08:27:14 theanets.trainer:168 RmsProp 126 loss=0.380390 err=0.380390
I 2015-05-26 08:27:26 theanets.trainer:168 RmsProp 127 loss=0.386053 err=0.386053
I 2015-05-26 08:27:37 theanets.trainer:168 RmsProp 128 loss=0.369969 err=0.369969
I 2015-05-26 08:27:49 theanets.trainer:168 RmsProp 129 loss=0.380157 err=0.380157
I 2015-05-26 08:28:00 theanets.trainer:168 RmsProp 130 loss=0.368972 err=0.368972
I 2015-05-26 08:28:01 theanets.trainer:168 validation 13 loss=1759.379761 err=1759.379761 *
I 2015-05-26 08:28:12 theanets.trainer:168 RmsProp 131 loss=0.374114 err=0.374114
I 2015-05-26 08:28:23 theanets.trainer:168 RmsProp 132 loss=0.365196 err=0.365196
I 2015-05-26 08:28:34 theanets.trainer:168 RmsProp 133 loss=0.364113 err=0.364113
I 2015-05-26 08:28:45 theanets.trainer:168 RmsProp 134 loss=0.365164 err=0.365164
I 2015-05-26 08:28:57 theanets.trainer:168 RmsProp 135 loss=0.367482 err=0.367482
I 2015-05-26 08:29:09 theanets.trainer:168 RmsProp 136 loss=0.361888 err=0.361888
I 2015-05-26 08:29:20 theanets.trainer:168 RmsProp 137 loss=0.367256 err=0.367256
I 2015-05-26 08:29:32 theanets.trainer:168 RmsProp 138 loss=0.360190 err=0.360190
I 2015-05-26 08:29:43 theanets.trainer:168 RmsProp 139 loss=0.350366 err=0.350366
I 2015-05-26 08:29:54 theanets.trainer:168 RmsProp 140 loss=0.364709 err=0.364709
I 2015-05-26 08:29:55 theanets.trainer:168 validation 14 loss=1760.398315 err=1760.398315
I 2015-05-26 08:30:06 theanets.trainer:168 RmsProp 141 loss=0.365582 err=0.365582
I 2015-05-26 08:30:18 theanets.trainer:168 RmsProp 142 loss=0.340567 err=0.340567
I 2015-05-26 08:30:30 theanets.trainer:168 RmsProp 143 loss=0.336886 err=0.336886
I 2015-05-26 08:30:42 theanets.trainer:168 RmsProp 144 loss=0.343725 err=0.343725
I 2015-05-26 08:30:53 theanets.trainer:168 RmsProp 145 loss=0.342762 err=0.342762
I 2015-05-26 08:31:05 theanets.trainer:168 RmsProp 146 loss=0.345545 err=0.345545
I 2015-05-26 08:31:16 theanets.trainer:168 RmsProp 147 loss=0.340738 err=0.340738
I 2015-05-26 08:31:27 theanets.trainer:168 RmsProp 148 loss=0.337161 err=0.337161
I 2015-05-26 08:31:37 theanets.trainer:168 RmsProp 149 loss=0.347391 err=0.347391
I 2015-05-26 08:31:48 theanets.trainer:168 RmsProp 150 loss=0.328268 err=0.328268
I 2015-05-26 08:31:48 theanets.trainer:168 validation 15 loss=1759.281250 err=1759.281250 *
I 2015-05-26 08:31:59 theanets.trainer:168 RmsProp 151 loss=0.330608 err=0.330608
I 2015-05-26 08:32:09 theanets.trainer:168 RmsProp 152 loss=0.337932 err=0.337932
I 2015-05-26 08:32:20 theanets.trainer:168 RmsProp 153 loss=0.343727 err=0.343727
I 2015-05-26 08:32:30 theanets.trainer:168 RmsProp 154 loss=0.330565 err=0.330565
I 2015-05-26 08:32:41 theanets.trainer:168 RmsProp 155 loss=0.328427 err=0.328427
I 2015-05-26 08:32:52 theanets.trainer:168 RmsProp 156 loss=0.338307 err=0.338307
I 2015-05-26 08:33:02 theanets.trainer:168 RmsProp 157 loss=0.323936 err=0.323936
I 2015-05-26 08:33:13 theanets.trainer:168 RmsProp 158 loss=0.319199 err=0.319199
I 2015-05-26 08:33:23 theanets.trainer:168 RmsProp 159 loss=0.311693 err=0.311693
I 2015-05-26 08:33:34 theanets.trainer:168 RmsProp 160 loss=0.324817 err=0.324817
I 2015-05-26 08:33:34 theanets.trainer:168 validation 16 loss=1759.082275 err=1759.082275 *
I 2015-05-26 08:33:44 theanets.trainer:168 RmsProp 161 loss=0.323852 err=0.323852
I 2015-05-26 08:33:55 theanets.trainer:168 RmsProp 162 loss=0.319315 err=0.319315
I 2015-05-26 08:34:05 theanets.trainer:168 RmsProp 163 loss=0.318561 err=0.318561
I 2015-05-26 08:34:15 theanets.trainer:168 RmsProp 164 loss=0.313793 err=0.313793
I 2015-05-26 08:34:26 theanets.trainer:168 RmsProp 165 loss=0.310832 err=0.310832
I 2015-05-26 08:34:36 theanets.trainer:168 RmsProp 166 loss=0.311324 err=0.311324
I 2015-05-26 08:34:47 theanets.trainer:168 RmsProp 167 loss=0.320361 err=0.320361
I 2015-05-26 08:34:57 theanets.trainer:168 RmsProp 168 loss=0.311688 err=0.311688
I 2015-05-26 08:35:08 theanets.trainer:168 RmsProp 169 loss=0.304979 err=0.304979
I 2015-05-26 08:35:18 theanets.trainer:168 RmsProp 170 loss=0.308875 err=0.308875
I 2015-05-26 08:35:19 theanets.trainer:168 validation 17 loss=1756.375854 err=1756.375854 *
I 2015-05-26 08:35:29 theanets.trainer:168 RmsProp 171 loss=0.305234 err=0.305234
I 2015-05-26 08:35:40 theanets.trainer:168 RmsProp 172 loss=0.309800 err=0.309800
I 2015-05-26 08:35:50 theanets.trainer:168 RmsProp 173 loss=0.316313 err=0.316313
I 2015-05-26 08:36:00 theanets.trainer:168 RmsProp 174 loss=0.303022 err=0.303022
I 2015-05-26 08:36:11 theanets.trainer:168 RmsProp 175 loss=0.299152 err=0.299152
I 2015-05-26 08:36:21 theanets.trainer:168 RmsProp 176 loss=0.296971 err=0.296971
I 2015-05-26 08:36:31 theanets.trainer:168 RmsProp 177 loss=0.311475 err=0.311475
I 2015-05-26 08:36:42 theanets.trainer:168 RmsProp 178 loss=0.290570 err=0.290570
I 2015-05-26 08:36:52 theanets.trainer:168 RmsProp 179 loss=0.307644 err=0.307644
I 2015-05-26 08:37:03 theanets.trainer:168 RmsProp 180 loss=0.294568 err=0.294568
I 2015-05-26 08:37:03 theanets.trainer:168 validation 18 loss=1755.810913 err=1755.810913 *
I 2015-05-26 08:37:14 theanets.trainer:168 RmsProp 181 loss=0.295631 err=0.295631
I 2015-05-26 08:37:24 theanets.trainer:168 RmsProp 182 loss=0.302883 err=0.302883
I 2015-05-26 08:37:35 theanets.trainer:168 RmsProp 183 loss=0.287519 err=0.287519
I 2015-05-26 08:37:45 theanets.trainer:168 RmsProp 184 loss=0.295171 err=0.295171
I 2015-05-26 08:37:56 theanets.trainer:168 RmsProp 185 loss=0.284579 err=0.284579
I 2015-05-26 08:38:06 theanets.trainer:168 RmsProp 186 loss=0.290618 err=0.290618
I 2015-05-26 08:38:17 theanets.trainer:168 RmsProp 187 loss=0.285778 err=0.285778
I 2015-05-26 08:38:27 theanets.trainer:168 RmsProp 188 loss=0.291740 err=0.291740
I 2015-05-26 08:38:37 theanets.trainer:168 RmsProp 189 loss=0.285053 err=0.285053
I 2015-05-26 08:38:47 theanets.trainer:168 RmsProp 190 loss=0.280592 err=0.280592
I 2015-05-26 08:38:48 theanets.trainer:168 validation 19 loss=1754.337769 err=1754.337769 *
I 2015-05-26 08:38:58 theanets.trainer:168 RmsProp 191 loss=0.282897 err=0.282897
I 2015-05-26 08:39:09 theanets.trainer:168 RmsProp 192 loss=0.286313 err=0.286313
I 2015-05-26 08:39:19 theanets.trainer:168 RmsProp 193 loss=0.284424 err=0.284424
I 2015-05-26 08:39:29 theanets.trainer:168 RmsProp 194 loss=0.273845 err=0.273845
I 2015-05-26 08:39:40 theanets.trainer:168 RmsProp 195 loss=0.285558 err=0.285558
I 2015-05-26 08:39:51 theanets.trainer:168 RmsProp 196 loss=0.278187 err=0.278187
I 2015-05-26 08:40:01 theanets.trainer:168 RmsProp 197 loss=0.268922 err=0.268922
I 2015-05-26 08:40:12 theanets.trainer:168 RmsProp 198 loss=0.286136 err=0.286136
I 2015-05-26 08:40:23 theanets.trainer:168 RmsProp 199 loss=0.276015 err=0.276015
I 2015-05-26 08:40:33 theanets.trainer:168 RmsProp 200 loss=0.272698 err=0.272698
I 2015-05-26 08:40:34 theanets.trainer:168 validation 20 loss=1753.082031 err=1753.082031 *
I 2015-05-26 08:40:44 theanets.trainer:168 RmsProp 201 loss=0.271428 err=0.271428
I 2015-05-26 08:40:54 theanets.trainer:168 RmsProp 202 loss=0.284762 err=0.284762
I 2015-05-26 08:41:05 theanets.trainer:168 RmsProp 203 loss=0.276832 err=0.276832
I 2015-05-26 08:41:15 theanets.trainer:168 RmsProp 204 loss=0.270865 err=0.270865
I 2015-05-26 08:41:26 theanets.trainer:168 RmsProp 205 loss=0.275429 err=0.275429
I 2015-05-26 08:41:36 theanets.trainer:168 RmsProp 206 loss=0.273247 err=0.273247
I 2015-05-26 08:41:47 theanets.trainer:168 RmsProp 207 loss=0.261742 err=0.261742
I 2015-05-26 08:41:57 theanets.trainer:168 RmsProp 208 loss=0.268093 err=0.268093
I 2015-05-26 08:42:07 theanets.trainer:168 RmsProp 209 loss=0.273208 err=0.273208
I 2015-05-26 08:42:17 theanets.trainer:168 RmsProp 210 loss=0.264141 err=0.264141
I 2015-05-26 08:42:18 theanets.trainer:168 validation 21 loss=1753.099976 err=1753.099976
I 2015-05-26 08:42:28 theanets.trainer:168 RmsProp 211 loss=0.256664 err=0.256664
I 2015-05-26 08:42:38 theanets.trainer:168 RmsProp 212 loss=0.252513 err=0.252513
I 2015-05-26 08:42:48 theanets.trainer:168 RmsProp 213 loss=0.261911 err=0.261911
I 2015-05-26 08:42:59 theanets.trainer:168 RmsProp 214 loss=0.255380 err=0.255380
I 2015-05-26 08:43:09 theanets.trainer:168 RmsProp 215 loss=0.256317 err=0.256317
I 2015-05-26 08:43:20 theanets.trainer:168 RmsProp 216 loss=0.263169 err=0.263169
I 2015-05-26 08:43:30 theanets.trainer:168 RmsProp 217 loss=0.258550 err=0.258550
I 2015-05-26 08:43:41 theanets.trainer:168 RmsProp 218 loss=0.259020 err=0.259020
I 2015-05-26 08:43:51 theanets.trainer:168 RmsProp 219 loss=0.257895 err=0.257895
I 2015-05-26 08:44:01 theanets.trainer:168 RmsProp 220 loss=0.253861 err=0.253861
I 2015-05-26 08:44:02 theanets.trainer:168 validation 22 loss=1751.887573 err=1751.887573 *
I 2015-05-26 08:44:12 theanets.trainer:168 RmsProp 221 loss=0.252335 err=0.252335
I 2015-05-26 08:44:23 theanets.trainer:168 RmsProp 222 loss=0.247825 err=0.247825
I 2015-05-26 08:44:33 theanets.trainer:168 RmsProp 223 loss=0.250977 err=0.250977
I 2015-05-26 08:44:44 theanets.trainer:168 RmsProp 224 loss=0.251503 err=0.251503
I 2015-05-26 08:44:54 theanets.trainer:168 RmsProp 225 loss=0.250431 err=0.250431
I 2015-05-26 08:45:05 theanets.trainer:168 RmsProp 226 loss=0.258934 err=0.258934
I 2015-05-26 08:45:15 theanets.trainer:168 RmsProp 227 loss=0.239868 err=0.239868
I 2015-05-26 08:45:25 theanets.trainer:168 RmsProp 228 loss=0.260027 err=0.260027
I 2015-05-26 08:45:36 theanets.trainer:168 RmsProp 229 loss=0.252733 err=0.252733
I 2015-05-26 08:45:46 theanets.trainer:168 RmsProp 230 loss=0.244604 err=0.244604
I 2015-05-26 08:45:47 theanets.trainer:168 validation 23 loss=1750.895508 err=1750.895508 *
I 2015-05-26 08:45:57 theanets.trainer:168 RmsProp 231 loss=0.248999 err=0.248999
I 2015-05-26 08:46:07 theanets.trainer:168 RmsProp 232 loss=0.250214 err=0.250214
I 2015-05-26 08:46:17 theanets.trainer:168 RmsProp 233 loss=0.245313 err=0.245313
I 2015-05-26 08:46:27 theanets.trainer:168 RmsProp 234 loss=0.243597 err=0.243597
I 2015-05-26 08:46:37 theanets.trainer:168 RmsProp 235 loss=0.242592 err=0.242592
I 2015-05-26 08:46:47 theanets.trainer:168 RmsProp 236 loss=0.243918 err=0.243918
I 2015-05-26 08:46:57 theanets.trainer:168 RmsProp 237 loss=0.234225 err=0.234225
I 2015-05-26 08:47:07 theanets.trainer:168 RmsProp 238 loss=0.240892 err=0.240892
I 2015-05-26 08:47:17 theanets.trainer:168 RmsProp 239 loss=0.226815 err=0.226815
I 2015-05-26 08:47:26 theanets.trainer:168 RmsProp 240 loss=0.238598 err=0.238598
I 2015-05-26 08:47:27 theanets.trainer:168 validation 24 loss=1750.243042 err=1750.243042 *
I 2015-05-26 08:47:37 theanets.trainer:168 RmsProp 241 loss=0.243418 err=0.243418
I 2015-05-26 08:47:47 theanets.trainer:168 RmsProp 242 loss=0.235565 err=0.235565
I 2015-05-26 08:47:57 theanets.trainer:168 RmsProp 243 loss=0.233920 err=0.233920
I 2015-05-26 08:48:07 theanets.trainer:168 RmsProp 244 loss=0.237971 err=0.237971
I 2015-05-26 08:48:18 theanets.trainer:168 RmsProp 245 loss=0.232608 err=0.232608
I 2015-05-26 08:48:28 theanets.trainer:168 RmsProp 246 loss=0.237185 err=0.237185
I 2015-05-26 08:48:38 theanets.trainer:168 RmsProp 247 loss=0.239074 err=0.239074
I 2015-05-26 08:48:48 theanets.trainer:168 RmsProp 248 loss=0.242348 err=0.242348
I 2015-05-26 08:48:57 theanets.trainer:168 RmsProp 249 loss=0.232186 err=0.232186
I 2015-05-26 08:49:07 theanets.trainer:168 RmsProp 250 loss=0.226650 err=0.226650
I 2015-05-26 08:49:08 theanets.trainer:168 validation 25 loss=1750.000366 err=1750.000366 *
I 2015-05-26 08:49:18 theanets.trainer:168 RmsProp 251 loss=0.233740 err=0.233740
I 2015-05-26 08:49:28 theanets.trainer:168 RmsProp 252 loss=0.230214 err=0.230214
I 2015-05-26 08:49:38 theanets.trainer:168 RmsProp 253 loss=0.221498 err=0.221498
I 2015-05-26 08:49:48 theanets.trainer:168 RmsProp 254 loss=0.231100 err=0.231100
I 2015-05-26 08:49:58 theanets.trainer:168 RmsProp 255 loss=0.223600 err=0.223600
I 2015-05-26 08:50:08 theanets.trainer:168 RmsProp 256 loss=0.231550 err=0.231550
I 2015-05-26 08:50:18 theanets.trainer:168 RmsProp 257 loss=0.226540 err=0.226540
I 2015-05-26 08:50:28 theanets.trainer:168 RmsProp 258 loss=0.222383 err=0.222383
I 2015-05-26 08:50:38 theanets.trainer:168 RmsProp 259 loss=0.223346 err=0.223346
I 2015-05-26 08:50:48 theanets.trainer:168 RmsProp 260 loss=0.223546 err=0.223546
I 2015-05-26 08:50:48 theanets.trainer:168 validation 26 loss=1748.182007 err=1748.182007 *
I 2015-05-26 08:50:56 theanets.trainer:168 RmsProp 261 loss=0.228463 err=0.228463
I 2015-05-26 08:51:05 theanets.trainer:168 RmsProp 262 loss=0.232570 err=0.232570
I 2015-05-26 08:51:13 theanets.trainer:168 RmsProp 263 loss=0.222173 err=0.222173
I 2015-05-26 08:51:22 theanets.trainer:168 RmsProp 264 loss=0.225066 err=0.225066
I 2015-05-26 08:51:31 theanets.trainer:168 RmsProp 265 loss=0.223054 err=0.223054
I 2015-05-26 08:51:40 theanets.trainer:168 RmsProp 266 loss=0.218781 err=0.218781
I 2015-05-26 08:51:49 theanets.trainer:168 RmsProp 267 loss=0.221826 err=0.221826
I 2015-05-26 08:51:58 theanets.trainer:168 RmsProp 268 loss=0.229508 err=0.229508
I 2015-05-26 08:52:06 theanets.trainer:168 RmsProp 269 loss=0.213944 err=0.213944
I 2015-05-26 08:52:15 theanets.trainer:168 RmsProp 270 loss=0.212672 err=0.212672
I 2015-05-26 08:52:15 theanets.trainer:168 validation 27 loss=1748.366455 err=1748.366455
I 2015-05-26 08:52:24 theanets.trainer:168 RmsProp 271 loss=0.229241 err=0.229241
I 2015-05-26 08:52:32 theanets.trainer:168 RmsProp 272 loss=0.218358 err=0.218358
I 2015-05-26 08:52:41 theanets.trainer:168 RmsProp 273 loss=0.210162 err=0.210162
I 2015-05-26 08:52:49 theanets.trainer:168 RmsProp 274 loss=0.236138 err=0.236138
I 2015-05-26 08:52:58 theanets.trainer:168 RmsProp 275 loss=0.212483 err=0.212483
I 2015-05-26 08:53:06 theanets.trainer:168 RmsProp 276 loss=0.211740 err=0.211740
I 2015-05-26 08:53:14 theanets.trainer:168 RmsProp 277 loss=0.212977 err=0.212977
I 2015-05-26 08:53:23 theanets.trainer:168 RmsProp 278 loss=0.214014 err=0.214014
I 2015-05-26 08:53:31 theanets.trainer:168 RmsProp 279 loss=0.219783 err=0.219783
I 2015-05-26 08:53:40 theanets.trainer:168 RmsProp 280 loss=0.208996 err=0.208996
I 2015-05-26 08:53:40 theanets.trainer:168 validation 28 loss=1748.015625 err=1748.015625 *
I 2015-05-26 08:53:49 theanets.trainer:168 RmsProp 281 loss=0.215058 err=0.215058
I 2015-05-26 08:53:57 theanets.trainer:168 RmsProp 282 loss=0.206172 err=0.206172
I 2015-05-26 08:54:05 theanets.trainer:168 RmsProp 283 loss=0.209310 err=0.209310
I 2015-05-26 08:54:13 theanets.trainer:168 RmsProp 284 loss=0.213689 err=0.213689
I 2015-05-26 08:54:20 theanets.trainer:168 RmsProp 285 loss=0.203434 err=0.203434
I 2015-05-26 08:54:28 theanets.trainer:168 RmsProp 286 loss=0.213297 err=0.213297
I 2015-05-26 08:54:36 theanets.trainer:168 RmsProp 287 loss=0.212347 err=0.212347
I 2015-05-26 08:54:44 theanets.trainer:168 RmsProp 288 loss=0.220507 err=0.220507
I 2015-05-26 08:54:51 theanets.trainer:168 RmsProp 289 loss=0.209716 err=0.209716
I 2015-05-26 08:54:58 theanets.trainer:168 RmsProp 290 loss=0.201980 err=0.201980
I 2015-05-26 08:54:59 theanets.trainer:168 validation 29 loss=1747.102905 err=1747.102905 *
I 2015-05-26 08:55:06 theanets.trainer:168 RmsProp 291 loss=0.208309 err=0.208309
I 2015-05-26 08:55:13 theanets.trainer:168 RmsProp 292 loss=0.201212 err=0.201212
I 2015-05-26 08:55:22 theanets.trainer:168 RmsProp 293 loss=0.215003 err=0.215003
I 2015-05-26 08:55:30 theanets.trainer:168 RmsProp 294 loss=0.216066 err=0.216066
I 2015-05-26 08:55:38 theanets.trainer:168 RmsProp 295 loss=0.200049 err=0.200049
I 2015-05-26 08:55:45 theanets.trainer:168 RmsProp 296 loss=0.198302 err=0.198302
I 2015-05-26 08:55:52 theanets.trainer:168 RmsProp 297 loss=0.226728 err=0.226728
I 2015-05-26 08:56:00 theanets.trainer:168 RmsProp 298 loss=0.214619 err=0.214619
I 2015-05-26 08:56:07 theanets.trainer:168 RmsProp 299 loss=0.199809 err=0.199809
I 2015-05-26 08:56:14 theanets.trainer:168 RmsProp 300 loss=0.198935 err=0.198935
I 2015-05-26 08:56:15 theanets.trainer:168 validation 30 loss=1746.115234 err=1746.115234 *
I 2015-05-26 08:56:21 theanets.trainer:168 RmsProp 301 loss=0.197297 err=0.197297
I 2015-05-26 08:56:29 theanets.trainer:168 RmsProp 302 loss=0.198994 err=0.198994
I 2015-05-26 08:56:36 theanets.trainer:168 RmsProp 303 loss=0.196494 err=0.196494
I 2015-05-26 08:56:44 theanets.trainer:168 RmsProp 304 loss=0.194270 err=0.194270
I 2015-05-26 08:56:51 theanets.trainer:168 RmsProp 305 loss=0.205045 err=0.205045
I 2015-05-26 08:56:59 theanets.trainer:168 RmsProp 306 loss=0.198099 err=0.198099
I 2015-05-26 08:57:08 theanets.trainer:168 RmsProp 307 loss=0.206507 err=0.206507
I 2015-05-26 08:57:16 theanets.trainer:168 RmsProp 308 loss=0.192813 err=0.192813
I 2015-05-26 08:57:24 theanets.trainer:168 RmsProp 309 loss=0.198476 err=0.198476
I 2015-05-26 08:57:32 theanets.trainer:168 RmsProp 310 loss=0.199265 err=0.199265
I 2015-05-26 08:57:32 theanets.trainer:168 validation 31 loss=1746.000854 err=1746.000854 *
I 2015-05-26 08:57:40 theanets.trainer:168 RmsProp 311 loss=0.192529 err=0.192529
I 2015-05-26 08:57:47 theanets.trainer:168 RmsProp 312 loss=0.194170 err=0.194170
I 2015-05-26 08:57:55 theanets.trainer:168 RmsProp 313 loss=0.193626 err=0.193626
I 2015-05-26 08:58:03 theanets.trainer:168 RmsProp 314 loss=0.198473 err=0.198473
I 2015-05-26 08:58:11 theanets.trainer:168 RmsProp 315 loss=0.189233 err=0.189233
I 2015-05-26 08:58:19 theanets.trainer:168 RmsProp 316 loss=0.193418 err=0.193418
I 2015-05-26 08:58:26 theanets.trainer:168 RmsProp 317 loss=0.190267 err=0.190267
I 2015-05-26 08:58:33 theanets.trainer:168 RmsProp 318 loss=0.197351 err=0.197351
I 2015-05-26 08:58:41 theanets.trainer:168 RmsProp 319 loss=0.188704 err=0.188704
I 2015-05-26 08:58:49 theanets.trainer:168 RmsProp 320 loss=0.191532 err=0.191532
I 2015-05-26 08:58:50 theanets.trainer:168 validation 32 loss=1742.922729 err=1742.922729 *
I 2015-05-26 08:58:57 theanets.trainer:168 RmsProp 321 loss=0.190784 err=0.190784
I 2015-05-26 08:59:04 theanets.trainer:168 RmsProp 322 loss=0.184121 err=0.184121
I 2015-05-26 08:59:12 theanets.trainer:168 RmsProp 323 loss=0.192466 err=0.192466
I 2015-05-26 08:59:19 theanets.trainer:168 RmsProp 324 loss=0.189070 err=0.189070
I 2015-05-26 08:59:27 theanets.trainer:168 RmsProp 325 loss=0.190861 err=0.190861
I 2015-05-26 08:59:34 theanets.trainer:168 RmsProp 326 loss=0.195555 err=0.195555
I 2015-05-26 08:59:42 theanets.trainer:168 RmsProp 327 loss=0.190818 err=0.190818
I 2015-05-26 08:59:49 theanets.trainer:168 RmsProp 328 loss=0.185427 err=0.185427
I 2015-05-26 08:59:56 theanets.trainer:168 RmsProp 329 loss=0.184607 err=0.184607
I 2015-05-26 09:00:04 theanets.trainer:168 RmsProp 330 loss=0.187523 err=0.187523
I 2015-05-26 09:00:05 theanets.trainer:168 validation 33 loss=1743.813843 err=1743.813843
I 2015-05-26 09:00:12 theanets.trainer:168 RmsProp 331 loss=0.189466 err=0.189466
I 2015-05-26 09:00:19 theanets.trainer:168 RmsProp 332 loss=0.190531 err=0.190531
I 2015-05-26 09:00:27 theanets.trainer:168 RmsProp 333 loss=0.177364 err=0.177364
I 2015-05-26 09:00:34 theanets.trainer:168 RmsProp 334 loss=0.197750 err=0.197750
I 2015-05-26 09:00:42 theanets.trainer:168 RmsProp 335 loss=0.175015 err=0.175015
I 2015-05-26 09:00:50 theanets.trainer:168 RmsProp 336 loss=0.191026 err=0.191026
I 2015-05-26 09:00:58 theanets.trainer:168 RmsProp 337 loss=0.178533 err=0.178533
I 2015-05-26 09:01:06 theanets.trainer:168 RmsProp 338 loss=0.184408 err=0.184408
I 2015-05-26 09:01:14 theanets.trainer:168 RmsProp 339 loss=0.186698 err=0.186698
I 2015-05-26 09:01:22 theanets.trainer:168 RmsProp 340 loss=0.175135 err=0.175135
I 2015-05-26 09:01:22 theanets.trainer:168 validation 34 loss=1742.468384 err=1742.468384 *
I 2015-05-26 09:01:30 theanets.trainer:168 RmsProp 341 loss=0.192768 err=0.192768
I 2015-05-26 09:01:38 theanets.trainer:168 RmsProp 342 loss=0.185762 err=0.185762
I 2015-05-26 09:01:45 theanets.trainer:168 RmsProp 343 loss=0.180825 err=0.180825
I 2015-05-26 09:01:53 theanets.trainer:168 RmsProp 344 loss=0.176569 err=0.176569
I 2015-05-26 09:02:01 theanets.trainer:168 RmsProp 345 loss=0.178135 err=0.178135
I 2015-05-26 09:02:08 theanets.trainer:168 RmsProp 346 loss=0.178682 err=0.178682
I 2015-05-26 09:02:16 theanets.trainer:168 RmsProp 347 loss=0.179823 err=0.179823
I 2015-05-26 09:02:24 theanets.trainer:168 RmsProp 348 loss=0.178127 err=0.178127
I 2015-05-26 09:02:32 theanets.trainer:168 RmsProp 349 loss=0.183022 err=0.183022
I 2015-05-26 09:02:39 theanets.trainer:168 RmsProp 350 loss=0.172667 err=0.172667
I 2015-05-26 09:02:39 theanets.trainer:168 validation 35 loss=1742.924805 err=1742.924805
I 2015-05-26 09:02:47 theanets.trainer:168 RmsProp 351 loss=0.177902 err=0.177902
I 2015-05-26 09:02:54 theanets.trainer:168 RmsProp 352 loss=0.174113 err=0.174113
I 2015-05-26 09:03:00 theanets.trainer:168 RmsProp 353 loss=0.182779 err=0.182779
I 2015-05-26 09:03:09 theanets.trainer:168 RmsProp 354 loss=0.177917 err=0.177917
I 2015-05-26 09:03:17 theanets.trainer:168 RmsProp 355 loss=0.170254 err=0.170254
I 2015-05-26 09:03:25 theanets.trainer:168 RmsProp 356 loss=0.175129 err=0.175129
I 2015-05-26 09:03:33 theanets.trainer:168 RmsProp 357 loss=0.174002 err=0.174002
I 2015-05-26 09:03:40 theanets.trainer:168 RmsProp 358 loss=0.175428 err=0.175428
I 2015-05-26 09:03:48 theanets.trainer:168 RmsProp 359 loss=0.178040 err=0.178040
I 2015-05-26 09:03:56 theanets.trainer:168 RmsProp 360 loss=0.176633 err=0.176633
I 2015-05-26 09:03:56 theanets.trainer:168 validation 36 loss=1741.214111 err=1741.214111 *
I 2015-05-26 09:04:03 theanets.trainer:168 RmsProp 361 loss=0.168472 err=0.168472
I 2015-05-26 09:04:11 theanets.trainer:168 RmsProp 362 loss=0.183526 err=0.183526
I 2015-05-26 09:04:19 theanets.trainer:168 RmsProp 363 loss=0.170513 err=0.170513
I 2015-05-26 09:04:27 theanets.trainer:168 RmsProp 364 loss=0.179424 err=0.179424
I 2015-05-26 09:04:35 theanets.trainer:168 RmsProp 365 loss=0.176488 err=0.176488
I 2015-05-26 09:04:43 theanets.trainer:168 RmsProp 366 loss=0.162418 err=0.162418
I 2015-05-26 09:04:51 theanets.trainer:168 RmsProp 367 loss=0.178721 err=0.178721
I 2015-05-26 09:04:59 theanets.trainer:168 RmsProp 368 loss=0.167126 err=0.167126
I 2015-05-26 09:05:06 theanets.trainer:168 RmsProp 369 loss=0.171139 err=0.171139
I 2015-05-26 09:05:15 theanets.trainer:168 RmsProp 370 loss=0.166750 err=0.166750
I 2015-05-26 09:05:15 theanets.trainer:168 validation 37 loss=1742.391602 err=1742.391602
I 2015-05-26 09:05:23 theanets.trainer:168 RmsProp 371 loss=0.169159 err=0.169159
I 2015-05-26 09:05:31 theanets.trainer:168 RmsProp 372 loss=0.173464 err=0.173464
I 2015-05-26 09:05:39 theanets.trainer:168 RmsProp 373 loss=0.168627 err=0.168627
I 2015-05-26 09:05:47 theanets.trainer:168 RmsProp 374 loss=0.173263 err=0.173263
I 2015-05-26 09:05:55 theanets.trainer:168 RmsProp 375 loss=0.168936 err=0.168936
I 2015-05-26 09:06:02 theanets.trainer:168 RmsProp 376 loss=0.162849 err=0.162849
I 2015-05-26 09:06:09 theanets.trainer:168 RmsProp 377 loss=0.173100 err=0.173100
I 2015-05-26 09:06:16 theanets.trainer:168 RmsProp 378 loss=0.170225 err=0.170225
I 2015-05-26 09:06:23 theanets.trainer:168 RmsProp 379 loss=0.167328 err=0.167328
I 2015-05-26 09:06:30 theanets.trainer:168 RmsProp 380 loss=0.171390 err=0.171390
I 2015-05-26 09:06:31 theanets.trainer:168 validation 38 loss=1740.340698 err=1740.340698 *
I 2015-05-26 09:06:38 theanets.trainer:168 RmsProp 381 loss=0.162972 err=0.162972
I 2015-05-26 09:06:45 theanets.trainer:168 RmsProp 382 loss=0.165427 err=0.165427
I 2015-05-26 09:06:53 theanets.trainer:168 RmsProp 383 loss=0.165609 err=0.165609
I 2015-05-26 09:07:01 theanets.trainer:168 RmsProp 384 loss=0.166193 err=0.166193
I 2015-05-26 09:07:09 theanets.trainer:168 RmsProp 385 loss=0.167909 err=0.167909
I 2015-05-26 09:07:17 theanets.trainer:168 RmsProp 386 loss=0.168258 err=0.168258
I 2015-05-26 09:07:24 theanets.trainer:168 RmsProp 387 loss=0.161740 err=0.161740
I 2015-05-26 09:07:32 theanets.trainer:168 RmsProp 388 loss=0.168718 err=0.168718
I 2015-05-26 09:07:39 theanets.trainer:168 RmsProp 389 loss=0.161559 err=0.161559
I 2015-05-26 09:07:47 theanets.trainer:168 RmsProp 390 loss=0.163657 err=0.163657
I 2015-05-26 09:07:48 theanets.trainer:168 validation 39 loss=1738.912720 err=1738.912720 *
I 2015-05-26 09:07:55 theanets.trainer:168 RmsProp 391 loss=0.172669 err=0.172669
I 2015-05-26 09:08:03 theanets.trainer:168 RmsProp 392 loss=0.161341 err=0.161341
I 2015-05-26 09:08:10 theanets.trainer:168 RmsProp 393 loss=0.171281 err=0.171281
I 2015-05-26 09:08:17 theanets.trainer:168 RmsProp 394 loss=0.170571 err=0.170571
I 2015-05-26 09:08:24 theanets.trainer:168 RmsProp 395 loss=0.155153 err=0.155153
I 2015-05-26 09:08:32 theanets.trainer:168 RmsProp 396 loss=0.155299 err=0.155299
I 2015-05-26 09:08:41 theanets.trainer:168 RmsProp 397 loss=0.189580 err=0.189580
I 2015-05-26 09:08:49 theanets.trainer:168 RmsProp 398 loss=0.168149 err=0.168149
I 2015-05-26 09:08:56 theanets.trainer:168 RmsProp 399 loss=0.151734 err=0.151734
I 2015-05-26 09:09:04 theanets.trainer:168 RmsProp 400 loss=0.160049 err=0.160049
I 2015-05-26 09:09:04 theanets.trainer:168 validation 40 loss=1740.472900 err=1740.472900
I 2015-05-26 09:09:11 theanets.trainer:168 RmsProp 401 loss=0.163541 err=0.163541
I 2015-05-26 09:09:19 theanets.trainer:168 RmsProp 402 loss=0.153485 err=0.153485
I 2015-05-26 09:09:27 theanets.trainer:168 RmsProp 403 loss=0.170803 err=0.170803
I 2015-05-26 09:09:35 theanets.trainer:168 RmsProp 404 loss=0.154084 err=0.154084
I 2015-05-26 09:09:42 theanets.trainer:168 RmsProp 405 loss=0.170446 err=0.170446
I 2015-05-26 09:09:50 theanets.trainer:168 RmsProp 406 loss=0.160402 err=0.160402
I 2015-05-26 09:09:57 theanets.trainer:168 RmsProp 407 loss=0.155620 err=0.155620
I 2015-05-26 09:10:05 theanets.trainer:168 RmsProp 408 loss=0.160978 err=0.160978
I 2015-05-26 09:10:12 theanets.trainer:168 RmsProp 409 loss=0.161823 err=0.161823
I 2015-05-26 09:10:19 theanets.trainer:168 RmsProp 410 loss=0.157183 err=0.157183
I 2015-05-26 09:10:20 theanets.trainer:168 validation 41 loss=1739.142578 err=1739.142578
I 2015-05-26 09:10:27 theanets.trainer:168 RmsProp 411 loss=0.157774 err=0.157774
I 2015-05-26 09:10:34 theanets.trainer:168 RmsProp 412 loss=0.169774 err=0.169774
I 2015-05-26 09:10:42 theanets.trainer:168 RmsProp 413 loss=0.153661 err=0.153661
I 2015-05-26 09:10:51 theanets.trainer:168 RmsProp 414 loss=0.153035 err=0.153035
I 2015-05-26 09:10:59 theanets.trainer:168 RmsProp 415 loss=0.153866 err=0.153866
I 2015-05-26 09:11:06 theanets.trainer:168 RmsProp 416 loss=0.150610 err=0.150610
I 2015-05-26 09:11:13 theanets.trainer:168 RmsProp 417 loss=0.163521 err=0.163521
I 2015-05-26 09:11:20 theanets.trainer:168 RmsProp 418 loss=0.160358 err=0.160358
I 2015-05-26 09:11:26 theanets.trainer:168 RmsProp 419 loss=0.157504 err=0.157504
I 2015-05-26 09:11:33 theanets.trainer:168 RmsProp 420 loss=0.153889 err=0.153889
I 2015-05-26 09:11:33 theanets.trainer:168 validation 42 loss=1738.141479 err=1738.141479 *
I 2015-05-26 09:11:39 theanets.trainer:168 RmsProp 421 loss=0.157029 err=0.157029
I 2015-05-26 09:11:45 theanets.trainer:168 RmsProp 422 loss=0.154387 err=0.154387
I 2015-05-26 09:11:51 theanets.trainer:168 RmsProp 423 loss=0.151589 err=0.151589
I 2015-05-26 09:11:57 theanets.trainer:168 RmsProp 424 loss=0.155960 err=0.155960
I 2015-05-26 09:12:03 theanets.trainer:168 RmsProp 425 loss=0.153418 err=0.153418
I 2015-05-26 09:12:09 theanets.trainer:168 RmsProp 426 loss=0.154414 err=0.154414
I 2015-05-26 09:12:15 theanets.trainer:168 RmsProp 427 loss=0.148820 err=0.148820
I 2015-05-26 09:12:22 theanets.trainer:168 RmsProp 428 loss=0.153138 err=0.153138
I 2015-05-26 09:12:28 theanets.trainer:168 RmsProp 429 loss=0.160979 err=0.160979
I 2015-05-26 09:12:34 theanets.trainer:168 RmsProp 430 loss=0.154751 err=0.154751
I 2015-05-26 09:12:34 theanets.trainer:168 validation 43 loss=1738.359375 err=1738.359375
I 2015-05-26 09:12:41 theanets.trainer:168 RmsProp 431 loss=0.144449 err=0.144449
I 2015-05-26 09:12:47 theanets.trainer:168 RmsProp 432 loss=0.159654 err=0.159654
I 2015-05-26 09:12:54 theanets.trainer:168 RmsProp 433 loss=0.146651 err=0.146651
I 2015-05-26 09:13:01 theanets.trainer:168 RmsProp 434 loss=0.147936 err=0.147936
I 2015-05-26 09:13:08 theanets.trainer:168 RmsProp 435 loss=0.159275 err=0.159275
I 2015-05-26 09:13:15 theanets.trainer:168 RmsProp 436 loss=0.151781 err=0.151781
I 2015-05-26 09:13:22 theanets.trainer:168 RmsProp 437 loss=0.146624 err=0.146624
I 2015-05-26 09:13:28 theanets.trainer:168 RmsProp 438 loss=0.160951 err=0.160951
I 2015-05-26 09:13:34 theanets.trainer:168 RmsProp 439 loss=0.149151 err=0.149151
I 2015-05-26 09:13:40 theanets.trainer:168 RmsProp 440 loss=0.145880 err=0.145880
I 2015-05-26 09:13:40 theanets.trainer:168 validation 44 loss=1737.941040 err=1737.941040 *
I 2015-05-26 09:13:47 theanets.trainer:168 RmsProp 441 loss=0.148891 err=0.148891
I 2015-05-26 09:13:53 theanets.trainer:168 RmsProp 442 loss=0.156122 err=0.156122
I 2015-05-26 09:13:59 theanets.trainer:168 RmsProp 443 loss=0.147553 err=0.147553
I 2015-05-26 09:14:05 theanets.trainer:168 RmsProp 444 loss=0.152334 err=0.152334
I 2015-05-26 09:14:10 theanets.trainer:168 RmsProp 445 loss=0.151141 err=0.151141
I 2015-05-26 09:14:16 theanets.trainer:168 RmsProp 446 loss=0.149849 err=0.149849
I 2015-05-26 09:14:22 theanets.trainer:168 RmsProp 447 loss=0.148418 err=0.148418
I 2015-05-26 09:14:27 theanets.trainer:168 RmsProp 448 loss=0.147297 err=0.147297
I 2015-05-26 09:14:32 theanets.trainer:168 RmsProp 449 loss=0.145435 err=0.145435
I 2015-05-26 09:14:38 theanets.trainer:168 RmsProp 450 loss=0.143438 err=0.143438
I 2015-05-26 09:14:38 theanets.trainer:168 validation 45 loss=1737.599487 err=1737.599487 *
I 2015-05-26 09:14:43 theanets.trainer:168 RmsProp 451 loss=0.151699 err=0.151699
I 2015-05-26 09:14:49 theanets.trainer:168 RmsProp 452 loss=0.146308 err=0.146308
I 2015-05-26 09:14:54 theanets.trainer:168 RmsProp 453 loss=0.139128 err=0.139128
I 2015-05-26 09:14:59 theanets.trainer:168 RmsProp 454 loss=0.159216 err=0.159216
I 2015-05-26 09:15:04 theanets.trainer:168 RmsProp 455 loss=0.155856 err=0.155856
I 2015-05-26 09:15:09 theanets.trainer:168 RmsProp 456 loss=0.144274 err=0.144274
I 2015-05-26 09:15:15 theanets.trainer:168 RmsProp 457 loss=0.141254 err=0.141254
I 2015-05-26 09:15:20 theanets.trainer:168 RmsProp 458 loss=0.145563 err=0.145563
I 2015-05-26 09:15:25 theanets.trainer:168 RmsProp 459 loss=0.146526 err=0.146526
I 2015-05-26 09:15:31 theanets.trainer:168 RmsProp 460 loss=0.143615 err=0.143615
I 2015-05-26 09:15:32 theanets.trainer:168 validation 46 loss=1736.796875 err=1736.796875 *
I 2015-05-26 09:15:38 theanets.trainer:168 RmsProp 461 loss=0.152409 err=0.152409
I 2015-05-26 09:15:44 theanets.trainer:168 RmsProp 462 loss=0.146667 err=0.146667
I 2015-05-26 09:15:50 theanets.trainer:168 RmsProp 463 loss=0.143829 err=0.143829
I 2015-05-26 09:15:57 theanets.trainer:168 RmsProp 464 loss=0.141195 err=0.141195
I 2015-05-26 09:16:04 theanets.trainer:168 RmsProp 465 loss=0.137062 err=0.137062
I 2015-05-26 09:16:11 theanets.trainer:168 RmsProp 466 loss=0.148540 err=0.148540
I 2015-05-26 09:16:17 theanets.trainer:168 RmsProp 467 loss=0.139491 err=0.139491
I 2015-05-26 09:16:24 theanets.trainer:168 RmsProp 468 loss=0.141168 err=0.141168
I 2015-05-26 09:16:30 theanets.trainer:168 RmsProp 469 loss=0.140782 err=0.140782
I 2015-05-26 09:16:37 theanets.trainer:168 RmsProp 470 loss=0.140864 err=0.140864
I 2015-05-26 09:16:38 theanets.trainer:168 validation 47 loss=1735.499023 err=1735.499023 *
I 2015-05-26 09:16:43 theanets.trainer:168 RmsProp 471 loss=0.131883 err=0.131883
I 2015-05-26 09:16:50 theanets.trainer:168 RmsProp 472 loss=0.166600 err=0.166600
I 2015-05-26 09:16:56 theanets.trainer:168 RmsProp 473 loss=0.138987 err=0.138987
I 2015-05-26 09:17:02 theanets.trainer:168 RmsProp 474 loss=0.134351 err=0.134351
I 2015-05-26 09:17:08 theanets.trainer:168 RmsProp 475 loss=0.143093 err=0.143093
I 2015-05-26 09:17:14 theanets.trainer:168 RmsProp 476 loss=0.142707 err=0.142707
I 2015-05-26 09:17:20 theanets.trainer:168 RmsProp 477 loss=0.140364 err=0.140364
I 2015-05-26 09:17:26 theanets.trainer:168 RmsProp 478 loss=0.146781 err=0.146781
I 2015-05-26 09:17:32 theanets.trainer:168 RmsProp 479 loss=0.129544 err=0.129544
I 2015-05-26 09:17:38 theanets.trainer:168 RmsProp 480 loss=0.157784 err=0.157784
I 2015-05-26 09:17:39 theanets.trainer:168 validation 48 loss=1735.004883 err=1735.004883 *
I 2015-05-26 09:17:45 theanets.trainer:168 RmsProp 481 loss=0.142522 err=0.142522
I 2015-05-26 09:17:51 theanets.trainer:168 RmsProp 482 loss=0.133696 err=0.133696
I 2015-05-26 09:17:58 theanets.trainer:168 RmsProp 483 loss=0.148701 err=0.148701
I 2015-05-26 09:18:03 theanets.trainer:168 RmsProp 484 loss=0.140155 err=0.140155
I 2015-05-26 09:18:10 theanets.trainer:168 RmsProp 485 loss=0.130832 err=0.130832
I 2015-05-26 09:18:16 theanets.trainer:168 RmsProp 486 loss=0.142764 err=0.142764
I 2015-05-26 09:18:22 theanets.trainer:168 RmsProp 487 loss=0.134931 err=0.134931
I 2015-05-26 09:18:28 theanets.trainer:168 RmsProp 488 loss=0.149436 err=0.149436
I 2015-05-26 09:18:34 theanets.trainer:168 RmsProp 489 loss=0.138258 err=0.138258
I 2015-05-26 09:18:41 theanets.trainer:168 RmsProp 490 loss=0.135903 err=0.135903
I 2015-05-26 09:18:42 theanets.trainer:168 validation 49 loss=1734.042603 err=1734.042603 *
I 2015-05-26 09:18:48 theanets.trainer:168 RmsProp 491 loss=0.135366 err=0.135366
I 2015-05-26 09:18:55 theanets.trainer:168 RmsProp 492 loss=0.139504 err=0.139504
I 2015-05-26 09:19:02 theanets.trainer:168 RmsProp 493 loss=0.136488 err=0.136488
I 2015-05-26 09:19:08 theanets.trainer:168 RmsProp 494 loss=0.137518 err=0.137518
I 2015-05-26 09:19:15 theanets.trainer:168 RmsProp 495 loss=0.131290 err=0.131290
I 2015-05-26 09:19:21 theanets.trainer:168 RmsProp 496 loss=0.140729 err=0.140729
I 2015-05-26 09:19:27 theanets.trainer:168 RmsProp 497 loss=0.129996 err=0.129996
I 2015-05-26 09:19:33 theanets.trainer:168 RmsProp 498 loss=0.144656 err=0.144656
I 2015-05-26 09:19:39 theanets.trainer:168 RmsProp 499 loss=0.135484 err=0.135484
I 2015-05-26 09:19:46 theanets.trainer:168 RmsProp 500 loss=0.128362 err=0.128362
I 2015-05-26 09:19:46 theanets.trainer:168 validation 50 loss=1735.159180 err=1735.159180
I 2015-05-26 09:19:52 theanets.trainer:168 RmsProp 501 loss=0.137819 err=0.137819
I 2015-05-26 09:19:59 theanets.trainer:168 RmsProp 502 loss=0.133331 err=0.133331
I 2015-05-26 09:20:05 theanets.trainer:168 RmsProp 503 loss=0.149381 err=0.149381
I 2015-05-26 09:20:11 theanets.trainer:168 RmsProp 504 loss=0.135047 err=0.135047
I 2015-05-26 09:20:17 theanets.trainer:168 RmsProp 505 loss=0.130758 err=0.130758
I 2015-05-26 09:20:24 theanets.trainer:168 RmsProp 506 loss=0.127964 err=0.127964
I 2015-05-26 09:20:30 theanets.trainer:168 RmsProp 507 loss=0.143269 err=0.143269
I 2015-05-26 09:20:36 theanets.trainer:168 RmsProp 508 loss=0.132694 err=0.132694
I 2015-05-26 09:20:43 theanets.trainer:168 RmsProp 509 loss=0.129674 err=0.129674
I 2015-05-26 09:20:49 theanets.trainer:168 RmsProp 510 loss=0.132504 err=0.132504
I 2015-05-26 09:20:49 theanets.trainer:168 validation 51 loss=1733.391479 err=1733.391479 *
I 2015-05-26 09:20:55 theanets.trainer:168 RmsProp 511 loss=0.134710 err=0.134710
I 2015-05-26 09:21:01 theanets.trainer:168 RmsProp 512 loss=0.134113 err=0.134113
I 2015-05-26 09:21:08 theanets.trainer:168 RmsProp 513 loss=0.139682 err=0.139682
I 2015-05-26 09:21:15 theanets.trainer:168 RmsProp 514 loss=0.134801 err=0.134801
I 2015-05-26 09:21:22 theanets.trainer:168 RmsProp 515 loss=0.132390 err=0.132390
I 2015-05-26 09:21:28 theanets.trainer:168 RmsProp 516 loss=0.134492 err=0.134492
I 2015-05-26 09:21:35 theanets.trainer:168 RmsProp 517 loss=0.130402 err=0.130402
I 2015-05-26 09:21:42 theanets.trainer:168 RmsProp 518 loss=0.130532 err=0.130532
I 2015-05-26 09:21:47 theanets.trainer:168 RmsProp 519 loss=0.130697 err=0.130697
I 2015-05-26 09:21:53 theanets.trainer:168 RmsProp 520 loss=0.128433 err=0.128433
I 2015-05-26 09:21:53 theanets.trainer:168 validation 52 loss=1731.981323 err=1731.981323 *
I 2015-05-26 09:21:58 theanets.trainer:168 RmsProp 521 loss=0.137460 err=0.137460
I 2015-05-26 09:22:04 theanets.trainer:168 RmsProp 522 loss=0.126884 err=0.126884
I 2015-05-26 09:22:08 theanets.trainer:168 RmsProp 523 loss=0.137376 err=0.137376
I 2015-05-26 09:22:13 theanets.trainer:168 RmsProp 524 loss=0.133457 err=0.133457
I 2015-05-26 09:22:19 theanets.trainer:168 RmsProp 525 loss=0.128402 err=0.128402
I 2015-05-26 09:22:24 theanets.trainer:168 RmsProp 526 loss=0.124580 err=0.124580
I 2015-05-26 09:22:30 theanets.trainer:168 RmsProp 527 loss=0.134623 err=0.134623
I 2015-05-26 09:22:35 theanets.trainer:168 RmsProp 528 loss=0.127899 err=0.127899
I 2015-05-26 09:22:40 theanets.trainer:168 RmsProp 529 loss=0.140795 err=0.140795
I 2015-05-26 09:22:46 theanets.trainer:168 RmsProp 530 loss=0.124194 err=0.124194
I 2015-05-26 09:22:46 theanets.trainer:168 validation 53 loss=1732.342773 err=1732.342773
I 2015-05-26 09:22:51 theanets.trainer:168 RmsProp 531 loss=0.132064 err=0.132064
I 2015-05-26 09:22:55 theanets.trainer:168 RmsProp 532 loss=0.124542 err=0.124542
I 2015-05-26 09:22:59 theanets.trainer:168 RmsProp 533 loss=0.137596 err=0.137596
I 2015-05-26 09:23:03 theanets.trainer:168 RmsProp 534 loss=0.126102 err=0.126102
I 2015-05-26 09:23:07 theanets.trainer:168 RmsProp 535 loss=0.133195 err=0.133195
I 2015-05-26 09:23:11 theanets.trainer:168 RmsProp 536 loss=0.125181 err=0.125181
I 2015-05-26 09:23:15 theanets.trainer:168 RmsProp 537 loss=0.127558 err=0.127558
I 2015-05-26 09:23:19 theanets.trainer:168 RmsProp 538 loss=0.135191 err=0.135191
I 2015-05-26 09:23:23 theanets.trainer:168 RmsProp 539 loss=0.127466 err=0.127466
I 2015-05-26 09:23:27 theanets.trainer:168 RmsProp 540 loss=0.125233 err=0.125233
I 2015-05-26 09:23:27 theanets.trainer:168 validation 54 loss=1732.722046 err=1732.722046
I 2015-05-26 09:23:31 theanets.trainer:168 RmsProp 541 loss=0.128494 err=0.128494
I 2015-05-26 09:23:35 theanets.trainer:168 RmsProp 542 loss=0.127409 err=0.127409
I 2015-05-26 09:23:39 theanets.trainer:168 RmsProp 543 loss=0.121735 err=0.121735
I 2015-05-26 09:23:43 theanets.trainer:168 RmsProp 544 loss=0.142735 err=0.142735
I 2015-05-26 09:23:48 theanets.trainer:168 RmsProp 545 loss=0.142948 err=0.142948
I 2015-05-26 09:23:52 theanets.trainer:168 RmsProp 546 loss=0.120404 err=0.120404
I 2015-05-26 09:23:56 theanets.trainer:168 RmsProp 547 loss=0.115275 err=0.115275
I 2015-05-26 09:24:00 theanets.trainer:168 RmsProp 548 loss=0.144544 err=0.144544
I 2015-05-26 09:24:04 theanets.trainer:168 RmsProp 549 loss=0.124182 err=0.124182
I 2015-05-26 09:24:08 theanets.trainer:168 RmsProp 550 loss=0.115176 err=0.115176
I 2015-05-26 09:24:09 theanets.trainer:168 validation 55 loss=1732.360352 err=1732.360352
I 2015-05-26 09:24:13 theanets.trainer:168 RmsProp 551 loss=0.144748 err=0.144748
I 2015-05-26 09:24:17 theanets.trainer:168 RmsProp 552 loss=0.132262 err=0.132262
I 2015-05-26 09:24:21 theanets.trainer:168 RmsProp 553 loss=0.117019 err=0.117019
I 2015-05-26 09:24:26 theanets.trainer:168 RmsProp 554 loss=0.127528 err=0.127528
I 2015-05-26 09:24:30 theanets.trainer:168 RmsProp 555 loss=0.127574 err=0.127574
I 2015-05-26 09:24:35 theanets.trainer:168 RmsProp 556 loss=0.123384 err=0.123384
I 2015-05-26 09:24:39 theanets.trainer:168 RmsProp 557 loss=0.122653 err=0.122653
I 2015-05-26 09:24:43 theanets.trainer:168 RmsProp 558 loss=0.119642 err=0.119642
I 2015-05-26 09:24:47 theanets.trainer:168 RmsProp 559 loss=0.129983 err=0.129983
I 2015-05-26 09:24:51 theanets.trainer:168 RmsProp 560 loss=0.125965 err=0.125965
I 2015-05-26 09:24:52 theanets.trainer:168 validation 56 loss=1731.739624 err=1731.739624 *
I 2015-05-26 09:24:56 theanets.trainer:168 RmsProp 561 loss=0.117062 err=0.117062
I 2015-05-26 09:25:01 theanets.trainer:168 RmsProp 562 loss=0.130278 err=0.130278
I 2015-05-26 09:25:05 theanets.trainer:168 RmsProp 563 loss=0.125211 err=0.125211
I 2015-05-26 09:25:09 theanets.trainer:168 RmsProp 564 loss=0.124074 err=0.124074
I 2015-05-26 09:25:13 theanets.trainer:168 RmsProp 565 loss=0.116492 err=0.116492
I 2015-05-26 09:25:17 theanets.trainer:168 RmsProp 566 loss=0.143493 err=0.143493
I 2015-05-26 09:25:21 theanets.trainer:168 RmsProp 567 loss=0.127205 err=0.127205
I 2015-05-26 09:25:25 theanets.trainer:168 RmsProp 568 loss=0.117792 err=0.117792
I 2015-05-26 09:25:30 theanets.trainer:168 RmsProp 569 loss=0.128579 err=0.128579
I 2015-05-26 09:25:34 theanets.trainer:168 RmsProp 570 loss=0.117322 err=0.117322
I 2015-05-26 09:25:34 theanets.trainer:168 validation 57 loss=1731.438354 err=1731.438354 *
I 2015-05-26 09:25:39 theanets.trainer:168 RmsProp 571 loss=0.119213 err=0.119213
I 2015-05-26 09:25:43 theanets.trainer:168 RmsProp 572 loss=0.123570 err=0.123570
I 2015-05-26 09:25:47 theanets.trainer:168 RmsProp 573 loss=0.126945 err=0.126945
I 2015-05-26 09:25:51 theanets.trainer:168 RmsProp 574 loss=0.127351 err=0.127351
I 2015-05-26 09:25:55 theanets.trainer:168 RmsProp 575 loss=0.115608 err=0.115608
I 2015-05-26 09:25:59 theanets.trainer:168 RmsProp 576 loss=0.126227 err=0.126227
I 2015-05-26 09:26:03 theanets.trainer:168 RmsProp 577 loss=0.120059 err=0.120059
I 2015-05-26 09:26:07 theanets.trainer:168 RmsProp 578 loss=0.125552 err=0.125552
I 2015-05-26 09:26:11 theanets.trainer:168 RmsProp 579 loss=0.118272 err=0.118272
I 2015-05-26 09:26:15 theanets.trainer:168 RmsProp 580 loss=0.113941 err=0.113941
I 2015-05-26 09:26:16 theanets.trainer:168 validation 58 loss=1730.975464 err=1730.975464 *
I 2015-05-26 09:26:20 theanets.trainer:168 RmsProp 581 loss=0.129551 err=0.129551
I 2015-05-26 09:26:24 theanets.trainer:168 RmsProp 582 loss=0.125014 err=0.125014
I 2015-05-26 09:26:28 theanets.trainer:168 RmsProp 583 loss=0.117779 err=0.117779
I 2015-05-26 09:26:33 theanets.trainer:168 RmsProp 584 loss=0.123411 err=0.123411
I 2015-05-26 09:26:37 theanets.trainer:168 RmsProp 585 loss=0.116994 err=0.116994
I 2015-05-26 09:26:41 theanets.trainer:168 RmsProp 586 loss=0.124656 err=0.124656
I 2015-05-26 09:26:45 theanets.trainer:168 RmsProp 587 loss=0.118287 err=0.118287
I 2015-05-26 09:26:49 theanets.trainer:168 RmsProp 588 loss=0.123521 err=0.123521
I 2015-05-26 09:26:53 theanets.trainer:168 RmsProp 589 loss=0.120183 err=0.120183
I 2015-05-26 09:26:58 theanets.trainer:168 RmsProp 590 loss=0.117823 err=0.117823
I 2015-05-26 09:26:58 theanets.trainer:168 validation 59 loss=1731.302734 err=1731.302734
I 2015-05-26 09:27:02 theanets.trainer:168 RmsProp 591 loss=0.121398 err=0.121398
I 2015-05-26 09:27:07 theanets.trainer:168 RmsProp 592 loss=0.124748 err=0.124748
I 2015-05-26 09:27:11 theanets.trainer:168 RmsProp 593 loss=0.120939 err=0.120939
I 2015-05-26 09:27:16 theanets.trainer:168 RmsProp 594 loss=0.115499 err=0.115499
I 2015-05-26 09:27:20 theanets.trainer:168 RmsProp 595 loss=0.117885 err=0.117885
I 2015-05-26 09:27:24 theanets.trainer:168 RmsProp 596 loss=0.115557 err=0.115557
I 2015-05-26 09:27:28 theanets.trainer:168 RmsProp 597 loss=0.117167 err=0.117167
I 2015-05-26 09:27:32 theanets.trainer:168 RmsProp 598 loss=0.113148 err=0.113148
I 2015-05-26 09:27:36 theanets.trainer:168 RmsProp 599 loss=0.118494 err=0.118494
I 2015-05-26 09:27:41 theanets.trainer:168 RmsProp 600 loss=0.108646 err=0.108646
I 2015-05-26 09:27:41 theanets.trainer:168 validation 60 loss=1730.146484 err=1730.146484 *
I 2015-05-26 09:27:45 theanets.trainer:168 RmsProp 601 loss=0.128459 err=0.128459
I 2015-05-26 09:27:49 theanets.trainer:168 RmsProp 602 loss=0.114466 err=0.114466
I 2015-05-26 09:27:53 theanets.trainer:168 RmsProp 603 loss=0.122385 err=0.122385
I 2015-05-26 09:27:58 theanets.trainer:168 RmsProp 604 loss=0.110690 err=0.110690
I 2015-05-26 09:28:02 theanets.trainer:168 RmsProp 605 loss=0.124366 err=0.124366
I 2015-05-26 09:28:06 theanets.trainer:168 RmsProp 606 loss=0.116778 err=0.116778
I 2015-05-26 09:28:10 theanets.trainer:168 RmsProp 607 loss=0.122538 err=0.122538
I 2015-05-26 09:28:14 theanets.trainer:168 RmsProp 608 loss=0.115890 err=0.115890
I 2015-05-26 09:28:18 theanets.trainer:168 RmsProp 609 loss=0.108139 err=0.108139
I 2015-05-26 09:28:22 theanets.trainer:168 RmsProp 610 loss=0.120803 err=0.120803
I 2015-05-26 09:28:22 theanets.trainer:168 validation 61 loss=1728.036377 err=1728.036377 *
I 2015-05-26 09:28:27 theanets.trainer:168 RmsProp 611 loss=0.116201 err=0.116201
I 2015-05-26 09:28:31 theanets.trainer:168 RmsProp 612 loss=0.109767 err=0.109767
I 2015-05-26 09:28:36 theanets.trainer:168 RmsProp 613 loss=0.120305 err=0.120305
I 2015-05-26 09:28:40 theanets.trainer:168 RmsProp 614 loss=0.120217 err=0.120217
I 2015-05-26 09:28:44 theanets.trainer:168 RmsProp 615 loss=0.115694 err=0.115694
I 2015-05-26 09:28:48 theanets.trainer:168 RmsProp 616 loss=0.111004 err=0.111004
I 2015-05-26 09:28:52 theanets.trainer:168 RmsProp 617 loss=0.122833 err=0.122833
I 2015-05-26 09:28:57 theanets.trainer:168 RmsProp 618 loss=0.119737 err=0.119737
I 2015-05-26 09:29:02 theanets.trainer:168 RmsProp 619 loss=0.111982 err=0.111982
I 2015-05-26 09:29:06 theanets.trainer:168 RmsProp 620 loss=0.114570 err=0.114570
I 2015-05-26 09:29:06 theanets.trainer:168 validation 62 loss=1728.291016 err=1728.291016
I 2015-05-26 09:29:11 theanets.trainer:168 RmsProp 621 loss=0.113944 err=0.113944
I 2015-05-26 09:29:15 theanets.trainer:168 RmsProp 622 loss=0.114173 err=0.114173
I 2015-05-26 09:29:19 theanets.trainer:168 RmsProp 623 loss=0.114201 err=0.114201
I 2015-05-26 09:29:22 theanets.trainer:168 RmsProp 624 loss=0.114855 err=0.114855
I 2015-05-26 09:29:26 theanets.trainer:168 RmsProp 625 loss=0.109169 err=0.109169
I 2015-05-26 09:29:31 theanets.trainer:168 RmsProp 626 loss=0.120848 err=0.120848
I 2015-05-26 09:29:35 theanets.trainer:168 RmsProp 627 loss=0.117206 err=0.117206
I 2015-05-26 09:29:39 theanets.trainer:168 RmsProp 628 loss=0.116638 err=0.116638
I 2015-05-26 09:29:44 theanets.trainer:168 RmsProp 629 loss=0.110800 err=0.110800
I 2015-05-26 09:29:49 theanets.trainer:168 RmsProp 630 loss=0.110400 err=0.110400
I 2015-05-26 09:29:49 theanets.trainer:168 validation 63 loss=1728.309448 err=1728.309448
I 2015-05-26 09:29:53 theanets.trainer:168 RmsProp 631 loss=0.114161 err=0.114161
I 2015-05-26 09:29:57 theanets.trainer:168 RmsProp 632 loss=0.115900 err=0.115900
I 2015-05-26 09:30:01 theanets.trainer:168 RmsProp 633 loss=0.115151 err=0.115151
I 2015-05-26 09:30:05 theanets.trainer:168 RmsProp 634 loss=0.110118 err=0.110118
I 2015-05-26 09:30:09 theanets.trainer:168 RmsProp 635 loss=0.116987 err=0.116987
I 2015-05-26 09:30:13 theanets.trainer:168 RmsProp 636 loss=0.119637 err=0.119637
I 2015-05-26 09:30:17 theanets.trainer:168 RmsProp 637 loss=0.113523 err=0.113523
I 2015-05-26 09:30:21 theanets.trainer:168 RmsProp 638 loss=0.118531 err=0.118531
I 2015-05-26 09:30:25 theanets.trainer:168 RmsProp 639 loss=0.111774 err=0.111774
I 2015-05-26 09:30:29 theanets.trainer:168 RmsProp 640 loss=0.115233 err=0.115233
I 2015-05-26 09:30:29 theanets.trainer:168 validation 64 loss=1728.386597 err=1728.386597
I 2015-05-26 09:30:33 theanets.trainer:168 RmsProp 641 loss=0.113797 err=0.113797
I 2015-05-26 09:30:38 theanets.trainer:168 RmsProp 642 loss=0.110736 err=0.110736
I 2015-05-26 09:30:42 theanets.trainer:168 RmsProp 643 loss=0.111228 err=0.111228
I 2015-05-26 09:30:46 theanets.trainer:168 RmsProp 644 loss=0.111266 err=0.111266
I 2015-05-26 09:30:51 theanets.trainer:168 RmsProp 645 loss=0.112039 err=0.112039
I 2015-05-26 09:30:55 theanets.trainer:168 RmsProp 646 loss=0.113827 err=0.113827
I 2015-05-26 09:30:59 theanets.trainer:168 RmsProp 647 loss=0.113194 err=0.113194
I 2015-05-26 09:31:03 theanets.trainer:168 RmsProp 648 loss=0.112543 err=0.112543
I 2015-05-26 09:31:08 theanets.trainer:168 RmsProp 649 loss=0.106942 err=0.106942
I 2015-05-26 09:31:12 theanets.trainer:168 RmsProp 650 loss=0.123413 err=0.123413
I 2015-05-26 09:31:13 theanets.trainer:168 validation 65 loss=1728.899658 err=1728.899658
I 2015-05-26 09:31:17 theanets.trainer:168 RmsProp 651 loss=0.110538 err=0.110538
I 2015-05-26 09:31:21 theanets.trainer:168 RmsProp 652 loss=0.110635 err=0.110635
I 2015-05-26 09:31:25 theanets.trainer:168 RmsProp 653 loss=0.115523 err=0.115523
I 2015-05-26 09:31:29 theanets.trainer:168 RmsProp 654 loss=0.110546 err=0.110546
I 2015-05-26 09:31:33 theanets.trainer:168 RmsProp 655 loss=0.110584 err=0.110584
I 2015-05-26 09:31:37 theanets.trainer:168 RmsProp 656 loss=0.112398 err=0.112398
I 2015-05-26 09:31:41 theanets.trainer:168 RmsProp 657 loss=0.108536 err=0.108536
I 2015-05-26 09:31:46 theanets.trainer:168 RmsProp 658 loss=0.108570 err=0.108570
I 2015-05-26 09:31:50 theanets.trainer:168 RmsProp 659 loss=0.115578 err=0.115578
I 2015-05-26 09:31:54 theanets.trainer:168 RmsProp 660 loss=0.106078 err=0.106078
I 2015-05-26 09:31:54 theanets.trainer:168 validation 66 loss=1728.552368 err=1728.552368
I 2015-05-26 09:31:54 theanets.trainer:252 patience elapsed!
I 2015-05-26 09:31:54 theanets.main:237 models_deep_post_code_sep/95125-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl: saving model
I 2015-05-26 09:31:54 theanets.graph:477 models_deep_post_code_sep/95125-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl: saved model parameters

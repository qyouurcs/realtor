I 2015-05-26 03:35:25 theanets.layers:465 layer in: out:0 -> 1000, linear, 0 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm1_fw: in.out:1000 -> 100, logistic, 440700 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm1_bw: in.out:1000 -> 100, logistic, 440700 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm1: in.out:1000 -> 200, logistic, 881400 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm2_fw: bdlstm1.out:200 -> 50, logistic, 50350 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm2_bw: bdlstm1.out:200 -> 50, logistic, 50350 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer bdlstm2: bdlstm1.out:200 -> 100, logistic, 100700 parameters
I 2015-05-26 03:35:26 theanets.layers:465 layer out: bdlstm2.out:100 -> 1, linear, 101 parameters
I 2015-05-26 03:35:26 theanets.graph:145 network has 982201 total parameters
models_deep_post_code_sep/95132-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl
I 2015-05-26 03:35:26 theanets.dataset:133 valid: 3 mini-batches from callable
I 2015-05-26 03:35:26 theanets.dataset:133 train: 30 mini-batches from callable
I 2015-05-26 03:35:26 theanets.main:87 creating trainer <class 'theanets.trainer.RmsProp'>
I 2015-05-26 03:35:26 theanets.main:89 --batch_size = 1024
I 2015-05-26 03:35:26 theanets.main:89 --gradient_clip = 1
I 2015-05-26 03:35:26 theanets.main:89 --hidden_l1 = None
I 2015-05-26 03:35:26 theanets.main:89 --learning_rate = 0.001
I 2015-05-26 03:35:26 theanets.main:89 --train_batches = 30
I 2015-05-26 03:35:26 theanets.main:89 --valid_batches = 3
I 2015-05-26 03:35:26 theanets.main:89 --weight_l1 = None
I 2015-05-26 03:35:26 theanets.main:89 --weight_l2 = None
I 2015-05-26 03:35:26 theanets.trainer:134 compiling evaluation function
I 2015-05-26 03:35:42 theanets.trainer:294 compiling RmsProp learning function
I 2015-05-26 03:38:56 theanets.trainer:168 validation 0 loss=14152.983398 err=14152.983398 *
I 2015-05-26 03:39:56 theanets.trainer:168 RmsProp 1 loss=13173.270508 err=13173.270508
I 2015-05-26 03:40:56 theanets.trainer:168 RmsProp 2 loss=13188.847656 err=13188.847656
I 2015-05-26 03:41:57 theanets.trainer:168 RmsProp 3 loss=12153.049805 err=12153.049805
I 2015-05-26 03:42:56 theanets.trainer:168 RmsProp 4 loss=10623.577148 err=10623.577148
I 2015-05-26 03:43:55 theanets.trainer:168 RmsProp 5 loss=9616.033203 err=9616.033203
I 2015-05-26 03:44:54 theanets.trainer:168 RmsProp 6 loss=8717.084961 err=8717.084961
I 2015-05-26 03:45:54 theanets.trainer:168 RmsProp 7 loss=8095.832520 err=8095.832520
I 2015-05-26 03:46:55 theanets.trainer:168 RmsProp 8 loss=8112.416016 err=8112.416016
I 2015-05-26 03:47:55 theanets.trainer:168 RmsProp 9 loss=7691.629395 err=7691.629395
I 2015-05-26 03:48:56 theanets.trainer:168 RmsProp 10 loss=7167.264160 err=7167.264160
I 2015-05-26 03:48:57 theanets.trainer:168 validation 1 loss=7218.595215 err=7218.595215 *
I 2015-05-26 03:49:57 theanets.trainer:168 RmsProp 11 loss=6780.281250 err=6780.281250
I 2015-05-26 03:50:58 theanets.trainer:168 RmsProp 12 loss=6306.620605 err=6306.620605
I 2015-05-26 03:51:59 theanets.trainer:168 RmsProp 13 loss=5938.912598 err=5938.912598
I 2015-05-26 03:53:00 theanets.trainer:168 RmsProp 14 loss=5473.422363 err=5473.422363
I 2015-05-26 03:54:02 theanets.trainer:168 RmsProp 15 loss=4962.115723 err=4962.115723
I 2015-05-26 03:55:03 theanets.trainer:168 RmsProp 16 loss=4581.857422 err=4581.857422
I 2015-05-26 03:56:04 theanets.trainer:168 RmsProp 17 loss=4249.145996 err=4249.145996
I 2015-05-26 03:57:06 theanets.trainer:168 RmsProp 18 loss=3946.859375 err=3946.859375
I 2015-05-26 03:58:07 theanets.trainer:168 RmsProp 19 loss=3698.282471 err=3698.282471
I 2015-05-26 03:59:08 theanets.trainer:168 RmsProp 20 loss=3420.185059 err=3420.185059
I 2015-05-26 03:59:09 theanets.trainer:168 validation 2 loss=3945.400146 err=3945.400146 *
I 2015-05-26 04:00:10 theanets.trainer:168 RmsProp 21 loss=3203.629639 err=3203.629639
I 2015-05-26 04:01:11 theanets.trainer:168 RmsProp 22 loss=3074.680664 err=3074.680664
I 2015-05-26 04:02:12 theanets.trainer:168 RmsProp 23 loss=2954.889160 err=2954.889160
I 2015-05-26 04:03:13 theanets.trainer:168 RmsProp 24 loss=2858.330322 err=2858.330322
I 2015-05-26 04:04:14 theanets.trainer:168 RmsProp 25 loss=2639.683594 err=2639.683594
I 2015-05-26 04:05:15 theanets.trainer:168 RmsProp 26 loss=2487.444824 err=2487.444824
I 2015-05-26 04:06:16 theanets.trainer:168 RmsProp 27 loss=2368.136963 err=2368.136963
I 2015-05-26 04:07:16 theanets.trainer:168 RmsProp 28 loss=2291.026611 err=2291.026611
I 2015-05-26 04:08:17 theanets.trainer:168 RmsProp 29 loss=2211.455566 err=2211.455566
I 2015-05-26 04:09:18 theanets.trainer:168 RmsProp 30 loss=2094.179199 err=2094.179199
I 2015-05-26 04:09:19 theanets.trainer:168 validation 3 loss=3147.968750 err=3147.968750 *
I 2015-05-26 04:10:20 theanets.trainer:168 RmsProp 31 loss=1991.499634 err=1991.499634
I 2015-05-26 04:11:20 theanets.trainer:168 RmsProp 32 loss=1930.705322 err=1930.705322
I 2015-05-26 04:12:20 theanets.trainer:168 RmsProp 33 loss=1963.347168 err=1963.347168
I 2015-05-26 04:13:19 theanets.trainer:168 RmsProp 34 loss=2070.727783 err=2070.727783
I 2015-05-26 04:14:16 theanets.trainer:168 RmsProp 35 loss=2279.862549 err=2279.862549
I 2015-05-26 04:15:13 theanets.trainer:168 RmsProp 36 loss=2369.080322 err=2369.080322
I 2015-05-26 04:16:09 theanets.trainer:168 RmsProp 37 loss=2187.566406 err=2187.566406
I 2015-05-26 04:17:06 theanets.trainer:168 RmsProp 38 loss=1884.432373 err=1884.432373
I 2015-05-26 04:18:02 theanets.trainer:168 RmsProp 39 loss=1676.388672 err=1676.388672
I 2015-05-26 04:18:58 theanets.trainer:168 RmsProp 40 loss=1554.785522 err=1554.785522
I 2015-05-26 04:18:59 theanets.trainer:168 validation 4 loss=2835.481689 err=2835.481689 *
I 2015-05-26 04:19:55 theanets.trainer:168 RmsProp 41 loss=1457.391724 err=1457.391724
I 2015-05-26 04:20:52 theanets.trainer:168 RmsProp 42 loss=1387.636963 err=1387.636963
I 2015-05-26 04:21:50 theanets.trainer:168 RmsProp 43 loss=1299.136963 err=1299.136963
I 2015-05-26 04:22:45 theanets.trainer:168 RmsProp 44 loss=1247.084595 err=1247.084595
I 2015-05-26 04:23:38 theanets.trainer:168 RmsProp 45 loss=1192.736572 err=1192.736572
I 2015-05-26 04:24:31 theanets.trainer:168 RmsProp 46 loss=1146.008179 err=1146.008179
I 2015-05-26 04:25:24 theanets.trainer:168 RmsProp 47 loss=1120.227661 err=1120.227661
I 2015-05-26 04:26:18 theanets.trainer:168 RmsProp 48 loss=1040.280762 err=1040.280762
I 2015-05-26 04:27:11 theanets.trainer:168 RmsProp 49 loss=1014.440674 err=1014.440674
I 2015-05-26 04:28:05 theanets.trainer:168 RmsProp 50 loss=973.337646 err=973.337646
I 2015-05-26 04:28:06 theanets.trainer:168 validation 5 loss=2475.861328 err=2475.861328 *
I 2015-05-26 04:28:58 theanets.trainer:168 RmsProp 51 loss=934.987610 err=934.987610
I 2015-05-26 04:29:51 theanets.trainer:168 RmsProp 52 loss=897.572937 err=897.572937
I 2015-05-26 04:30:44 theanets.trainer:168 RmsProp 53 loss=859.251831 err=859.251831
I 2015-05-26 04:31:38 theanets.trainer:168 RmsProp 54 loss=820.805298 err=820.805298
I 2015-05-26 04:32:30 theanets.trainer:168 RmsProp 55 loss=797.351135 err=797.351135
I 2015-05-26 04:33:23 theanets.trainer:168 RmsProp 56 loss=766.686707 err=766.686707
I 2015-05-26 04:34:16 theanets.trainer:168 RmsProp 57 loss=773.189880 err=773.189880
I 2015-05-26 04:35:10 theanets.trainer:168 RmsProp 58 loss=732.145264 err=732.145264
I 2015-05-26 04:36:04 theanets.trainer:168 RmsProp 59 loss=695.402344 err=695.402344
I 2015-05-26 04:36:58 theanets.trainer:168 RmsProp 60 loss=658.316284 err=658.316284
I 2015-05-26 04:36:59 theanets.trainer:168 validation 6 loss=2475.614502 err=2475.614502 *
I 2015-05-26 04:37:53 theanets.trainer:168 RmsProp 61 loss=653.838623 err=653.838623
I 2015-05-26 04:38:47 theanets.trainer:168 RmsProp 62 loss=620.225647 err=620.225647
I 2015-05-26 04:39:41 theanets.trainer:168 RmsProp 63 loss=594.933533 err=594.933533
I 2015-05-26 04:40:36 theanets.trainer:168 RmsProp 64 loss=569.028198 err=569.028198
I 2015-05-26 04:41:30 theanets.trainer:168 RmsProp 65 loss=573.779480 err=573.779480
I 2015-05-26 04:42:24 theanets.trainer:168 RmsProp 66 loss=542.098206 err=542.098206
I 2015-05-26 04:43:19 theanets.trainer:168 RmsProp 67 loss=522.068970 err=522.068970
I 2015-05-26 04:44:13 theanets.trainer:168 RmsProp 68 loss=512.431091 err=512.431091
I 2015-05-26 04:45:07 theanets.trainer:168 RmsProp 69 loss=488.648041 err=488.648041
I 2015-05-26 04:46:01 theanets.trainer:168 RmsProp 70 loss=469.151001 err=469.151001
I 2015-05-26 04:46:02 theanets.trainer:168 validation 7 loss=2193.895020 err=2193.895020 *
I 2015-05-26 04:46:56 theanets.trainer:168 RmsProp 71 loss=461.455109 err=461.455109
I 2015-05-26 04:47:49 theanets.trainer:168 RmsProp 72 loss=435.210052 err=435.210052
I 2015-05-26 04:48:43 theanets.trainer:168 RmsProp 73 loss=437.715851 err=437.715851
I 2015-05-26 04:49:37 theanets.trainer:168 RmsProp 74 loss=469.639496 err=469.639496
I 2015-05-26 04:50:32 theanets.trainer:168 RmsProp 75 loss=423.892365 err=423.892365
I 2015-05-26 04:51:25 theanets.trainer:168 RmsProp 76 loss=396.170746 err=396.170746
I 2015-05-26 04:52:20 theanets.trainer:168 RmsProp 77 loss=387.748657 err=387.748657
I 2015-05-26 04:53:15 theanets.trainer:168 RmsProp 78 loss=370.548492 err=370.548492
I 2015-05-26 04:54:08 theanets.trainer:168 RmsProp 79 loss=365.473328 err=365.473328
I 2015-05-26 04:55:02 theanets.trainer:168 RmsProp 80 loss=358.471313 err=358.471313
I 2015-05-26 04:55:03 theanets.trainer:168 validation 8 loss=2061.649170 err=2061.649170 *
I 2015-05-26 04:55:55 theanets.trainer:168 RmsProp 81 loss=347.556030 err=347.556030
I 2015-05-26 04:56:48 theanets.trainer:168 RmsProp 82 loss=337.116821 err=337.116821
I 2015-05-26 04:57:40 theanets.trainer:168 RmsProp 83 loss=329.023590 err=329.023590
I 2015-05-26 04:58:33 theanets.trainer:168 RmsProp 84 loss=321.833862 err=321.833862
I 2015-05-26 04:59:26 theanets.trainer:168 RmsProp 85 loss=310.096802 err=310.096802
I 2015-05-26 05:00:18 theanets.trainer:168 RmsProp 86 loss=305.577667 err=305.577667
I 2015-05-26 05:01:11 theanets.trainer:168 RmsProp 87 loss=297.302917 err=297.302917
I 2015-05-26 05:02:04 theanets.trainer:168 RmsProp 88 loss=292.195160 err=292.195160
I 2015-05-26 05:02:56 theanets.trainer:168 RmsProp 89 loss=289.623199 err=289.623199
I 2015-05-26 05:03:49 theanets.trainer:168 RmsProp 90 loss=273.083801 err=273.083801
I 2015-05-26 05:03:51 theanets.trainer:168 validation 9 loss=1996.776245 err=1996.776245 *
I 2015-05-26 05:04:44 theanets.trainer:168 RmsProp 91 loss=264.000549 err=264.000549
I 2015-05-26 05:05:37 theanets.trainer:168 RmsProp 92 loss=259.543915 err=259.543915
I 2015-05-26 05:06:30 theanets.trainer:168 RmsProp 93 loss=258.316193 err=258.316193
I 2015-05-26 05:07:24 theanets.trainer:168 RmsProp 94 loss=251.384232 err=251.384232
I 2015-05-26 05:08:15 theanets.trainer:168 RmsProp 95 loss=241.845871 err=241.845871
I 2015-05-26 05:09:06 theanets.trainer:168 RmsProp 96 loss=236.427780 err=236.427780
I 2015-05-26 05:09:57 theanets.trainer:168 RmsProp 97 loss=238.306976 err=238.306976
I 2015-05-26 05:10:47 theanets.trainer:168 RmsProp 98 loss=226.178162 err=226.178162
I 2015-05-26 05:11:37 theanets.trainer:168 RmsProp 99 loss=224.517715 err=224.517715
I 2015-05-26 05:12:27 theanets.trainer:168 RmsProp 100 loss=216.621414 err=216.621414
I 2015-05-26 05:12:28 theanets.trainer:168 validation 10 loss=1936.676147 err=1936.676147 *
I 2015-05-26 05:13:17 theanets.trainer:168 RmsProp 101 loss=206.078064 err=206.078064
I 2015-05-26 05:14:07 theanets.trainer:168 RmsProp 102 loss=209.376694 err=209.376694
I 2015-05-26 05:14:58 theanets.trainer:168 RmsProp 103 loss=195.735336 err=195.735336
I 2015-05-26 05:15:48 theanets.trainer:168 RmsProp 104 loss=198.039398 err=198.039398
I 2015-05-26 05:16:39 theanets.trainer:168 RmsProp 105 loss=196.762894 err=196.762894
I 2015-05-26 05:17:29 theanets.trainer:168 RmsProp 106 loss=185.078354 err=185.078354
I 2015-05-26 05:18:20 theanets.trainer:168 RmsProp 107 loss=184.964920 err=184.964920
I 2015-05-26 05:19:10 theanets.trainer:168 RmsProp 108 loss=179.424240 err=179.424240
I 2015-05-26 05:20:01 theanets.trainer:168 RmsProp 109 loss=169.940384 err=169.940384
I 2015-05-26 05:20:51 theanets.trainer:168 RmsProp 110 loss=184.565323 err=184.565323
I 2015-05-26 05:20:52 theanets.trainer:168 validation 11 loss=1902.417603 err=1902.417603 *
I 2015-05-26 05:21:43 theanets.trainer:168 RmsProp 111 loss=172.151947 err=172.151947
I 2015-05-26 05:22:34 theanets.trainer:168 RmsProp 112 loss=163.476379 err=163.476379
I 2015-05-26 05:23:25 theanets.trainer:168 RmsProp 113 loss=162.632523 err=162.632523
I 2015-05-26 05:24:16 theanets.trainer:168 RmsProp 114 loss=163.670013 err=163.670013
I 2015-05-26 05:25:06 theanets.trainer:168 RmsProp 115 loss=159.054047 err=159.054047
I 2015-05-26 05:25:57 theanets.trainer:168 RmsProp 116 loss=151.144882 err=151.144882
I 2015-05-26 05:26:47 theanets.trainer:168 RmsProp 117 loss=147.459915 err=147.459915
I 2015-05-26 05:27:37 theanets.trainer:168 RmsProp 118 loss=149.270966 err=149.270966
I 2015-05-26 05:28:28 theanets.trainer:168 RmsProp 119 loss=144.951706 err=144.951706
I 2015-05-26 05:29:19 theanets.trainer:168 RmsProp 120 loss=139.496231 err=139.496231
I 2015-05-26 05:29:20 theanets.trainer:168 validation 12 loss=1853.191040 err=1853.191040 *
I 2015-05-26 05:30:11 theanets.trainer:168 RmsProp 121 loss=139.439041 err=139.439041
I 2015-05-26 05:31:02 theanets.trainer:168 RmsProp 122 loss=139.417130 err=139.417130
I 2015-05-26 05:31:52 theanets.trainer:168 RmsProp 123 loss=133.508942 err=133.508942
I 2015-05-26 05:32:43 theanets.trainer:168 RmsProp 124 loss=124.663574 err=124.663574
I 2015-05-26 05:33:33 theanets.trainer:168 RmsProp 125 loss=131.159363 err=131.159363
I 2015-05-26 05:34:24 theanets.trainer:168 RmsProp 126 loss=129.724686 err=129.724686
I 2015-05-26 05:35:15 theanets.trainer:168 RmsProp 127 loss=124.188957 err=124.188957
I 2015-05-26 05:36:06 theanets.trainer:168 RmsProp 128 loss=120.973846 err=120.973846
I 2015-05-26 05:36:56 theanets.trainer:168 RmsProp 129 loss=124.789024 err=124.789024
I 2015-05-26 05:37:45 theanets.trainer:168 RmsProp 130 loss=114.928230 err=114.928230
I 2015-05-26 05:37:47 theanets.trainer:168 validation 13 loss=1867.411743 err=1867.411743
I 2015-05-26 05:38:35 theanets.trainer:168 RmsProp 131 loss=112.619400 err=112.619400
I 2015-05-26 05:39:23 theanets.trainer:168 RmsProp 132 loss=112.314056 err=112.314056
I 2015-05-26 05:40:11 theanets.trainer:168 RmsProp 133 loss=112.580635 err=112.580635
I 2015-05-26 05:41:00 theanets.trainer:168 RmsProp 134 loss=110.510880 err=110.510880
I 2015-05-26 05:41:49 theanets.trainer:168 RmsProp 135 loss=109.517296 err=109.517296
I 2015-05-26 05:42:38 theanets.trainer:168 RmsProp 136 loss=102.686325 err=102.686325
I 2015-05-26 05:43:26 theanets.trainer:168 RmsProp 137 loss=100.603958 err=100.603958
I 2015-05-26 05:44:16 theanets.trainer:168 RmsProp 138 loss=98.596626 err=98.596626
I 2015-05-26 05:45:05 theanets.trainer:168 RmsProp 139 loss=95.860870 err=95.860870
I 2015-05-26 05:45:54 theanets.trainer:168 RmsProp 140 loss=94.171471 err=94.171471
I 2015-05-26 05:45:56 theanets.trainer:168 validation 14 loss=1835.835571 err=1835.835571 *
I 2015-05-26 05:46:45 theanets.trainer:168 RmsProp 141 loss=96.764275 err=96.764275
I 2015-05-26 05:47:35 theanets.trainer:168 RmsProp 142 loss=94.917267 err=94.917267
I 2015-05-26 05:48:25 theanets.trainer:168 RmsProp 143 loss=97.241997 err=97.241997
I 2015-05-26 05:49:14 theanets.trainer:168 RmsProp 144 loss=85.490677 err=85.490677
I 2015-05-26 05:50:04 theanets.trainer:168 RmsProp 145 loss=86.325203 err=86.325203
I 2015-05-26 05:50:54 theanets.trainer:168 RmsProp 146 loss=85.563492 err=85.563492
I 2015-05-26 05:51:43 theanets.trainer:168 RmsProp 147 loss=82.339630 err=82.339630
I 2015-05-26 05:52:32 theanets.trainer:168 RmsProp 148 loss=80.266205 err=80.266205
I 2015-05-26 05:53:21 theanets.trainer:168 RmsProp 149 loss=75.960045 err=75.960045
I 2015-05-26 05:54:11 theanets.trainer:168 RmsProp 150 loss=78.691360 err=78.691360
I 2015-05-26 05:54:12 theanets.trainer:168 validation 15 loss=1792.194214 err=1792.194214 *
I 2015-05-26 05:55:01 theanets.trainer:168 RmsProp 151 loss=77.373726 err=77.373726
I 2015-05-26 05:55:50 theanets.trainer:168 RmsProp 152 loss=73.058846 err=73.058846
I 2015-05-26 05:56:40 theanets.trainer:168 RmsProp 153 loss=71.061592 err=71.061592
I 2015-05-26 05:57:30 theanets.trainer:168 RmsProp 154 loss=70.268456 err=70.268456
I 2015-05-26 05:58:19 theanets.trainer:168 RmsProp 155 loss=74.172874 err=74.172874
I 2015-05-26 05:59:08 theanets.trainer:168 RmsProp 156 loss=69.460106 err=69.460106
I 2015-05-26 05:59:57 theanets.trainer:168 RmsProp 157 loss=63.954189 err=63.954189
I 2015-05-26 06:00:47 theanets.trainer:168 RmsProp 158 loss=63.085365 err=63.085365
I 2015-05-26 06:01:36 theanets.trainer:168 RmsProp 159 loss=66.078636 err=66.078636
I 2015-05-26 06:02:26 theanets.trainer:168 RmsProp 160 loss=63.476646 err=63.476646
I 2015-05-26 06:02:27 theanets.trainer:168 validation 16 loss=1745.743530 err=1745.743530 *
I 2015-05-26 06:03:16 theanets.trainer:168 RmsProp 161 loss=61.991806 err=61.991806
I 2015-05-26 06:04:06 theanets.trainer:168 RmsProp 162 loss=59.786793 err=59.786793
I 2015-05-26 06:04:56 theanets.trainer:168 RmsProp 163 loss=59.459263 err=59.459263
I 2015-05-26 06:05:45 theanets.trainer:168 RmsProp 164 loss=62.406166 err=62.406166
I 2015-05-26 06:06:34 theanets.trainer:168 RmsProp 165 loss=62.054134 err=62.054134
I 2015-05-26 06:07:21 theanets.trainer:168 RmsProp 166 loss=56.551952 err=56.551952
I 2015-05-26 06:08:09 theanets.trainer:168 RmsProp 167 loss=53.852898 err=53.852898
I 2015-05-26 06:08:58 theanets.trainer:168 RmsProp 168 loss=51.747673 err=51.747673
I 2015-05-26 06:09:48 theanets.trainer:168 RmsProp 169 loss=51.155098 err=51.155098
I 2015-05-26 06:10:37 theanets.trainer:168 RmsProp 170 loss=52.499512 err=52.499512
I 2015-05-26 06:10:38 theanets.trainer:168 validation 17 loss=1733.121460 err=1733.121460 *
I 2015-05-26 06:11:27 theanets.trainer:168 RmsProp 171 loss=53.281925 err=53.281925
I 2015-05-26 06:12:16 theanets.trainer:168 RmsProp 172 loss=48.821293 err=48.821293
I 2015-05-26 06:13:06 theanets.trainer:168 RmsProp 173 loss=46.856567 err=46.856567
I 2015-05-26 06:13:56 theanets.trainer:168 RmsProp 174 loss=48.296658 err=48.296658
I 2015-05-26 06:14:46 theanets.trainer:168 RmsProp 175 loss=48.553627 err=48.553627
I 2015-05-26 06:15:35 theanets.trainer:168 RmsProp 176 loss=45.644157 err=45.644157
I 2015-05-26 06:16:25 theanets.trainer:168 RmsProp 177 loss=45.503151 err=45.503151
I 2015-05-26 06:17:14 theanets.trainer:168 RmsProp 178 loss=44.218105 err=44.218105
I 2015-05-26 06:18:05 theanets.trainer:168 RmsProp 179 loss=46.262989 err=46.262989
I 2015-05-26 06:18:55 theanets.trainer:168 RmsProp 180 loss=44.930016 err=44.930016
I 2015-05-26 06:18:56 theanets.trainer:168 validation 18 loss=1702.706665 err=1702.706665 *
I 2015-05-26 06:19:44 theanets.trainer:168 RmsProp 181 loss=40.726585 err=40.726585
I 2015-05-26 06:20:31 theanets.trainer:168 RmsProp 182 loss=41.922058 err=41.922058
I 2015-05-26 06:21:20 theanets.trainer:168 RmsProp 183 loss=38.420517 err=38.420517
I 2015-05-26 06:22:09 theanets.trainer:168 RmsProp 184 loss=34.679390 err=34.679390
I 2015-05-26 06:22:58 theanets.trainer:168 RmsProp 185 loss=34.282726 err=34.282726
I 2015-05-26 06:23:47 theanets.trainer:168 RmsProp 186 loss=36.482113 err=36.482113
I 2015-05-26 06:24:36 theanets.trainer:168 RmsProp 187 loss=32.446995 err=32.446995
I 2015-05-26 06:25:25 theanets.trainer:168 RmsProp 188 loss=37.833450 err=37.833450
I 2015-05-26 06:26:14 theanets.trainer:168 RmsProp 189 loss=44.421886 err=44.421886
I 2015-05-26 06:27:03 theanets.trainer:168 RmsProp 190 loss=44.541309 err=44.541309
I 2015-05-26 06:27:04 theanets.trainer:168 validation 19 loss=1625.835938 err=1625.835938 *
I 2015-05-26 06:27:54 theanets.trainer:168 RmsProp 191 loss=38.734032 err=38.734032
I 2015-05-26 06:28:43 theanets.trainer:168 RmsProp 192 loss=35.029137 err=35.029137
I 2015-05-26 06:29:33 theanets.trainer:168 RmsProp 193 loss=32.980419 err=32.980419
I 2015-05-26 06:30:22 theanets.trainer:168 RmsProp 194 loss=33.049995 err=33.049995
I 2015-05-26 06:31:12 theanets.trainer:168 RmsProp 195 loss=32.534744 err=32.534744
I 2015-05-26 06:32:02 theanets.trainer:168 RmsProp 196 loss=33.728195 err=33.728195
I 2015-05-26 06:32:52 theanets.trainer:168 RmsProp 197 loss=31.470713 err=31.470713
I 2015-05-26 06:33:41 theanets.trainer:168 RmsProp 198 loss=34.167446 err=34.167446
I 2015-05-26 06:34:30 theanets.trainer:168 RmsProp 199 loss=33.960926 err=33.960926
I 2015-05-26 06:35:15 theanets.trainer:168 RmsProp 200 loss=29.173491 err=29.173491
I 2015-05-26 06:35:16 theanets.trainer:168 validation 20 loss=1658.133789 err=1658.133789
I 2015-05-26 06:36:02 theanets.trainer:168 RmsProp 201 loss=29.557159 err=29.557159
I 2015-05-26 06:36:48 theanets.trainer:168 RmsProp 202 loss=29.849360 err=29.849360
I 2015-05-26 06:37:35 theanets.trainer:168 RmsProp 203 loss=28.608219 err=28.608219
I 2015-05-26 06:38:21 theanets.trainer:168 RmsProp 204 loss=30.127460 err=30.127460
I 2015-05-26 06:39:08 theanets.trainer:168 RmsProp 205 loss=27.367470 err=27.367470
I 2015-05-26 06:39:55 theanets.trainer:168 RmsProp 206 loss=26.655926 err=26.655926
I 2015-05-26 06:40:40 theanets.trainer:168 RmsProp 207 loss=26.748964 err=26.748964
I 2015-05-26 06:41:26 theanets.trainer:168 RmsProp 208 loss=25.666368 err=25.666368
I 2015-05-26 06:42:11 theanets.trainer:168 RmsProp 209 loss=26.127745 err=26.127745
I 2015-05-26 06:42:56 theanets.trainer:168 RmsProp 210 loss=32.363789 err=32.363789
I 2015-05-26 06:42:57 theanets.trainer:168 validation 21 loss=1744.598999 err=1744.598999
I 2015-05-26 06:43:42 theanets.trainer:168 RmsProp 211 loss=29.997204 err=29.997204
I 2015-05-26 06:44:27 theanets.trainer:168 RmsProp 212 loss=26.636070 err=26.636070
I 2015-05-26 06:45:12 theanets.trainer:168 RmsProp 213 loss=23.640633 err=23.640633
I 2015-05-26 06:45:57 theanets.trainer:168 RmsProp 214 loss=26.448875 err=26.448875
I 2015-05-26 06:46:42 theanets.trainer:168 RmsProp 215 loss=24.608049 err=24.608049
I 2015-05-26 06:47:27 theanets.trainer:168 RmsProp 216 loss=25.970222 err=25.970222
I 2015-05-26 06:48:12 theanets.trainer:168 RmsProp 217 loss=23.195562 err=23.195562
I 2015-05-26 06:48:56 theanets.trainer:168 RmsProp 218 loss=22.430840 err=22.430840
I 2015-05-26 06:49:41 theanets.trainer:168 RmsProp 219 loss=20.637022 err=20.637022
I 2015-05-26 06:50:26 theanets.trainer:168 RmsProp 220 loss=22.806143 err=22.806143
I 2015-05-26 06:50:27 theanets.trainer:168 validation 22 loss=1631.361328 err=1631.361328
I 2015-05-26 06:51:12 theanets.trainer:168 RmsProp 221 loss=20.615664 err=20.615664
I 2015-05-26 06:51:57 theanets.trainer:168 RmsProp 222 loss=17.670685 err=17.670685
I 2015-05-26 06:52:42 theanets.trainer:168 RmsProp 223 loss=15.108705 err=15.108705
I 2015-05-26 06:53:27 theanets.trainer:168 RmsProp 224 loss=14.152385 err=14.152385
I 2015-05-26 06:54:11 theanets.trainer:168 RmsProp 225 loss=13.579701 err=13.579701
I 2015-05-26 06:54:56 theanets.trainer:168 RmsProp 226 loss=13.831565 err=13.831565
I 2015-05-26 06:55:42 theanets.trainer:168 RmsProp 227 loss=13.089310 err=13.089310
I 2015-05-26 06:56:27 theanets.trainer:168 RmsProp 228 loss=14.168158 err=14.168158
I 2015-05-26 06:57:12 theanets.trainer:168 RmsProp 229 loss=13.247116 err=13.247116
I 2015-05-26 06:57:55 theanets.trainer:168 RmsProp 230 loss=11.161292 err=11.161292
I 2015-05-26 06:57:56 theanets.trainer:168 validation 23 loss=1638.211792 err=1638.211792
I 2015-05-26 06:58:37 theanets.trainer:168 RmsProp 231 loss=21.054079 err=21.054079
I 2015-05-26 06:59:18 theanets.trainer:168 RmsProp 232 loss=21.012173 err=21.012173
I 2015-05-26 06:59:58 theanets.trainer:168 RmsProp 233 loss=19.447746 err=19.447746
I 2015-05-26 07:00:39 theanets.trainer:168 RmsProp 234 loss=17.589277 err=17.589277
I 2015-05-26 07:01:20 theanets.trainer:168 RmsProp 235 loss=17.639164 err=17.639164
I 2015-05-26 07:02:01 theanets.trainer:168 RmsProp 236 loss=16.789726 err=16.789726
I 2015-05-26 07:02:42 theanets.trainer:168 RmsProp 237 loss=16.237162 err=16.237162
I 2015-05-26 07:03:24 theanets.trainer:168 RmsProp 238 loss=15.760495 err=15.760495
I 2015-05-26 07:04:05 theanets.trainer:168 RmsProp 239 loss=15.639747 err=15.639747
I 2015-05-26 07:04:46 theanets.trainer:168 RmsProp 240 loss=14.493133 err=14.493133
I 2015-05-26 07:04:47 theanets.trainer:168 validation 24 loss=1665.843384 err=1665.843384
I 2015-05-26 07:04:47 theanets.trainer:252 patience elapsed!
I 2015-05-26 07:04:47 theanets.dataset:133 valid: 2 mini-batches from callable
I 2015-05-26 07:04:47 theanets.dataset:133 train: 10 mini-batches from callable
I 2015-05-26 07:04:47 theanets.main:87 creating trainer <class 'theanets.trainer.RmsProp'>
I 2015-05-26 07:04:47 theanets.main:89 --algorithms = rmsprop
I 2015-05-26 07:04:47 theanets.main:89 --batch_size = 1024
I 2015-05-26 07:04:47 theanets.main:89 --gradient_clip = 100000.0
I 2015-05-26 07:04:47 theanets.main:89 --hidden_l1 = None
I 2015-05-26 07:04:47 theanets.main:89 --learning_rate = 0.0001
I 2015-05-26 07:04:47 theanets.main:89 --train_batches = 10
I 2015-05-26 07:04:47 theanets.main:89 --valid_batches = 2
I 2015-05-26 07:04:47 theanets.main:89 --weight_l1 = None
I 2015-05-26 07:04:47 theanets.main:89 --weight_l2 = None
I 2015-05-26 07:04:47 theanets.trainer:134 compiling evaluation function
I 2015-05-26 07:04:57 theanets.trainer:294 compiling RmsProp learning function
I 2015-05-26 07:06:36 theanets.trainer:168 validation 0 loss=646.505371 err=646.505371 *
I 2015-05-26 07:06:49 theanets.trainer:168 RmsProp 1 loss=22.084938 err=22.084938
I 2015-05-26 07:07:03 theanets.trainer:168 RmsProp 2 loss=14.038709 err=14.038709
I 2015-05-26 07:07:17 theanets.trainer:168 RmsProp 3 loss=10.339698 err=10.339698
I 2015-05-26 07:07:31 theanets.trainer:168 RmsProp 4 loss=8.297450 err=8.297450
I 2015-05-26 07:07:45 theanets.trainer:168 RmsProp 5 loss=7.017715 err=7.017715
I 2015-05-26 07:07:59 theanets.trainer:168 RmsProp 6 loss=6.097872 err=6.097872
I 2015-05-26 07:08:13 theanets.trainer:168 RmsProp 7 loss=5.314610 err=5.314610
I 2015-05-26 07:08:27 theanets.trainer:168 RmsProp 8 loss=4.649741 err=4.649741
I 2015-05-26 07:08:40 theanets.trainer:168 RmsProp 9 loss=4.236445 err=4.236445
I 2015-05-26 07:08:54 theanets.trainer:168 RmsProp 10 loss=3.926360 err=3.926360
I 2015-05-26 07:08:54 theanets.trainer:168 validation 1 loss=577.492554 err=577.492554 *
I 2015-05-26 07:09:08 theanets.trainer:168 RmsProp 11 loss=3.552954 err=3.552954
I 2015-05-26 07:09:21 theanets.trainer:168 RmsProp 12 loss=3.281113 err=3.281113
I 2015-05-26 07:09:35 theanets.trainer:168 RmsProp 13 loss=3.146208 err=3.146208
I 2015-05-26 07:09:49 theanets.trainer:168 RmsProp 14 loss=2.889839 err=2.889839
I 2015-05-26 07:10:03 theanets.trainer:168 RmsProp 15 loss=2.738774 err=2.738774
I 2015-05-26 07:10:16 theanets.trainer:168 RmsProp 16 loss=2.588589 err=2.588589
I 2015-05-26 07:10:30 theanets.trainer:168 RmsProp 17 loss=2.559611 err=2.559611
I 2015-05-26 07:10:44 theanets.trainer:168 RmsProp 18 loss=2.322085 err=2.322085
I 2015-05-26 07:10:58 theanets.trainer:168 RmsProp 19 loss=2.195451 err=2.195451
I 2015-05-26 07:11:11 theanets.trainer:168 RmsProp 20 loss=2.124437 err=2.124437
I 2015-05-26 07:11:12 theanets.trainer:168 validation 2 loss=567.319519 err=567.319519 *
I 2015-05-26 07:11:26 theanets.trainer:168 RmsProp 21 loss=2.106537 err=2.106537
I 2015-05-26 07:11:39 theanets.trainer:168 RmsProp 22 loss=1.988886 err=1.988886
I 2015-05-26 07:11:52 theanets.trainer:168 RmsProp 23 loss=1.913825 err=1.913825
I 2015-05-26 07:12:05 theanets.trainer:168 RmsProp 24 loss=1.804738 err=1.804738
I 2015-05-26 07:12:18 theanets.trainer:168 RmsProp 25 loss=1.788197 err=1.788197
I 2015-05-26 07:12:31 theanets.trainer:168 RmsProp 26 loss=1.738398 err=1.738398
I 2015-05-26 07:12:44 theanets.trainer:168 RmsProp 27 loss=1.623552 err=1.623552
I 2015-05-26 07:12:57 theanets.trainer:168 RmsProp 28 loss=1.592778 err=1.592778
I 2015-05-26 07:13:10 theanets.trainer:168 RmsProp 29 loss=1.591905 err=1.591905
I 2015-05-26 07:13:22 theanets.trainer:168 RmsProp 30 loss=1.520680 err=1.520680
I 2015-05-26 07:13:23 theanets.trainer:168 validation 3 loss=561.071106 err=561.071106 *
I 2015-05-26 07:13:36 theanets.trainer:168 RmsProp 31 loss=1.455916 err=1.455916
I 2015-05-26 07:13:50 theanets.trainer:168 RmsProp 32 loss=1.462713 err=1.462713
I 2015-05-26 07:14:03 theanets.trainer:168 RmsProp 33 loss=1.389069 err=1.389069
I 2015-05-26 07:14:17 theanets.trainer:168 RmsProp 34 loss=1.366393 err=1.366393
I 2015-05-26 07:14:30 theanets.trainer:168 RmsProp 35 loss=1.389481 err=1.389481
I 2015-05-26 07:14:44 theanets.trainer:168 RmsProp 36 loss=1.300952 err=1.300952
I 2015-05-26 07:14:58 theanets.trainer:168 RmsProp 37 loss=1.259895 err=1.259895
I 2015-05-26 07:15:11 theanets.trainer:168 RmsProp 38 loss=1.280320 err=1.280320
I 2015-05-26 07:15:25 theanets.trainer:168 RmsProp 39 loss=1.195532 err=1.195532
I 2015-05-26 07:15:39 theanets.trainer:168 RmsProp 40 loss=1.194190 err=1.194190
I 2015-05-26 07:15:39 theanets.trainer:168 validation 4 loss=557.792114 err=557.792114 *
I 2015-05-26 07:15:53 theanets.trainer:168 RmsProp 41 loss=1.157833 err=1.157833
I 2015-05-26 07:16:06 theanets.trainer:168 RmsProp 42 loss=1.162893 err=1.162893
I 2015-05-26 07:16:20 theanets.trainer:168 RmsProp 43 loss=1.105045 err=1.105045
I 2015-05-26 07:16:34 theanets.trainer:168 RmsProp 44 loss=1.126963 err=1.126963
I 2015-05-26 07:16:47 theanets.trainer:168 RmsProp 45 loss=1.089258 err=1.089258
I 2015-05-26 07:17:01 theanets.trainer:168 RmsProp 46 loss=1.072461 err=1.072461
I 2015-05-26 07:17:14 theanets.trainer:168 RmsProp 47 loss=1.038306 err=1.038306
I 2015-05-26 07:17:28 theanets.trainer:168 RmsProp 48 loss=1.042353 err=1.042353
I 2015-05-26 07:17:42 theanets.trainer:168 RmsProp 49 loss=1.014442 err=1.014442
I 2015-05-26 07:17:55 theanets.trainer:168 RmsProp 50 loss=0.967121 err=0.967121
I 2015-05-26 07:17:55 theanets.trainer:168 validation 5 loss=557.417480 err=557.417480 *
I 2015-05-26 07:18:09 theanets.trainer:168 RmsProp 51 loss=0.988225 err=0.988225
I 2015-05-26 07:18:22 theanets.trainer:168 RmsProp 52 loss=0.960706 err=0.960706
I 2015-05-26 07:18:36 theanets.trainer:168 RmsProp 53 loss=0.934444 err=0.934444
I 2015-05-26 07:18:49 theanets.trainer:168 RmsProp 54 loss=0.929944 err=0.929944
I 2015-05-26 07:19:03 theanets.trainer:168 RmsProp 55 loss=0.897350 err=0.897350
I 2015-05-26 07:19:17 theanets.trainer:168 RmsProp 56 loss=0.913604 err=0.913604
I 2015-05-26 07:19:30 theanets.trainer:168 RmsProp 57 loss=0.883118 err=0.883118
I 2015-05-26 07:19:44 theanets.trainer:168 RmsProp 58 loss=0.875894 err=0.875894
I 2015-05-26 07:19:58 theanets.trainer:168 RmsProp 59 loss=0.879431 err=0.879431
I 2015-05-26 07:20:12 theanets.trainer:168 RmsProp 60 loss=0.837417 err=0.837417
I 2015-05-26 07:20:12 theanets.trainer:168 validation 6 loss=553.756836 err=553.756836 *
I 2015-05-26 07:20:26 theanets.trainer:168 RmsProp 61 loss=0.828466 err=0.828466
I 2015-05-26 07:20:39 theanets.trainer:168 RmsProp 62 loss=0.832013 err=0.832013
I 2015-05-26 07:20:53 theanets.trainer:168 RmsProp 63 loss=0.798449 err=0.798449
I 2015-05-26 07:21:07 theanets.trainer:168 RmsProp 64 loss=0.807968 err=0.807968
I 2015-05-26 07:21:20 theanets.trainer:168 RmsProp 65 loss=0.793531 err=0.793531
I 2015-05-26 07:21:34 theanets.trainer:168 RmsProp 66 loss=0.760521 err=0.760521
I 2015-05-26 07:21:48 theanets.trainer:168 RmsProp 67 loss=0.760898 err=0.760898
I 2015-05-26 07:22:02 theanets.trainer:168 RmsProp 68 loss=0.781931 err=0.781931
I 2015-05-26 07:22:15 theanets.trainer:168 RmsProp 69 loss=0.743686 err=0.743686
I 2015-05-26 07:22:29 theanets.trainer:168 RmsProp 70 loss=0.721124 err=0.721124
I 2015-05-26 07:22:29 theanets.trainer:168 validation 7 loss=552.435181 err=552.435181 *
I 2015-05-26 07:22:43 theanets.trainer:168 RmsProp 71 loss=0.749238 err=0.749238
I 2015-05-26 07:22:56 theanets.trainer:168 RmsProp 72 loss=0.726148 err=0.726148
I 2015-05-26 07:23:10 theanets.trainer:168 RmsProp 73 loss=0.716781 err=0.716781
I 2015-05-26 07:23:24 theanets.trainer:168 RmsProp 74 loss=0.706189 err=0.706189
I 2015-05-26 07:23:38 theanets.trainer:168 RmsProp 75 loss=0.685308 err=0.685308
I 2015-05-26 07:23:52 theanets.trainer:168 RmsProp 76 loss=0.707697 err=0.707697
I 2015-05-26 07:24:06 theanets.trainer:168 RmsProp 77 loss=0.667849 err=0.667849
I 2015-05-26 07:24:20 theanets.trainer:168 RmsProp 78 loss=0.661446 err=0.661446
I 2015-05-26 07:24:33 theanets.trainer:168 RmsProp 79 loss=0.696189 err=0.696189
I 2015-05-26 07:24:47 theanets.trainer:168 RmsProp 80 loss=0.672116 err=0.672116
I 2015-05-26 07:24:47 theanets.trainer:168 validation 8 loss=551.557739 err=551.557739 *
I 2015-05-26 07:25:01 theanets.trainer:168 RmsProp 81 loss=0.645384 err=0.645384
I 2015-05-26 07:25:13 theanets.trainer:168 RmsProp 82 loss=0.635610 err=0.635610
I 2015-05-26 07:25:26 theanets.trainer:168 RmsProp 83 loss=0.619112 err=0.619112
I 2015-05-26 07:25:39 theanets.trainer:168 RmsProp 84 loss=0.677052 err=0.677052
I 2015-05-26 07:25:52 theanets.trainer:168 RmsProp 85 loss=0.633030 err=0.633030
I 2015-05-26 07:26:05 theanets.trainer:168 RmsProp 86 loss=0.619983 err=0.619983
I 2015-05-26 07:26:18 theanets.trainer:168 RmsProp 87 loss=0.596996 err=0.596996
I 2015-05-26 07:26:31 theanets.trainer:168 RmsProp 88 loss=0.609295 err=0.609295
I 2015-05-26 07:26:44 theanets.trainer:168 RmsProp 89 loss=0.601013 err=0.601013
I 2015-05-26 07:26:56 theanets.trainer:168 RmsProp 90 loss=0.597270 err=0.597270
I 2015-05-26 07:26:57 theanets.trainer:168 validation 9 loss=550.437561 err=550.437561 *
I 2015-05-26 07:27:09 theanets.trainer:168 RmsProp 91 loss=0.581189 err=0.581189
I 2015-05-26 07:27:22 theanets.trainer:168 RmsProp 92 loss=0.595105 err=0.595105
I 2015-05-26 07:27:34 theanets.trainer:168 RmsProp 93 loss=0.568688 err=0.568688
I 2015-05-26 07:27:47 theanets.trainer:168 RmsProp 94 loss=0.567218 err=0.567218
I 2015-05-26 07:27:59 theanets.trainer:168 RmsProp 95 loss=0.576091 err=0.576091
I 2015-05-26 07:28:12 theanets.trainer:168 RmsProp 96 loss=0.584671 err=0.584671
I 2015-05-26 07:28:24 theanets.trainer:168 RmsProp 97 loss=0.566142 err=0.566142
I 2015-05-26 07:28:37 theanets.trainer:168 RmsProp 98 loss=0.536036 err=0.536036
I 2015-05-26 07:28:49 theanets.trainer:168 RmsProp 99 loss=0.556628 err=0.556628
I 2015-05-26 07:29:02 theanets.trainer:168 RmsProp 100 loss=0.534527 err=0.534527
I 2015-05-26 07:29:02 theanets.trainer:168 validation 10 loss=549.990845 err=549.990845 *
I 2015-05-26 07:29:15 theanets.trainer:168 RmsProp 101 loss=0.557653 err=0.557653
I 2015-05-26 07:29:27 theanets.trainer:168 RmsProp 102 loss=0.545887 err=0.545887
I 2015-05-26 07:29:40 theanets.trainer:168 RmsProp 103 loss=0.529040 err=0.529040
I 2015-05-26 07:29:53 theanets.trainer:168 RmsProp 104 loss=0.516631 err=0.516631
I 2015-05-26 07:30:06 theanets.trainer:168 RmsProp 105 loss=0.534222 err=0.534222
I 2015-05-26 07:30:18 theanets.trainer:168 RmsProp 106 loss=0.522097 err=0.522097
I 2015-05-26 07:30:31 theanets.trainer:168 RmsProp 107 loss=0.505889 err=0.505889
I 2015-05-26 07:30:43 theanets.trainer:168 RmsProp 108 loss=0.535225 err=0.535225
I 2015-05-26 07:30:55 theanets.trainer:168 RmsProp 109 loss=0.518015 err=0.518015
I 2015-05-26 07:31:08 theanets.trainer:168 RmsProp 110 loss=0.484485 err=0.484485
I 2015-05-26 07:31:08 theanets.trainer:168 validation 11 loss=549.203003 err=549.203003 *
I 2015-05-26 07:31:21 theanets.trainer:168 RmsProp 111 loss=0.486391 err=0.486391
I 2015-05-26 07:31:33 theanets.trainer:168 RmsProp 112 loss=0.496708 err=0.496708
I 2015-05-26 07:31:45 theanets.trainer:168 RmsProp 113 loss=0.468648 err=0.468648
I 2015-05-26 07:31:57 theanets.trainer:168 RmsProp 114 loss=0.500143 err=0.500143
I 2015-05-26 07:32:09 theanets.trainer:168 RmsProp 115 loss=0.474364 err=0.474364
I 2015-05-26 07:32:21 theanets.trainer:168 RmsProp 116 loss=0.481510 err=0.481510
I 2015-05-26 07:32:33 theanets.trainer:168 RmsProp 117 loss=0.478075 err=0.478075
I 2015-05-26 07:32:45 theanets.trainer:168 RmsProp 118 loss=0.461106 err=0.461106
I 2015-05-26 07:32:57 theanets.trainer:168 RmsProp 119 loss=0.448963 err=0.448963
I 2015-05-26 07:33:09 theanets.trainer:168 RmsProp 120 loss=0.487220 err=0.487220
I 2015-05-26 07:33:09 theanets.trainer:168 validation 12 loss=548.340271 err=548.340271 *
I 2015-05-26 07:33:21 theanets.trainer:168 RmsProp 121 loss=0.460200 err=0.460200
I 2015-05-26 07:33:34 theanets.trainer:168 RmsProp 122 loss=0.459864 err=0.459864
I 2015-05-26 07:33:46 theanets.trainer:168 RmsProp 123 loss=0.447228 err=0.447228
I 2015-05-26 07:33:59 theanets.trainer:168 RmsProp 124 loss=0.457853 err=0.457853
I 2015-05-26 07:34:11 theanets.trainer:168 RmsProp 125 loss=0.450560 err=0.450560
I 2015-05-26 07:34:24 theanets.trainer:168 RmsProp 126 loss=0.454440 err=0.454440
I 2015-05-26 07:34:36 theanets.trainer:168 RmsProp 127 loss=0.439277 err=0.439277
I 2015-05-26 07:34:48 theanets.trainer:168 RmsProp 128 loss=0.423394 err=0.423394
I 2015-05-26 07:35:01 theanets.trainer:168 RmsProp 129 loss=0.439373 err=0.439373
I 2015-05-26 07:35:13 theanets.trainer:168 RmsProp 130 loss=0.444607 err=0.444607
I 2015-05-26 07:35:14 theanets.trainer:168 validation 13 loss=547.990723 err=547.990723 *
I 2015-05-26 07:35:26 theanets.trainer:168 RmsProp 131 loss=0.447407 err=0.447407
I 2015-05-26 07:35:38 theanets.trainer:168 RmsProp 132 loss=0.433882 err=0.433882
I 2015-05-26 07:35:51 theanets.trainer:168 RmsProp 133 loss=0.424289 err=0.424289
I 2015-05-26 07:36:03 theanets.trainer:168 RmsProp 134 loss=0.427290 err=0.427290
I 2015-05-26 07:36:16 theanets.trainer:168 RmsProp 135 loss=0.413753 err=0.413753
I 2015-05-26 07:36:28 theanets.trainer:168 RmsProp 136 loss=0.398972 err=0.398972
I 2015-05-26 07:36:40 theanets.trainer:168 RmsProp 137 loss=0.447510 err=0.447510
I 2015-05-26 07:36:53 theanets.trainer:168 RmsProp 138 loss=0.413159 err=0.413159
I 2015-05-26 07:37:06 theanets.trainer:168 RmsProp 139 loss=0.406072 err=0.406072
I 2015-05-26 07:37:18 theanets.trainer:168 RmsProp 140 loss=0.392333 err=0.392333
I 2015-05-26 07:37:18 theanets.trainer:168 validation 14 loss=548.131042 err=548.131042
I 2015-05-26 07:37:31 theanets.trainer:168 RmsProp 141 loss=0.402285 err=0.402285
I 2015-05-26 07:37:43 theanets.trainer:168 RmsProp 142 loss=0.403603 err=0.403603
I 2015-05-26 07:37:56 theanets.trainer:168 RmsProp 143 loss=0.386857 err=0.386857
I 2015-05-26 07:38:09 theanets.trainer:168 RmsProp 144 loss=0.398412 err=0.398412
I 2015-05-26 07:38:21 theanets.trainer:168 RmsProp 145 loss=0.405895 err=0.405895
I 2015-05-26 07:38:34 theanets.trainer:168 RmsProp 146 loss=0.387192 err=0.387192
I 2015-05-26 07:38:46 theanets.trainer:168 RmsProp 147 loss=0.382086 err=0.382086
I 2015-05-26 07:38:58 theanets.trainer:168 RmsProp 148 loss=0.399332 err=0.399332
I 2015-05-26 07:39:11 theanets.trainer:168 RmsProp 149 loss=0.372444 err=0.372444
I 2015-05-26 07:39:24 theanets.trainer:168 RmsProp 150 loss=0.400914 err=0.400914
I 2015-05-26 07:39:24 theanets.trainer:168 validation 15 loss=547.341125 err=547.341125 *
I 2015-05-26 07:39:37 theanets.trainer:168 RmsProp 151 loss=0.387903 err=0.387903
I 2015-05-26 07:39:50 theanets.trainer:168 RmsProp 152 loss=0.375258 err=0.375258
I 2015-05-26 07:40:03 theanets.trainer:168 RmsProp 153 loss=0.375836 err=0.375836
I 2015-05-26 07:40:16 theanets.trainer:168 RmsProp 154 loss=0.374745 err=0.374745
I 2015-05-26 07:40:29 theanets.trainer:168 RmsProp 155 loss=0.361659 err=0.361659
I 2015-05-26 07:40:41 theanets.trainer:168 RmsProp 156 loss=0.384878 err=0.384878
I 2015-05-26 07:40:54 theanets.trainer:168 RmsProp 157 loss=0.375265 err=0.375265
I 2015-05-26 07:41:07 theanets.trainer:168 RmsProp 158 loss=0.347168 err=0.347168
I 2015-05-26 07:41:20 theanets.trainer:168 RmsProp 159 loss=0.364165 err=0.364165
I 2015-05-26 07:41:32 theanets.trainer:168 RmsProp 160 loss=0.355984 err=0.355984
I 2015-05-26 07:41:33 theanets.trainer:168 validation 16 loss=546.989441 err=546.989441 *
I 2015-05-26 07:41:46 theanets.trainer:168 RmsProp 161 loss=0.362863 err=0.362863
I 2015-05-26 07:41:59 theanets.trainer:168 RmsProp 162 loss=0.350305 err=0.350305
I 2015-05-26 07:42:12 theanets.trainer:168 RmsProp 163 loss=0.338518 err=0.338518
I 2015-05-26 07:42:25 theanets.trainer:168 RmsProp 164 loss=0.381312 err=0.381312
I 2015-05-26 07:42:37 theanets.trainer:168 RmsProp 165 loss=0.363984 err=0.363984
I 2015-05-26 07:42:50 theanets.trainer:168 RmsProp 166 loss=0.347681 err=0.347681
I 2015-05-26 07:43:02 theanets.trainer:168 RmsProp 167 loss=0.343196 err=0.343196
I 2015-05-26 07:43:15 theanets.trainer:168 RmsProp 168 loss=0.344133 err=0.344133
I 2015-05-26 07:43:27 theanets.trainer:168 RmsProp 169 loss=0.322978 err=0.322978
I 2015-05-26 07:43:39 theanets.trainer:168 RmsProp 170 loss=0.368870 err=0.368870
I 2015-05-26 07:43:40 theanets.trainer:168 validation 17 loss=547.115173 err=547.115173
I 2015-05-26 07:43:53 theanets.trainer:168 RmsProp 171 loss=0.346896 err=0.346896
I 2015-05-26 07:44:05 theanets.trainer:168 RmsProp 172 loss=0.335463 err=0.335463
I 2015-05-26 07:44:17 theanets.trainer:168 RmsProp 173 loss=0.343566 err=0.343566
I 2015-05-26 07:44:29 theanets.trainer:168 RmsProp 174 loss=0.346875 err=0.346875
I 2015-05-26 07:44:42 theanets.trainer:168 RmsProp 175 loss=0.338883 err=0.338883
I 2015-05-26 07:44:54 theanets.trainer:168 RmsProp 176 loss=0.346606 err=0.346606
I 2015-05-26 07:45:06 theanets.trainer:168 RmsProp 177 loss=0.347202 err=0.347202
I 2015-05-26 07:45:18 theanets.trainer:168 RmsProp 178 loss=0.332019 err=0.332019
I 2015-05-26 07:45:30 theanets.trainer:168 RmsProp 179 loss=0.331918 err=0.331918
I 2015-05-26 07:45:42 theanets.trainer:168 RmsProp 180 loss=0.316838 err=0.316838
I 2015-05-26 07:45:43 theanets.trainer:168 validation 18 loss=547.024353 err=547.024353
I 2015-05-26 07:45:55 theanets.trainer:168 RmsProp 181 loss=0.324846 err=0.324846
I 2015-05-26 07:46:07 theanets.trainer:168 RmsProp 182 loss=0.316101 err=0.316101
I 2015-05-26 07:46:19 theanets.trainer:168 RmsProp 183 loss=0.320066 err=0.320066
I 2015-05-26 07:46:31 theanets.trainer:168 RmsProp 184 loss=0.316859 err=0.316859
I 2015-05-26 07:46:43 theanets.trainer:168 RmsProp 185 loss=0.331973 err=0.331973
I 2015-05-26 07:46:56 theanets.trainer:168 RmsProp 186 loss=0.322637 err=0.322637
I 2015-05-26 07:47:08 theanets.trainer:168 RmsProp 187 loss=0.318571 err=0.318571
I 2015-05-26 07:47:20 theanets.trainer:168 RmsProp 188 loss=0.325021 err=0.325021
I 2015-05-26 07:47:32 theanets.trainer:168 RmsProp 189 loss=0.307536 err=0.307536
I 2015-05-26 07:47:45 theanets.trainer:168 RmsProp 190 loss=0.310402 err=0.310402
I 2015-05-26 07:47:45 theanets.trainer:168 validation 19 loss=546.544556 err=546.544556 *
I 2015-05-26 07:47:57 theanets.trainer:168 RmsProp 191 loss=0.307394 err=0.307394
I 2015-05-26 07:48:10 theanets.trainer:168 RmsProp 192 loss=0.315691 err=0.315691
I 2015-05-26 07:48:22 theanets.trainer:168 RmsProp 193 loss=0.299803 err=0.299803
I 2015-05-26 07:48:34 theanets.trainer:168 RmsProp 194 loss=0.300304 err=0.300304
I 2015-05-26 07:48:46 theanets.trainer:168 RmsProp 195 loss=0.298164 err=0.298164
I 2015-05-26 07:48:58 theanets.trainer:168 RmsProp 196 loss=0.295965 err=0.295965
I 2015-05-26 07:49:10 theanets.trainer:168 RmsProp 197 loss=0.300759 err=0.300759
I 2015-05-26 07:49:22 theanets.trainer:168 RmsProp 198 loss=0.301743 err=0.301743
I 2015-05-26 07:49:34 theanets.trainer:168 RmsProp 199 loss=0.314171 err=0.314171
I 2015-05-26 07:49:46 theanets.trainer:168 RmsProp 200 loss=0.289254 err=0.289254
I 2015-05-26 07:49:47 theanets.trainer:168 validation 20 loss=547.780457 err=547.780457
I 2015-05-26 07:49:59 theanets.trainer:168 RmsProp 201 loss=0.302694 err=0.302694
I 2015-05-26 07:50:10 theanets.trainer:168 RmsProp 202 loss=0.304801 err=0.304801
I 2015-05-26 07:50:21 theanets.trainer:168 RmsProp 203 loss=0.292798 err=0.292798
I 2015-05-26 07:50:32 theanets.trainer:168 RmsProp 204 loss=0.284302 err=0.284302
I 2015-05-26 07:50:44 theanets.trainer:168 RmsProp 205 loss=0.279785 err=0.279785
I 2015-05-26 07:50:55 theanets.trainer:168 RmsProp 206 loss=0.290023 err=0.290023
I 2015-05-26 07:51:06 theanets.trainer:168 RmsProp 207 loss=0.284263 err=0.284263
I 2015-05-26 07:51:17 theanets.trainer:168 RmsProp 208 loss=0.289296 err=0.289296
I 2015-05-26 07:51:29 theanets.trainer:168 RmsProp 209 loss=0.275751 err=0.275751
I 2015-05-26 07:51:40 theanets.trainer:168 RmsProp 210 loss=0.273153 err=0.273153
I 2015-05-26 07:51:40 theanets.trainer:168 validation 21 loss=545.486084 err=545.486084 *
I 2015-05-26 07:51:51 theanets.trainer:168 RmsProp 211 loss=0.280491 err=0.280491
I 2015-05-26 07:52:03 theanets.trainer:168 RmsProp 212 loss=0.286761 err=0.286761
I 2015-05-26 07:52:15 theanets.trainer:168 RmsProp 213 loss=0.278886 err=0.278886
I 2015-05-26 07:52:27 theanets.trainer:168 RmsProp 214 loss=0.287067 err=0.287067
I 2015-05-26 07:52:40 theanets.trainer:168 RmsProp 215 loss=0.274264 err=0.274264
I 2015-05-26 07:52:52 theanets.trainer:168 RmsProp 216 loss=0.276061 err=0.276061
I 2015-05-26 07:53:03 theanets.trainer:168 RmsProp 217 loss=0.280108 err=0.280108
I 2015-05-26 07:53:16 theanets.trainer:168 RmsProp 218 loss=0.272028 err=0.272028
I 2015-05-26 07:53:28 theanets.trainer:168 RmsProp 219 loss=0.266815 err=0.266815
I 2015-05-26 07:53:40 theanets.trainer:168 RmsProp 220 loss=0.262602 err=0.262602
I 2015-05-26 07:53:41 theanets.trainer:168 validation 22 loss=547.133728 err=547.133728
I 2015-05-26 07:53:53 theanets.trainer:168 RmsProp 221 loss=0.267739 err=0.267739
I 2015-05-26 07:54:05 theanets.trainer:168 RmsProp 222 loss=0.268646 err=0.268646
I 2015-05-26 07:54:18 theanets.trainer:168 RmsProp 223 loss=0.261744 err=0.261744
I 2015-05-26 07:54:30 theanets.trainer:168 RmsProp 224 loss=0.280798 err=0.280798
I 2015-05-26 07:54:42 theanets.trainer:168 RmsProp 225 loss=0.266077 err=0.266077
I 2015-05-26 07:54:54 theanets.trainer:168 RmsProp 226 loss=0.259693 err=0.259693
I 2015-05-26 07:55:07 theanets.trainer:168 RmsProp 227 loss=0.268680 err=0.268680
I 2015-05-26 07:55:19 theanets.trainer:168 RmsProp 228 loss=0.253337 err=0.253337
I 2015-05-26 07:55:31 theanets.trainer:168 RmsProp 229 loss=0.266707 err=0.266707
I 2015-05-26 07:55:43 theanets.trainer:168 RmsProp 230 loss=0.282398 err=0.282398
I 2015-05-26 07:55:44 theanets.trainer:168 validation 23 loss=546.321472 err=546.321472
I 2015-05-26 07:55:56 theanets.trainer:168 RmsProp 231 loss=0.253568 err=0.253568
I 2015-05-26 07:56:08 theanets.trainer:168 RmsProp 232 loss=0.248596 err=0.248596
I 2015-05-26 07:56:21 theanets.trainer:168 RmsProp 233 loss=0.269519 err=0.269519
I 2015-05-26 07:56:33 theanets.trainer:168 RmsProp 234 loss=0.247987 err=0.247987
I 2015-05-26 07:56:45 theanets.trainer:168 RmsProp 235 loss=0.243650 err=0.243650
I 2015-05-26 07:56:57 theanets.trainer:168 RmsProp 236 loss=0.283586 err=0.283586
I 2015-05-26 07:57:10 theanets.trainer:168 RmsProp 237 loss=0.269549 err=0.269549
I 2015-05-26 07:57:22 theanets.trainer:168 RmsProp 238 loss=0.243419 err=0.243419
I 2015-05-26 07:57:35 theanets.trainer:168 RmsProp 239 loss=0.233441 err=0.233441
I 2015-05-26 07:57:47 theanets.trainer:168 RmsProp 240 loss=0.261329 err=0.261329
I 2015-05-26 07:57:48 theanets.trainer:168 validation 24 loss=546.007751 err=546.007751
I 2015-05-26 07:57:59 theanets.trainer:168 RmsProp 241 loss=0.255370 err=0.255370
I 2015-05-26 07:58:11 theanets.trainer:168 RmsProp 242 loss=0.253301 err=0.253301
I 2015-05-26 07:58:23 theanets.trainer:168 RmsProp 243 loss=0.270237 err=0.270237
I 2015-05-26 07:58:34 theanets.trainer:168 RmsProp 244 loss=0.243528 err=0.243528
I 2015-05-26 07:58:46 theanets.trainer:168 RmsProp 245 loss=0.235253 err=0.235253
I 2015-05-26 07:58:58 theanets.trainer:168 RmsProp 246 loss=0.244913 err=0.244913
I 2015-05-26 07:59:09 theanets.trainer:168 RmsProp 247 loss=0.244068 err=0.244068
I 2015-05-26 07:59:21 theanets.trainer:168 RmsProp 248 loss=0.254733 err=0.254733
I 2015-05-26 07:59:32 theanets.trainer:168 RmsProp 249 loss=0.240538 err=0.240538
I 2015-05-26 07:59:44 theanets.trainer:168 RmsProp 250 loss=0.242821 err=0.242821
I 2015-05-26 07:59:45 theanets.trainer:168 validation 25 loss=548.096863 err=548.096863
I 2015-05-26 07:59:57 theanets.trainer:168 RmsProp 251 loss=0.260245 err=0.260245
I 2015-05-26 08:00:09 theanets.trainer:168 RmsProp 252 loss=0.243824 err=0.243824
I 2015-05-26 08:00:21 theanets.trainer:168 RmsProp 253 loss=0.242429 err=0.242429
I 2015-05-26 08:00:33 theanets.trainer:168 RmsProp 254 loss=0.228197 err=0.228197
I 2015-05-26 08:00:45 theanets.trainer:168 RmsProp 255 loss=0.276416 err=0.276416
I 2015-05-26 08:00:58 theanets.trainer:168 RmsProp 256 loss=0.244691 err=0.244691
I 2015-05-26 08:01:10 theanets.trainer:168 RmsProp 257 loss=0.221411 err=0.221411
I 2015-05-26 08:01:22 theanets.trainer:168 RmsProp 258 loss=0.240124 err=0.240124
I 2015-05-26 08:01:35 theanets.trainer:168 RmsProp 259 loss=0.241230 err=0.241230
I 2015-05-26 08:01:47 theanets.trainer:168 RmsProp 260 loss=0.230786 err=0.230786
I 2015-05-26 08:01:47 theanets.trainer:168 validation 26 loss=545.565369 err=545.565369
I 2015-05-26 08:01:47 theanets.trainer:252 patience elapsed!
I 2015-05-26 08:01:47 theanets.main:237 models_deep_post_code_sep/95132-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl: saving model
I 2015-05-26 08:01:47 theanets.graph:477 models_deep_post_code_sep/95132-models-sep_san_jose_realtor_200_100.conf-1024-None-None-None.pkl: saved model parameters
